{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a7948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.utils import PyDataset\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed, parallel_config\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from evaluation.evaluator import read_ground_truth_files\n",
    "\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from natsort import natsorted\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from joblib import Memory\n",
    "\n",
    "memory = Memory(\".cache\", verbose=0)\n",
    "# memory = Memory(\".cache_test\", verbose=1)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "@memory.cache\n",
    "def get_tokenizer_model():\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    bert_model = BertModel.from_pretrained(\"bert-base-cased\").to(DEVICE)\n",
    "\n",
    "    return bert_tokenizer, bert_model\n",
    "\n",
    "@memory.cache\n",
    "def read_problem_files(problem_folder, start=0, stop=None):\n",
    "    # print(f\"{start=} {stop=}\")\n",
    "    problems = []\n",
    "    files = itertools.islice(natsorted(Path(problem_folder).glob('problem-*.txt')), start, stop)\n",
    "    for problem_file in files:\n",
    "        # print(f\"{problem_file=}\")\n",
    "        # number = problem_file.name[len(\"problem-\") : -len(\".txt\")]\n",
    "        with open(problem_file, 'r', encoding=\"utf8\") as fh:\n",
    "            problems.append(fh.readlines())\n",
    "    return problems\n",
    "\n",
    "def pad_paragraph(paragraph_embedding, desired_length):\n",
    "    d1, d2, d3 = paragraph_embedding.shape\n",
    "    # print(f\"{paragraph_embedding.shape=}\")\n",
    "\n",
    "    target = torch.zeros(d1, desired_length, d3)\n",
    "    # print(f\"{target.shape=}\")\n",
    "    target[:, :d2, :] = paragraph_embedding\n",
    "\n",
    "    return target\n",
    "\n",
    "def get_simple_ground_truth(ground_truth, problem_numbers):\n",
    "    simple_ground_truth = []\n",
    "    for num in problem_numbers:\n",
    "        task_3_ground_truth = ground_truth[f\"problem-{num}\"][\"paragraph-authors\"]\n",
    "        simple_ground_truth.append(task_3_ground_truth)\n",
    "    return simple_ground_truth\n",
    "\n",
    "def get_task_3_ground_truth(simple_ground_truth):\n",
    "    # print(f\"{len(simple_ground_truth)=}\")\n",
    "    task_gt = []\n",
    "    for problem in simple_ground_truth:\n",
    "        problem_gt = []\n",
    "        for author1, author2 in itertools.combinations(problem, 2):\n",
    "            problem_gt.append(int(author1 != author2))\n",
    "        task_gt.append(problem_gt)\n",
    "    # print(f\"{len(task_gt)=}\")\n",
    "    return task_gt\n",
    "\n",
    "# TODO: Invert the function get_task_3_ground_truth. Our model will output a bunch of binary labels which need to be converted to the task 3 ground truth format\n",
    "# Ground truth format (gtf): [1, 2, 2, 2, 2, 3, 2, 2]\n",
    "# Binary labels for comparisons (bl): [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0]\n",
    "# Each binary label is the result of comparing two paragraphs. 1 means there was an author change, 0 means there was no author change\n",
    "# For example, bl[0], is the result of comparing gtf[0]=1 and gtf[1]=2. 1 != 2, therefore bl[0] = 1. bl[1]=1 is the result of gtf[0] == gtf[2] (1 == 2)\n",
    "def get_simple_ground_truth_from_task_3(task_3_ground_truth):\n",
    "    simple_gt = []\n",
    "    for problem in task_3_ground_truth:\n",
    "        # k = n*(n-1)/2\n",
    "        # n**2 - n - 2k = 0\n",
    "        coeff = [1, -1, len(problem) * -2]\n",
    "        roots = np.roots(coeff)\n",
    "        gt_length = int(roots[roots > 0][0])\n",
    "        # print(gt_length)\n",
    "\n",
    "        gt = np.zeros(gt_length, dtype=np.uint8)\n",
    "        gt[0] = 1\n",
    "        for i in range(1, gt_length):\n",
    "            # loop for gt[i]\n",
    "            num_comparisons = i\n",
    "            pointer = i - 1\n",
    "            modified_flag = False\n",
    "            # print(f\"{i=} {num_comparisons=} {pointer=}\")\n",
    "            for gt_i, j in enumerate(range(gt_length-2, 1, -1)[:num_comparisons]):\n",
    "                # comparison between gt[gt_i] and gt[i]\n",
    "                # print(f\"{gt_i=} {j=} {pointer=} {task_3_ground_truth[pointer]=}\")\n",
    "                bin_label = task_3_ground_truth[pointer]\n",
    "                if bin_label == 0:\n",
    "                    # print(f\"{gt[i]=} {gt[gt_i]=}\")\n",
    "                    gt[i] = gt[gt_i]\n",
    "                    modified_flag = True\n",
    "                    break\n",
    "\n",
    "                pointer += j\n",
    "            if not modified_flag:\n",
    "                # print(f\"No modified\")\n",
    "                gt[i] = np.max(gt) + 1\n",
    "            # print(f\"{gt}\\n\")\n",
    "        simple_gt.append(gt)\n",
    "    return simple_gt\n",
    "\n",
    "def get_cuda_memory(i):\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0\n",
    "\n",
    "    total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "    reserved_memory = torch.cuda.memory_reserved(i)\n",
    "    allocated_memory = torch.cuda.memory_allocated(i)\n",
    "    free_memory = total_memory - (reserved_memory + allocated_memory)\n",
    "    \n",
    "    return free_memory / (1024 ** 2) # MB\n",
    "\n",
    "def flatten_problems(problems_list, squeeze=False):\n",
    "        # [print(f\"{pair=}\") for problem in problems_list for pair in problem]\n",
    "        return [pair.squeeze(0) if squeeze else pair for problem in problems_list for pair in problem]\n",
    "\n",
    "from itertools import islice\n",
    "bert_tokenizer, bert_model = get_tokenizer_model()\n",
    "\n",
    "@memory.cache\n",
    "def get_problem_embeddings(problems, max_input_length, verbose=False):\n",
    "    def get_paragraph_pairs(problem_text, max_input_length):\n",
    "        def get_embeddings(paragraph):\n",
    "            inputs = bert_tokenizer(paragraph, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Move inputs to GPU\n",
    "            inputs = {key: value.to(DEVICE) for key, value in inputs.items()}\n",
    "\n",
    "            # Step 4: Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = bert_model(**inputs)\n",
    "\n",
    "            # The last hidden state contains the embeddings for each token\n",
    "            return outputs.last_hidden_state\n",
    "        # print(problem_text)\n",
    "        paragraph_embeddings = [pad_paragraph(get_embeddings(para[:max_input_length]), max_input_length) for para in problem_text]\n",
    "        # print(f\"{[paras.shape for paras in paragraph_embeddings]=}\")\n",
    "        # print(f\"{[x.shape for x in paragraph_embeddings]}\")\n",
    "        # print(f\"{len(paragraph_embeddings)=}\")\n",
    "        pairs = itertools.combinations(paragraph_embeddings, 2)\n",
    "        return [torch.flatten(torch.stack(pair, dim=2), start_dim=1, end_dim=2) for pair in pairs]\n",
    "    if verbose:\n",
    "        return [get_paragraph_pairs(problem_text, max_input_length) for problem_text in tqdm(problems)]\n",
    "    else:\n",
    "        return [get_paragraph_pairs(problem_text, max_input_length) for problem_text in problems]\n",
    "\n",
    "class Pan21PyDataset(PyDataset):\n",
    "    def __init__(self, x_path, y_path, max_input_length=256, batch_size=128,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.x_path = x_path\n",
    "        y = read_ground_truth_files(y_path)\n",
    "        self.task_3_y = get_task_3_ground_truth(get_simple_ground_truth(y, range(1, len(y)+1)))\n",
    "        self.task_3_lens = [len(problem) for problem in self.task_3_y]\n",
    "        self.num_problems = len(self.task_3_y)\n",
    "        self.pair_end_index = np.cumsum(self.task_3_lens)\n",
    "        self.num_pairs = np.sum(self.task_3_lens)\n",
    "\n",
    "        self.max_input_length = max_input_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of batches.\n",
    "        return math.ceil(self.num_pairs / self.batch_size)\n",
    "\n",
    "    def get_data(self, low_problem_idx, high_problem_idx, low_idx, high_idx):\n",
    "        # print(f\"{low_problem_idx=} {high_problem_idx=} {low_idx=} {high_idx=}\")\n",
    "        if high_problem_idx < self.num_problems:\n",
    "            high_problem_idx += 1\n",
    "            # print(f\"{self.x_path=}\")\n",
    "            embeddings = get_problem_embeddings(read_problem_files(self.x_path, low_problem_idx, high_problem_idx), self.max_input_length)\n",
    "\n",
    "            batch_x = np.array(flatten_problems(embeddings, squeeze=True)[low_idx:high_idx])\n",
    "            batch_y = np.array(flatten_problems(self.task_3_y[low_problem_idx:high_problem_idx])[low_idx:high_idx])\n",
    "        else:\n",
    "            embeddings = get_problem_embeddings(read_problem_files(self.x_path, low_problem_idx, high_problem_idx+1), self.max_input_length)\n",
    "\n",
    "            batch_x = np.array(flatten_problems(embeddings, squeeze=True)[low_idx:high_idx])\n",
    "            batch_y = np.array(flatten_problems(self.task_3_y[low_problem_idx])[low_idx:high_idx])\n",
    "\n",
    "        # print(f\"{self.task_3_y[low_problem_idx:high_problem_idx]=}\")\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"{idx=}\")\n",
    "        low = idx * self.batch_size\n",
    "        # Cap upper bound at array length; the last batch may be smaller\n",
    "        # if the total number of items is not a multiple of batch size.\n",
    "        high = min(low + self.batch_size, self.num_pairs)\n",
    "        # print(f\"{low=} {high=}\")\n",
    "\n",
    "        low_problem_index = 0\n",
    "        high_problem_index = 0\n",
    "        low_index_within_problem = 0\n",
    "        high_index_within_problem = 0\n",
    "        low_found = False\n",
    "        for problem_num, (end, num_pairs_in_problem) in enumerate(zip(self.pair_end_index, self.task_3_lens)):\n",
    "            start = end - num_pairs_in_problem\n",
    "            if not low_found:\n",
    "                if end > low:\n",
    "                    low_problem_index = problem_num\n",
    "                    low_index_within_problem = low - start\n",
    "                    low_found = True\n",
    "            if end >= high:\n",
    "                high_problem_index = problem_num\n",
    "                high_index_within_problem = low_index_within_problem + self.batch_size\n",
    "                break\n",
    "\n",
    "        batch_x, batch_y = self.get_data(low_problem_index, high_problem_index, low_index_within_problem, high_index_within_problem)\n",
    "\n",
    "        assert batch_x.shape[0] != 0, f\"{idx=}: Dimension is 0 {low_problem_index}, {high_problem_index}, {low_index_within_problem}, {high_index_within_problem}\"\n",
    "        assert batch_x.shape[0] == batch_y.shape[0], f\"{idx=}: Dimension mismatch {batch_x.shape=} {batch_y.shape=}\"\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "class Pan21FourierDataset(Pan21PyDataset):\n",
    "    def __init__(self, x_set, y_set, file_path, batch_size=32, num_fourier_features=512, **kwargs):\n",
    "        super().__init__(x_set, y_set, file_path, batch_size=batch_size, **kwargs)\n",
    "        self.num_fourier_features = num_fourier_features\n",
    "\n",
    "    def __getitem__(self, idx, force_compute=False):\n",
    "        idx_path = self.file_path / \"fourier\" / f\"{idx}.npz\"\n",
    "        return Pan21FourierDataset.__getitem__helper(idx_path, idx, num_fourier_features=self.num_fourier_features, force_compute=force_compute)\n",
    "\n",
    "    def __getitem__helper(idx_path, idx, num_fourier_features, force_compute=False):\n",
    "        # idx_path = file_path / \"fourier\" / f\"{idx}.npz\"\n",
    "        # print(f\"Pan21FourierDataset {idx_path=}\")\n",
    "\n",
    "        # batch_x, batch_y = super().__getitem__(idx, force_compute)\n",
    "        # print(f'{idx_path.parent / \"..\" / f\"{idx}.npz\"=}')\n",
    "        embed_file = np.load(idx_path.parent / \"..\" / f\"{idx}.npz\")\n",
    "        batch_x = embed_file['batch_x']\n",
    "        batch_y = embed_file['batch_y']\n",
    "        \n",
    "        if num_fourier_features > 0:\n",
    "            new_batch_x = batch_x.copy()\n",
    "            \n",
    "            num_features = len(batch_x[0])\n",
    "            # print(f\"{num_features=}\")\n",
    "            # 0:x will be BERT embeddings for paragraph 1\n",
    "            # x:length/2 will be fourier features for paragraph 1\n",
    "            num_non_fourier_features = (num_features - num_fourier_features) // 2\n",
    "            para1_fourier_features_low, para1_fourier_features_high = num_non_fourier_features, num_features // 2\n",
    "            para2_fourier_features_low, para2_fourier_features_high = num_features // 2 + num_non_fourier_features , num_features\n",
    "\n",
    "            # print(f\"{para1_fourier_features_low=} {para1_fourier_features_high=}\")\n",
    "            # print(f\"{para2_fourier_features_low=} {para2_fourier_features_high=}\")\n",
    "\n",
    "            if force_compute or not idx_path.exists():\n",
    "                para1_end = num_features//2\n",
    "                for i, x in enumerate(batch_x):\n",
    "                    para1_fft = np.real(np.fft.fft(x[:para1_end], axis=1))\n",
    "                    para2_fft = np.real(np.fft.fft(x[para1_end:], axis=1))\n",
    "\n",
    "                    new_batch_x[i, para1_fourier_features_low:para1_fourier_features_high] = para1_fft[:num_fourier_features//2]\n",
    "                    new_batch_x[i, para2_fourier_features_low:para2_fourier_features_high] = para2_fft[:num_fourier_features//2]\n",
    "            else:\n",
    "                npzfile = np.load(idx_path)\n",
    "                fourier_batch_x = npzfile['fourier_batch_x']\n",
    "\n",
    "                _,j,_ = fourier_batch_x.shape\n",
    "\n",
    "                new_batch_x[:, para1_fourier_features_low:para1_fourier_features_high, :] = fourier_batch_x[:, :num_fourier_features//2, :]\n",
    "                new_batch_x[:, para2_fourier_features_low:para2_fourier_features_high, :] = fourier_batch_x[:, j//2:(j+num_fourier_features)//2, :]\n",
    "\n",
    "            return new_batch_x, batch_y\n",
    "        else:\n",
    "            return batch_x, batch_y\n",
    "    \n",
    "    def save_file(idx_path, idx, num_fourier_features, force_compute=False, compress=False):\n",
    "        fourier_batch_x, _ = Pan21FourierDataset.__getitem__helper(idx_path, idx, num_fourier_features, force_compute)\n",
    "        if compress:\n",
    "            np.savez_compressed(idx_path, fourier_batch_x=fourier_batch_x)\n",
    "        else:\n",
    "            np.savez(idx_path, fourier_batch_x=fourier_batch_x)\n",
    "\n",
    "    def to_file(self, overwrite=False, compress=False):\n",
    "        fourier_file_path = self.file_path / \"fourier\"\n",
    "        fourier_file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        n_jobs = os.cpu_count() // 2\n",
    "        # print(f\"{n_jobs=} {free_memory=} {os.cpu_count()=}\")\n",
    "        args_for_jobs = []\n",
    "        for i in range(len(self)):\n",
    "            idx_path = self.file_path / \"fourier\" / f\"{i}.npz\"\n",
    "            if not idx_path.exists() or overwrite:\n",
    "                args_for_jobs.append((idx_path, i, 512, True, compress))\n",
    "        \n",
    "        with parallel_config(backend='threading', n_jobs=n_jobs):\n",
    "            Parallel()(delayed(Pan21FourierDataset.save_file)(*args) for args in tqdm(args_for_jobs))\n",
    "\n",
    "import gc\n",
    "\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "# from cupyx.scipy.signal import lfilter\n",
    "\n",
    "class Pan21FourierFilterDataset(Pan21PyDataset):\n",
    "    def __init__(self, x_set, y_set,  cutoff_frequencies, **kwargs):\n",
    "        super().__init__(x_set, y_set, **kwargs)\n",
    "        \n",
    "        order = 5\n",
    "        # clamp freqencies\n",
    "        cutoff_frequencies = [max(cutoff_frequencies[0], .001), min(cutoff_frequencies[1], .999)]\n",
    "        \n",
    "        self.i, self.u = butter(order, cutoff_frequencies, btype='bandstop')\n",
    "    \n",
    "    def __getitem__(self, idx, force_compute=False):\n",
    "        batch_x, batch_y = super().__getitem__(idx)\n",
    "\n",
    "        _, j, _ = batch_x.shape\n",
    "        batch_x[:, :j//2, :] = lfilter(self.i, self.u, batch_x[:, :j//2, :])\n",
    "        batch_x[:, j//2:, :] = lfilter(self.i, self.u, batch_x[:, j//2:, :])\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e309e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 0.5, (1, 3): 0.3, (1, 4): 0.4, (2, 3): 0.7, (2, 4): 0.2, (3, 4): 0.1}\n",
      "{'authors': 2, 'structure': [999], 'site': 'googole.com', 'multi-author': True, 'changes': [1, 1, 0], 'paragraph-authors': [1, 2, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "def map_predictions_to_json(predictions, threshold=0.5):\n",
    "    # Number of paragraphs inferred from the triangular number formula: n(n-1)/2 = len(predictions)\n",
    "    # Solving for n gives us n = 3 for len(predictions) = 3\n",
    "\n",
    "    num_paragraphs = int((1 + math.sqrt(1 + 8 * len(predictions))) / 2)\n",
    "    \n",
    "    # Initialize authorship\n",
    "    authors = list(range(1, num_paragraphs + 1))\n",
    "    \n",
    "    # Parse predictions into a matrix (or map) for easier access\n",
    "    prediction_map = {}\n",
    "    index = 0\n",
    "    for i in range(1, num_paragraphs):\n",
    "        for j in range(i + 1, num_paragraphs + 1):\n",
    "            prediction_map[(i, j)] = predictions[index]\n",
    "            index += 1\n",
    "    print(prediction_map)\n",
    "    # Clustering logic based on predictions\n",
    "    for i in range(1, num_paragraphs):\n",
    "        for j in range(i + 1, num_paragraphs + 1):\n",
    "            if prediction_map[(i, j)] < threshold:\n",
    "                # Assign the same author ID to paragraphs i and j if they are likely the same author\n",
    "                authors[j-1] = authors[i-1]\n",
    "    \n",
    "    # Set of unique authors to count different authors\n",
    "    unique_authors = set(authors)\n",
    "    \n",
    "    # Construct the JSON object\n",
    "    data = {\n",
    "        \"authors\": len(unique_authors),\n",
    "        \"structure\": [999],  # Placeholder or specific requirement\n",
    "        \"site\": \"googole.com\",\n",
    "        \"multi-author\": len(unique_authors) > 1,\n",
    "        \"changes\": [int(authors[i] != authors[i + 1]) for i in range(num_paragraphs - 1)],\n",
    "        \"paragraph-authors\": authors\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Example usage with the given array of probabilities\n",
    "predictions = [0.5, 0.3, 0.4, 0.7, 0.2, 0.1]  # As described for pairs (1-2, 1-3, 2-3)\n",
    "result_json = map_predictions_to_json(predictions)\n",
    "print(result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804996a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys \n",
    "sys.path.append(r'D:/FER/TAR/Prj/tar_project/evaluation') \n",
    "\n",
    "import evaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions_folders = [\n",
    "    \"D:/FER/TAR/Prj/tar_project/Models/0_2024_06_01-11_44_28_AM.keras\",\n",
    "    \"D:/FER/TAR/Prj/tar_project/Models/128_2024_06_01-08_57_22_PM.keras\",\n",
    "    \"D:/FER/TAR/Prj/tar_project/Models/256_2024_06_02-04_04_58_AM.keras\",\n",
    "    \"D:/FER/TAR/Prj/tar_project/Models/512_2024_06_02-01_36_02_PM.keras\"\n",
    "]\n",
    "truth_folder = \"D:/FER/TAR/Prj/tar_project/pan21/validation\"\n",
    "\n",
    "# Read ground truth\n",
    "truth = read_ground_truth_files(truth_folder)\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "task1_scores = []\n",
    "task2_scores = []\n",
    "task3_scores = []\n",
    "\n",
    "val_ds = Pan21PyDataset(\"pan21/validation\", \"pan21/validation\")\n",
    "nums_of_pars = val_ds.task_3_lens\n",
    "#print(nums_of_pars)\n",
    "num_of_pairs = [(pars * (pars - 1)) / 2 for pars in nums_of_pars]\n",
    "print(len(nums_of_pars))\n",
    "stating_indices = np.cumsum(nums_of_pars)\n",
    "print(len(stating_indices))\n",
    "\n",
    "dict_of_jsons_result = {}\n",
    "predictions_array = []\n",
    "\n",
    "# Loop through each model\n",
    "for model_f in predictions_folders:\n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(model_f)\n",
    "    # Generate predictions\n",
    "    predictions = model.predict(val_ds)\n",
    "    predictions_array.append(predictions)\n",
    "    #predictions = np.random.choice([0, 1], size=472*128, p=[.1, .9])\n",
    "    for i, (starting_index, num_of_pair) in enumerate(zip(stating_indices, nums_of_pars)):\n",
    "        problem_id = i + 1\n",
    "        #print(starting_index, num_of_pair)\n",
    "        prediction = predictions[int(starting_index):int(starting_index) + int(num_of_pair)]\n",
    "        result_json = map_predictions_to_json(prediction)\n",
    "        dict_of_jsons_result[f\"problem-{problem_id}\"] = result_json\n",
    "        #print(result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae72c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "2400\n",
      "\u001b[1m341/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m12:32\u001b[0m 6s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\joblib\\memory.py:573: UserWarning: Persisting input arguments took 1.29s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m431/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3:32\u001b[0m 5s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\joblib\\memory.py:573: UserWarning: Persisting input arguments took 0.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m445/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2:18\u001b[0m 5s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\joblib\\memory.py:573: UserWarning: Persisting input arguments took 0.86s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m10s\u001b[0m 5s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\joblib\\memory.py:573: UserWarning: Persisting input arguments took 0.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2352s\u001b[0m 5s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [14095, 14085]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 53\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#print(result_json)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Compute scores for each task\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# print(dict_of_jsons_result)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m task1_result \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mcompute_score_single_predictions(truth, dict_of_jsons_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti-author\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m task2_result \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mcompute_score_multiple_predictions(truth, dict_of_jsons_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchanges\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     54\u001b[0m task3_result \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mcompute_score_multiple_predictions(truth, dict_of_jsons_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraph-authors\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Append scores to the lists\u001b[39;00m\n",
      "File \u001b[1;32mD:\\FER/TAR/Prj/tar_project/evaluation\\evaluator.py:74\u001b[0m, in \u001b[0;36mcompute_score_multiple_predictions\u001b[1;34m(truth, solutions, key, labels)\u001b[0m\n\u001b[0;32m     72\u001b[0m task2_truth, task2_solution \u001b[38;5;241m=\u001b[39m extract_task_results(truth, solutions, key)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# task 2 - lists have to be flattened first\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f1_score(\u001b[38;5;28mlist\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(task2_truth)), \u001b[38;5;28mlist\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(task2_solution)), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1271\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1092\u001b[0m     {\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1119\u001b[0m ):\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \n\u001b[0;32m   1122\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1272\u001b[0m         y_true,\n\u001b[0;32m   1273\u001b[0m         y_pred,\n\u001b[0;32m   1274\u001b[0m         beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1275\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1276\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1277\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1278\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1279\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1280\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1463\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1284\u001b[0m     {\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1313\u001b[0m ):\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;124;03m    0.12...\u001b[39;00m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1463\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1464\u001b[0m         y_true,\n\u001b[0;32m   1465\u001b[0m         y_pred,\n\u001b[0;32m   1466\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[0;32m   1467\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1468\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1469\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1470\u001b[0m         warn_for\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-score\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[0;32m   1471\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1472\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1473\u001b[0m     )\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1767\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1605\u001b[0m \n\u001b[0;32m   1606\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1767\u001b[0m labels \u001b[38;5;241m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1539\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1539\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [14095, 14085]"
     ]
    }
   ],
   "source": [
    "for \n",
    "    # Compute scores for each task\n",
    "    # print(dict_of_jsons_result)\n",
    "    #task1_result = evaluator.compute_score_single_predictions(truth, dict_of_jsons_result, 'multi-author')\n",
    "    #task2_result = evaluator.compute_score_multiple_predictions(truth, dict_of_jsons_result, 'changes', labels=[0, 1])\n",
    "    task3_result = evaluator.compute_score_multiple_predictions(truth, dict_of_jsons_result, 'paragraph-authors', labels=[1, 2, 3, 4])\n",
    "\n",
    "    # Append scores to the lists\n",
    "    task1_scores.append(task1_result)\n",
    "    task2_scores.append(task2_result)\n",
    "    task3_scores.append(task3_result)\n",
    "\n",
    "# Create a DataFrame to store the evaluation results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': predictions_folders,\n",
    "    'Task 1 Score': task1_scores,\n",
    "    'Task 2 Score': task2_scores,\n",
    "    'Task 3 Score': task3_scores\n",
    "})\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Plotting the evaluation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(predictions_folders, task1_scores, label='Task 1')\n",
    "plt.plot(predictions_folders, task2_scores, label='Task 2')\n",
    "plt.plot(predictions_folders, task3_scores, label='Task 3')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Model Evaluation')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fafe3fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3_result = evaluator.compute_score_multiple_predictions(truth, dict_of_jsons_result, 'paragraph-authors', labels=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca70843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90c81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
