{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "def read_problem_files(problem_folder, n=None):\n",
    "    \"\"\"\n",
    "    reads ground truth files into dict\n",
    "    :param truth_folder: path to folder holding ground truth files\n",
    "    :return: dict of ground truth files with problem-id as key and file content as value\n",
    "    \"\"\"\n",
    "    problems = {}\n",
    "    files = itertools.islice(Path(problem_folder).glob('problem-*.txt'), n)\n",
    "    for problem_file in files:\n",
    "        number = problem_file.name[len(\"problem-\") : -len(\".txt\")]\n",
    "        with open(problem_file, 'r', encoding=\"utf8\") as fh:\n",
    "            problems[number] = fh.readlines()\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluator import read_ground_truth_files\n",
    "\n",
    "ground_truth = read_ground_truth_files(\"pan21/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(para1, para2):\n",
    "    embeddings1 = sbert_model.encode(para1, convert_to_numpy=True)\n",
    "    embeddings2 = sbert_model.encode(para2, convert_to_numpy=True)\n",
    "\n",
    "    # Calculate cosine similarity between all sentence pairs\n",
    "    similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "    # Find the indices of the most similar sentence pair\n",
    "    max_similarity_index = divmod(similarity_matrix.argmax(), similarity_matrix.shape[1])\n",
    "\n",
    "    # Get the most similar sentence pair\n",
    "    most_similar_pair = (para1[max_similarity_index[0]], para2[max_similarity_index[1]])\n",
    "\n",
    "    return most_similar_pair, similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = bert_tokenizer.tokenize(sentence)\n",
    "    token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Convert token IDs to tensor\n",
    "    input_ids = torch.tensor([token_ids])\n",
    "\n",
    "    # Get BERT model output\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids)\n",
    "\n",
    "    # Extract word embeddings from BERT model output\n",
    "    return outputs[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compare_sentences(sent1, sent2):\n",
    "    diff = len(sent1) - len(sent2)\n",
    "    embed1 = get_word_embeddings(sent1)\n",
    "    embed2 = get_word_embeddings(sent2)\n",
    "\n",
    "    if diff > 0:\n",
    "        excess = np.sum(embed1[:-diff])/len(sent2)\n",
    "        embed1 = [word + excess for word in embed1[:len(sent2)]]\n",
    "    elif diff < 0:\n",
    "        excess = np.sum(embed2[:-diff], axis=0)/len(sent1)\n",
    "        embed2 = [word + excess for word in embed2[:len(sent1)]]\n",
    "\n",
    "    mean_cos_sim = np.mean([cosine_similarity(w1, w2) for w1, w2 in zip(embed1, embed2)])\n",
    "\n",
    "    return mean_cos_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 7.85434470e-02  1.96468760e-03  3.33422869e-02  8.77521001e-03\n -4.53390777e-02 -4.55802456e-02 -7.99855664e-02  2.06299219e-02\n -4.89074476e-02  1.21216655e-01  4.43624742e-02 -1.71473436e-02\n -7.34435543e-02  1.75592247e-02  7.47129507e-03 -5.92222326e-02\n  4.30323742e-02  1.35327969e-02 -4.57067341e-02 -4.03561480e-02\n -4.45564911e-02 -2.74355561e-02  4.07087915e-02 -4.56368327e-02\n -8.04605894e-03  4.93553914e-02 -4.01860364e-02  4.55614291e-02\n -1.91965618e-03 -4.65979287e-03  3.09489272e-03  5.99665269e-02\n  8.00852664e-03 -8.86062346e-03 -4.91302721e-02  4.52211499e-02\n  8.80126667e-04  2.64338590e-02 -5.85633405e-02 -3.89617234e-02\n -9.33800861e-02  3.04856151e-03  4.27806303e-02  7.59484544e-02\n  5.80741279e-02  6.20034225e-02 -1.62492190e-02 -7.10250214e-02\n -4.57938202e-03 -1.61669217e-02 -5.50461598e-02 -6.02390580e-02\n  9.49239917e-03 -2.60158647e-02  1.05940878e-01  4.16961201e-02\n  1.44326501e-02 -4.01123660e-03 -5.42610418e-03 -9.34219081e-03\n  2.69243550e-02 -7.97608793e-02 -5.06606773e-02  1.87236760e-02\n  7.49365846e-03 -2.96333935e-02  2.77719670e-03  3.36912647e-02\n  7.53014581e-04 -1.88033469e-02  2.33384967e-02  4.69999239e-02\n  1.99357234e-02  1.26560451e-02  5.30578271e-02 -6.74495324e-02\n -3.71372998e-02  2.94899847e-02 -1.68878818e-03  1.33050531e-01\n -1.51929045e-02 -5.38188256e-02 -7.86362514e-02  1.45922247e-02\n  4.23583202e-02  1.87905282e-02 -1.50500266e-02  4.95835356e-02\n  8.02558009e-03 -2.71955431e-02 -1.75129455e-02 -2.10103225e-02\n  6.90583661e-02  7.02152960e-03  2.76244935e-02  2.06379946e-02\n -2.98221726e-02 -3.23068611e-02 -1.10694207e-01  5.57552848e-04\n -7.71440491e-02  4.15511988e-02  3.56166959e-02  1.62933722e-01\n  1.19610112e-02  9.28719118e-02 -3.36066633e-03 -2.67960280e-02\n  4.52782400e-03  1.38919996e-02 -2.59832945e-02  2.32658833e-02\n  7.45013654e-02  1.80887952e-02 -1.03554064e-02  2.45259833e-02\n -1.09235626e-02  5.72465770e-02 -4.35287207e-02  6.75813109e-02\n  3.15894820e-02 -3.43150795e-02  5.34788333e-02  6.86918423e-02\n  4.34934348e-02 -7.91294649e-02 -4.53179149e-04 -3.26011713e-33\n -5.32614440e-03  7.27346390e-02  8.19814354e-02  1.27394041e-02\n -5.71853062e-03 -2.27642860e-02  4.99541163e-02 -1.74720790e-02\n  8.11892450e-02  1.89954340e-02 -2.75897961e-02  4.34273481e-02\n  9.81900189e-03  1.30913891e-02  8.31644312e-02  5.91988862e-02\n  1.00057535e-02  2.72928867e-02  2.16337536e-02 -2.97174919e-02\n  7.40056261e-02 -7.29571432e-02  3.39088991e-04 -9.68732089e-02\n -4.22424003e-02  9.64537486e-02  4.06973250e-02  4.40896116e-02\n  2.75625009e-02 -1.21332463e-02 -7.87702203e-02 -4.69022896e-03\n -6.68891147e-02 -4.55598376e-04  3.21712941e-02  3.68975252e-02\n  1.87761465e-03 -7.53280818e-02  1.78162602e-03 -7.46820075e-03\n  1.58666857e-02  2.80366894e-02 -4.30848114e-02 -2.91757416e-02\n -2.58724429e-02 -3.05009913e-02  9.17648152e-02 -5.93264289e-02\n -1.03117675e-01  3.42186652e-02 -8.67569521e-02 -1.44418003e-03\n  4.55304570e-02 -7.79464990e-02  7.90045932e-02  4.46647592e-03\n  4.48911339e-02 -1.17840664e-02 -5.46260960e-02  7.18143210e-02\n -9.25383195e-02  1.49806133e-02  5.37165888e-02  3.97401713e-02\n -3.47823910e-02  9.38686579e-02 -6.01636432e-02  5.66456653e-02\n -2.15999726e-02 -3.72781721e-03 -3.26290703e-03  2.98855528e-02\n -2.82956418e-02 -3.77835892e-02  4.54343781e-02  3.02150082e-02\n  1.35732079e-02 -8.69066119e-02  6.83113337e-02 -2.32446026e-02\n  5.76125719e-02 -7.47763440e-02 -4.13494594e-02 -2.75478028e-02\n -5.19266203e-02 -4.51220432e-04  4.42005694e-02 -2.92017069e-02\n -2.61825752e-02 -5.87460585e-02 -1.11021794e-01 -6.60078153e-02\n -2.05489714e-03  2.08891705e-02  1.66825689e-02  2.23292407e-33\n -1.09043598e-01 -7.33560696e-02 -9.07503739e-02  8.27418119e-02\n  2.50198524e-02  6.24227412e-02 -1.62544306e-02 -5.05761132e-02\n -1.92256868e-02  1.27760554e-02  1.18429717e-02  2.26155836e-02\n -1.05573326e-01 -1.34549262e-02  1.21301496e-02  3.07270512e-02\n -4.24367711e-02 -2.55128182e-02  1.48091279e-02 -7.61433598e-03\n -5.98534755e-03  4.63716462e-02 -6.02022521e-02  8.41712207e-03\n  5.51153906e-03  2.70036515e-02 -6.03280440e-02 -1.16188645e-01\n -7.35428184e-02  1.06609799e-01  2.57754121e-02 -1.44128818e-02\n -1.87259521e-02 -1.55468415e-02 -2.72010304e-02  5.04146377e-03\n  8.03458467e-02  4.08201702e-02 -1.78139471e-02 -2.00522728e-02\n -4.88092117e-02  5.29215634e-02 -3.11869718e-02 -6.28764406e-02\n -4.98896576e-02  1.03716301e-02  7.82937855e-02  1.52016012e-02\n  2.34473143e-02  2.71274652e-02 -1.19562047e-02 -5.31127155e-02\n  2.58538555e-02 -1.04559183e-01 -1.63469333e-02  3.05774957e-02\n  1.05268536e-02  6.81004077e-02  4.32667807e-02 -2.36933269e-02\n  4.89221402e-02 -1.99437607e-02 -6.51230887e-02  7.91809708e-02\n  7.22391251e-03  3.35722230e-02 -8.63705948e-02  1.35053277e-01\n  3.61997485e-02 -8.89321715e-02 -6.42020255e-02  4.76485267e-02\n  5.69777517e-03 -3.93761955e-02 -4.26181592e-02  4.08178419e-02\n -3.93211143e-03  1.15460670e-02 -2.19038688e-02 -4.50032540e-02\n  2.97856312e-02  2.47805398e-02  5.63917533e-02 -3.52975866e-03\n -1.23554468e-03  3.13900784e-02 -5.01632690e-02 -8.86398405e-02\n  1.52008682e-02 -4.18417938e-02 -1.03961956e-02  5.21899164e-02\n  1.21143639e-01  8.00634176e-02 -1.31234929e-01 -1.96612682e-08\n -1.36951711e-02 -2.64070891e-02  6.54032975e-02 -7.64601678e-02\n -1.49700996e-02  9.22984555e-02 -2.56306231e-02  7.06283981e-03\n -6.01604208e-02  6.87206164e-02  1.07115861e-02 -1.53258638e-02\n -2.11847778e-02  9.87857282e-02  3.19834352e-02  1.13298930e-01\n  4.36197594e-02 -3.36320326e-02 -2.49263011e-02  9.79353637e-02\n  7.68016279e-02  8.86406284e-03 -3.90523858e-03  5.68657555e-03\n -2.08543129e-02 -2.62277406e-02 -3.19395140e-02  1.43050859e-02\n  2.41444632e-02  2.59916726e-02  5.00014378e-03 -8.07576180e-02\n  7.65641853e-02  3.84437181e-02  7.74244741e-02  6.59708902e-02\n  5.34467883e-02 -1.42907137e-02 -5.29293604e-02 -4.89799269e-02\n -1.21562794e-01  3.94784994e-02 -1.55078736e-03  7.63981491e-02\n  1.56554785e-02 -2.41052709e-04 -8.68636072e-02 -1.03422493e-01\n -5.08328490e-02  6.53656423e-02  7.61301592e-02  8.59087333e-02\n -1.93412602e-02 -6.11527637e-02  8.48016813e-02  1.02773510e-01\n  2.16339715e-03 -1.49494642e-02 -1.49114393e-02 -1.80853605e-02\n  2.90354788e-02  1.29992180e-02 -3.12690884e-02  3.69895063e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(paragraphs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     para1, para2 \u001b[38;5;241m=\u001b[39m (paragraphs[i], paragraphs[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m     most_similar_pair, similarity \u001b[38;5;241m=\u001b[39m \u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpara2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     highest_similarity \u001b[38;5;241m=\u001b[39m compare_sentences(\u001b[38;5;241m*\u001b[39mmost_similar_pair)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhighest_similarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mground_truth[problem_num][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchanges\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 16\u001b[0m, in \u001b[0;36mmost_similar\u001b[1;34m(para1, para2)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get the most similar sentence pair\u001b[39;00m\n\u001b[0;32m     14\u001b[0m most_similar_pair \u001b[38;5;241m=\u001b[39m (para1[max_similarity_index[\u001b[38;5;241m0\u001b[39m]], para2[max_similarity_index[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m most_similar_pair, \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmax_similarity_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmax_similarity_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Documents\\croatia\\masters\\semester2\\text_analysis\\tar_project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Documents\\croatia\\masters\\semester2\\text_analysis\\tar_project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1657\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m \n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1657\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Documents\\croatia\\masters\\semester2\\text_analysis\\tar_project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:164\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    155\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    156\u001b[0m         X,\n\u001b[0;32m    157\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    173\u001b[0m         Y,\n\u001b[0;32m    174\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Documents\\croatia\\masters\\semester2\\text_analysis\\tar_project\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1035\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1029\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1030\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m             )\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 7.85434470e-02  1.96468760e-03  3.33422869e-02  8.77521001e-03\n -4.53390777e-02 -4.55802456e-02 -7.99855664e-02  2.06299219e-02\n -4.89074476e-02  1.21216655e-01  4.43624742e-02 -1.71473436e-02\n -7.34435543e-02  1.75592247e-02  7.47129507e-03 -5.92222326e-02\n  4.30323742e-02  1.35327969e-02 -4.57067341e-02 -4.03561480e-02\n -4.45564911e-02 -2.74355561e-02  4.07087915e-02 -4.56368327e-02\n -8.04605894e-03  4.93553914e-02 -4.01860364e-02  4.55614291e-02\n -1.91965618e-03 -4.65979287e-03  3.09489272e-03  5.99665269e-02\n  8.00852664e-03 -8.86062346e-03 -4.91302721e-02  4.52211499e-02\n  8.80126667e-04  2.64338590e-02 -5.85633405e-02 -3.89617234e-02\n -9.33800861e-02  3.04856151e-03  4.27806303e-02  7.59484544e-02\n  5.80741279e-02  6.20034225e-02 -1.62492190e-02 -7.10250214e-02\n -4.57938202e-03 -1.61669217e-02 -5.50461598e-02 -6.02390580e-02\n  9.49239917e-03 -2.60158647e-02  1.05940878e-01  4.16961201e-02\n  1.44326501e-02 -4.01123660e-03 -5.42610418e-03 -9.34219081e-03\n  2.69243550e-02 -7.97608793e-02 -5.06606773e-02  1.87236760e-02\n  7.49365846e-03 -2.96333935e-02  2.77719670e-03  3.36912647e-02\n  7.53014581e-04 -1.88033469e-02  2.33384967e-02  4.69999239e-02\n  1.99357234e-02  1.26560451e-02  5.30578271e-02 -6.74495324e-02\n -3.71372998e-02  2.94899847e-02 -1.68878818e-03  1.33050531e-01\n -1.51929045e-02 -5.38188256e-02 -7.86362514e-02  1.45922247e-02\n  4.23583202e-02  1.87905282e-02 -1.50500266e-02  4.95835356e-02\n  8.02558009e-03 -2.71955431e-02 -1.75129455e-02 -2.10103225e-02\n  6.90583661e-02  7.02152960e-03  2.76244935e-02  2.06379946e-02\n -2.98221726e-02 -3.23068611e-02 -1.10694207e-01  5.57552848e-04\n -7.71440491e-02  4.15511988e-02  3.56166959e-02  1.62933722e-01\n  1.19610112e-02  9.28719118e-02 -3.36066633e-03 -2.67960280e-02\n  4.52782400e-03  1.38919996e-02 -2.59832945e-02  2.32658833e-02\n  7.45013654e-02  1.80887952e-02 -1.03554064e-02  2.45259833e-02\n -1.09235626e-02  5.72465770e-02 -4.35287207e-02  6.75813109e-02\n  3.15894820e-02 -3.43150795e-02  5.34788333e-02  6.86918423e-02\n  4.34934348e-02 -7.91294649e-02 -4.53179149e-04 -3.26011713e-33\n -5.32614440e-03  7.27346390e-02  8.19814354e-02  1.27394041e-02\n -5.71853062e-03 -2.27642860e-02  4.99541163e-02 -1.74720790e-02\n  8.11892450e-02  1.89954340e-02 -2.75897961e-02  4.34273481e-02\n  9.81900189e-03  1.30913891e-02  8.31644312e-02  5.91988862e-02\n  1.00057535e-02  2.72928867e-02  2.16337536e-02 -2.97174919e-02\n  7.40056261e-02 -7.29571432e-02  3.39088991e-04 -9.68732089e-02\n -4.22424003e-02  9.64537486e-02  4.06973250e-02  4.40896116e-02\n  2.75625009e-02 -1.21332463e-02 -7.87702203e-02 -4.69022896e-03\n -6.68891147e-02 -4.55598376e-04  3.21712941e-02  3.68975252e-02\n  1.87761465e-03 -7.53280818e-02  1.78162602e-03 -7.46820075e-03\n  1.58666857e-02  2.80366894e-02 -4.30848114e-02 -2.91757416e-02\n -2.58724429e-02 -3.05009913e-02  9.17648152e-02 -5.93264289e-02\n -1.03117675e-01  3.42186652e-02 -8.67569521e-02 -1.44418003e-03\n  4.55304570e-02 -7.79464990e-02  7.90045932e-02  4.46647592e-03\n  4.48911339e-02 -1.17840664e-02 -5.46260960e-02  7.18143210e-02\n -9.25383195e-02  1.49806133e-02  5.37165888e-02  3.97401713e-02\n -3.47823910e-02  9.38686579e-02 -6.01636432e-02  5.66456653e-02\n -2.15999726e-02 -3.72781721e-03 -3.26290703e-03  2.98855528e-02\n -2.82956418e-02 -3.77835892e-02  4.54343781e-02  3.02150082e-02\n  1.35732079e-02 -8.69066119e-02  6.83113337e-02 -2.32446026e-02\n  5.76125719e-02 -7.47763440e-02 -4.13494594e-02 -2.75478028e-02\n -5.19266203e-02 -4.51220432e-04  4.42005694e-02 -2.92017069e-02\n -2.61825752e-02 -5.87460585e-02 -1.11021794e-01 -6.60078153e-02\n -2.05489714e-03  2.08891705e-02  1.66825689e-02  2.23292407e-33\n -1.09043598e-01 -7.33560696e-02 -9.07503739e-02  8.27418119e-02\n  2.50198524e-02  6.24227412e-02 -1.62544306e-02 -5.05761132e-02\n -1.92256868e-02  1.27760554e-02  1.18429717e-02  2.26155836e-02\n -1.05573326e-01 -1.34549262e-02  1.21301496e-02  3.07270512e-02\n -4.24367711e-02 -2.55128182e-02  1.48091279e-02 -7.61433598e-03\n -5.98534755e-03  4.63716462e-02 -6.02022521e-02  8.41712207e-03\n  5.51153906e-03  2.70036515e-02 -6.03280440e-02 -1.16188645e-01\n -7.35428184e-02  1.06609799e-01  2.57754121e-02 -1.44128818e-02\n -1.87259521e-02 -1.55468415e-02 -2.72010304e-02  5.04146377e-03\n  8.03458467e-02  4.08201702e-02 -1.78139471e-02 -2.00522728e-02\n -4.88092117e-02  5.29215634e-02 -3.11869718e-02 -6.28764406e-02\n -4.98896576e-02  1.03716301e-02  7.82937855e-02  1.52016012e-02\n  2.34473143e-02  2.71274652e-02 -1.19562047e-02 -5.31127155e-02\n  2.58538555e-02 -1.04559183e-01 -1.63469333e-02  3.05774957e-02\n  1.05268536e-02  6.81004077e-02  4.32667807e-02 -2.36933269e-02\n  4.89221402e-02 -1.99437607e-02 -6.51230887e-02  7.91809708e-02\n  7.22391251e-03  3.35722230e-02 -8.63705948e-02  1.35053277e-01\n  3.61997485e-02 -8.89321715e-02 -6.42020255e-02  4.76485267e-02\n  5.69777517e-03 -3.93761955e-02 -4.26181592e-02  4.08178419e-02\n -3.93211143e-03  1.15460670e-02 -2.19038688e-02 -4.50032540e-02\n  2.97856312e-02  2.47805398e-02  5.63917533e-02 -3.52975866e-03\n -1.23554468e-03  3.13900784e-02 -5.01632690e-02 -8.86398405e-02\n  1.52008682e-02 -4.18417938e-02 -1.03961956e-02  5.21899164e-02\n  1.21143639e-01  8.00634176e-02 -1.31234929e-01 -1.96612682e-08\n -1.36951711e-02 -2.64070891e-02  6.54032975e-02 -7.64601678e-02\n -1.49700996e-02  9.22984555e-02 -2.56306231e-02  7.06283981e-03\n -6.01604208e-02  6.87206164e-02  1.07115861e-02 -1.53258638e-02\n -2.11847778e-02  9.87857282e-02  3.19834352e-02  1.13298930e-01\n  4.36197594e-02 -3.36320326e-02 -2.49263011e-02  9.79353637e-02\n  7.68016279e-02  8.86406284e-03 -3.90523858e-03  5.68657555e-03\n -2.08543129e-02 -2.62277406e-02 -3.19395140e-02  1.43050859e-02\n  2.41444632e-02  2.59916726e-02  5.00014378e-03 -8.07576180e-02\n  7.65641853e-02  3.84437181e-02  7.74244741e-02  6.59708902e-02\n  5.34467883e-02 -1.42907137e-02 -5.29293604e-02 -4.89799269e-02\n -1.21562794e-01  3.94784994e-02 -1.55078736e-03  7.63981491e-02\n  1.56554785e-02 -2.41052709e-04 -8.68636072e-02 -1.03422493e-01\n -5.08328490e-02  6.53656423e-02  7.61301592e-02  8.59087333e-02\n -1.93412602e-02 -6.11527637e-02  8.48016813e-02  1.02773510e-01\n  2.16339715e-03 -1.49494642e-02 -1.49114393e-02 -1.80853605e-02\n  2.90354788e-02  1.29992180e-02 -3.12690884e-02  3.69895063e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Our sentences to encode\n",
    "problems = read_problem_files(\"pan21/train\", n=4)\n",
    "\n",
    "for num, problem in problems.items():\n",
    "    problem_num = f\"problem-{num}\"\n",
    "    paragraphs = [[sent.text for sent in nlp(paragraph).sents] for paragraph in problem]\n",
    "    for i in range(len(paragraphs) - 1):\n",
    "        para1, para2 = (paragraphs[i], paragraphs[i + 1])\n",
    "\n",
    "        most_similar_pair, similarity = most_similar(para1, para2)\n",
    "        highest_similarity = compare_sentences(*most_similar_pair)\n",
    "        print(f\"{i}, {i+1}: {similarity:.2f} {highest_similarity:.2f} {ground_truth[problem_num]['changes'][i]}\")\n",
    "    print(f\"{ground_truth[problem_num]=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
