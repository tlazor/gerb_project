{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "def read_problem_files(problem_folder, n=None):\n",
    "    \"\"\"\n",
    "    reads ground truth files into dict\n",
    "    :param truth_folder: path to folder holding ground truth files\n",
    "    :return: dict of ground truth files with problem-id as key and file content as value\n",
    "    \"\"\"\n",
    "    problems = {}\n",
    "    files = itertools.islice(Path(problem_folder).glob('problem-*.txt'), n)\n",
    "    for problem_file in files:\n",
    "        number = problem_file.name[len(\"problem-\") : -len(\".txt\")]\n",
    "        with open(problem_file, 'r', encoding=\"utf8\") as fh:\n",
    "            problems[number] = fh.readlines()\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluator import read_ground_truth_files\n",
    "\n",
    "ground_truth = read_ground_truth_files(\"pan21/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import unravel_index\n",
    "\n",
    "def most_similar(para1, para2):\n",
    "    embeddings1 = sbert_model.encode(para1, convert_to_numpy=True)\n",
    "    embeddings2 = sbert_model.encode(para2, convert_to_numpy=True)\n",
    "\n",
    "    # Calculate cosine similarity between all sentence pairs\n",
    "    similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "    # Find the indices of the most similar sentence pair\n",
    "    \n",
    "    max_similarity = similarity_matrix.argmax()\n",
    "    idx_1, idx_2 = unravel_index(max_similarity, similarity_matrix.shape)\n",
    "\n",
    "    # Get the most similar sentence pair\n",
    "    most_similar_pair = (para1[idx_1], para2[idx_2])\n",
    "\n",
    "    return most_similar_pair, max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = bert_tokenizer.tokenize(sentence)\n",
    "    token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Convert token IDs to tensor\n",
    "    input_ids = torch.tensor([token_ids])\n",
    "\n",
    "    # Get BERT model output\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids)\n",
    "\n",
    "    # Extract word embeddings from BERT model output\n",
    "    return outputs[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compare_sentences(sent1, sent2):\n",
    "    diff = len(sent1) - len(sent2)\n",
    "    embed1 = get_word_embeddings(sent1)\n",
    "    embed2 = get_word_embeddings(sent2)\n",
    "\n",
    "    if diff > 0:\n",
    "        excess = np.sum(embed1[:-diff])/len(sent2)\n",
    "        embed1 = [word + excess for word in embed1[:len(sent2)]]\n",
    "    elif diff < 0:\n",
    "        excess = np.sum(embed2[:-diff], axis=0)/len(sent1)\n",
    "        embed2 = [word + excess for word in embed2[:len(sent1)]]\n",
    "\n",
    "    mean_cos_sim = np.mean([cosine_similarity(w1, w2) for w1, w2 in zip(embed1, embed2)])\n",
    "\n",
    "    return mean_cos_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1: 2.00 0.39 1\n",
      "1, 2: 3.00 0.37 0\n",
      "2, 3: 3.00 0.37 0\n",
      "3, 4: 1.00 0.38 0\n",
      "4, 5: 7.00 0.51 1\n",
      "5, 6: 6.00 0.44 1\n",
      "6, 7: 1.00 0.33 0\n",
      "ground_truth[problem_num]={'authors': 3, 'structure': [12611, 5862, 1424, 5862], 'site': 'gamedev.stackexchange.com.7z', 'multi-author': 1, 'changes': [1, 0, 0, 0, 1, 1, 0], 'paragraph-authors': [1, 2, 2, 2, 2, 3, 2, 2]}\n",
      "0, 1: 0.00 0.40 0\n",
      "1, 2: 0.00 0.35 0\n",
      "2, 3: 2.00 0.38 1\n",
      "3, 4: 4.00 0.38 1\n",
      "ground_truth[problem_num]={'authors': 2, 'structure': [2062, 2310, 2062], 'site': 'networkengineering.stackexchange.com.7z', 'multi-author': 1, 'changes': [0, 0, 1, 1], 'paragraph-authors': [1, 1, 1, 2, 1]}\n",
      "0, 1: 1.00 0.46 1\n",
      "1, 2: 0.00 0.50 1\n",
      "2, 3: 2.00 0.46 1\n",
      "3, 4: 0.00 0.37 0\n",
      "4, 5: 0.00 0.40 0\n",
      "5, 6: 0.00 0.37 1\n",
      "6, 7: 1.00 0.35 1\n",
      "7, 8: 2.00 0.36 1\n",
      "8, 9: 1.00 0.32 1\n",
      "ground_truth[problem_num]={'authors': 4, 'structure': [13726, 19152, 13726, 6436, 19152, 2477, 6436, 2477], 'site': 'serverfault.com.7z', 'multi-author': 1, 'changes': [1, 1, 1, 0, 0, 1, 1, 1, 1], 'paragraph-authors': [1, 2, 1, 3, 3, 3, 2, 4, 3, 4]}\n",
      "0, 1: 1.00 0.34 1\n",
      "1, 2: 2.00 0.36 1\n",
      "2, 3: 1.00 0.39 1\n",
      "3, 4: 6.00 0.34 0\n",
      "4, 5: 2.00 0.33 1\n",
      "5, 6: 0.00 0.32 1\n",
      "6, 7: 1.00 0.27 1\n",
      "7, 8: 3.00 0.33 0\n",
      "8, 9: 0.00 0.44 1\n",
      "ground_truth[problem_num]={'authors': 4, 'structure': [9278, 55514, 15677, 55514, 9278, 8625, 55514, 15677], 'site': 'serverfault.com.7z', 'multi-author': 1, 'changes': [1, 1, 1, 0, 1, 1, 1, 0, 1], 'paragraph-authors': [1, 2, 3, 2, 2, 1, 4, 2, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "# Our sentences to encode\n",
    "problems = read_problem_files(\"pan21/train\", n=4)\n",
    "\n",
    "for num, problem in problems.items():\n",
    "    problem_num = f\"problem-{num}\"\n",
    "    paragraphs = [[sent.text for sent in nlp(paragraph).sents] for paragraph in problem]\n",
    "    for i in range(len(paragraphs) - 1):\n",
    "        para1, para2 = (paragraphs[i], paragraphs[i + 1])\n",
    "\n",
    "        most_similar_pair, similarity = most_similar(para1, para2)\n",
    "        highest_similarity = compare_sentences(*most_similar_pair)\n",
    "        print(f\"{i}, {i+1}: {similarity:.2f} {highest_similarity:.2f} {ground_truth[problem_num]['changes'][i]}\")\n",
    "    print(f\"{ground_truth[problem_num]=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
