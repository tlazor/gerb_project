HTTP was originally a stateless protocol: each HTTP request would use a separate TCP connection.  This is why we need cookies (or something similar) to implement sessions.
I like the excellent wikipedia page on TCP. It clearly shows what happens with the port number. It, by chance, also contains a helpful chapter on ressource usage:
From the perspective of TCP, there is no client or server (client/server is an application concept that is off-topic here). TCP establishes a connection between peers, and both peers can send and receive on the connection until either peer closes it, or it times out from inactivity.
As others have pointed out, TCP absolutely allows a connection to stay open for any amount of time, exchanging any number of "messages" in either direction during that time.  That said, it's ultimately up to the applications (both client and server) to determine if that capability is utilized.
You are the own judge here; check your application and try to find out if you are connecting to Kafka with a separate request each time (maybe through some REST API proxy). If you do so, and you have huge numbers of clients, then you are certainly in danger.
If you only have a handful of clients, less than 65k-ish, and/or you keep a single connection to your Kafka browser, then you'll be fine.
What may be confusing the situation is that some applications, e.g. browsers, will open multiple connections in order to simultaneously load things like elements of a web page.
So, TCP is able to run out of ports, if a client opens up a lot of TCP connections in parallel without closing them. The problem only occurs client-side, and it does not matter if the connections are with the same or different server IP addresses or server ports.
Finally, as others have mentioned, TCP is stream-oriented.  There is no framing whatsoever.  Just because one peer wrote the data a particular way (e.g. 1 1024 byte write call following by 2 256 byte write calls), that does not guarantee that the other peer will read it in the same size chunks (e.g. it might get all 1536 bytes in one read call).  Thus if you are sending multiple "messages" over raw TCP sockets, you have to provide your own framing protocol to delineate the different messages.  While there certainly are simple ways to do this, it's generally ill-advised as there are many protocols built on top of TCP to solve this problem.  For further discussion, consult this: https://blog.stephencleary.com/2009/04/message-framing.html
In your setting, you seem to have one application which takes in many client requests (these could be individual TCP requests, as maybe your clients use this to log some events to your application and don't hold the TCP channel open inbetween), and create a new internal request to your Kafka broker (which very easily could be individual TCP connections if you chose to implement them like this). In this case, the bottleneck (in terms of ressources, not performance) would be if you manage to get huge numbers of requests at the same time from your clients (no problem for you, as on the server side you only need one port for all of them), and you open up a huge number of forward requests to your Kafka, and Kafka is not able to process them fast enough, ending up with you having more than 16bits worth of connections open concurrently.
Likewise, the server must be smart enough to keep the socket open on its side and wait for more data.  Like the client, it has the option of closing the socket at which point a fault-tolerant client wishing to send more data will have no choice but to open a new socket, leading to the same problem.
In short, TCP uses up one very finite ressource, which is the number of ports on the client (which is limited by the size of the port field in the TCP header, 16 bits).
In order to reuse the existing TCP connection (socket), the client application must keep that socket open and use it when it needs to write more data.  If the client does not do this, but instead discards the old socket and opens a new socket every time it needs one, then it will indeed force a new connection which could cause resource issues on either the client or server if done frequently enough to exhaust either TCP stack's connection pool.
TCP does not open a new connection for every segment it sends, but an application may open multiple TCP connections. Also, when a TCP connection is closed, the TCP port used in the connection is freed, and it is available to be used again. This answer gives some information, and it points you to the RFC for TCP.