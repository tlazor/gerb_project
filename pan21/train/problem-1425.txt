Other answers correctly refer to wattage as being the important measure of how much power you really use, however... many data centers and colo providers (like the two that I use, one in Canada, one in the US) will bill you a flat rate per circuit, measured in amps.
You can buy power bars that show you the amps used by whatever you plug into them. Good ones will let you poll that data by SNMP and you can graph it or whatever.
So it is useful to know the power draw in amps that your equipment will use. A very rough rule of thumb for ball-park estimation would be about 2A per "average" server. But if you need precision, then measure it precisely. Don't rely on ball-park estimates. :)
Amp is a current measure, not power. Although knowing the Voltage of you power source it is trivial to use current values as a measure of the power consumed. W=VA, Watt=Volt*Amper (disregarding the power factor, the phase between this two vectors).
Knowing this you can easily look at your HW power consumption details, it will give you the max Watt consumption, divide it by you electrical Voltage (110 or 200, depending of you location) and you will have how many Apms would consume, in the better case of course! 
When ordering co-location services, a certain amount of power is included, for example 0.25 amps, 0.5 amps, 1 amps, 2 amps, etc. How do I calculate how much a server is going to require?