If you don't have the tags for the questions you mined, you should consider unsupervised methods.  LDA, for example, could identify topics within the questions and important words within those topics 
This exact problem was a kaggle competition sponsored by Facebook. The particular forum thread of interest for you is the one where many of the top competitors explained their methodology, this should provide you with more information than you were probably looking for: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach 
Input: As a dataset I have mined stackoverflow questions, I have tokenized the data set and removed stopwords and punctuation from this data.
In general, it appears that most people treated the problem as a supervised one. Their primary feature was a tf-idf, or unweighted BOW, representations of the text and they ensembled 1000s of single-tag models. Owen, the winner of the competition, noted that the title text was a more powerful feature than the content of the body of the post.
I am trying to predict tags for stackoverflow questions and I am not able to decide which Machine Learning algorithm will be a correct approach for this.
A simpler approach might be to use the tf-idf vectors to compute the K-Nearest-Neighbors of a question; then you can use the tags of the most similar documents (by whatever distance metric does best) to predict the likelihood that question has each tag.
One interesting algorithm that I've once tested is called TopMine: http://web.engr.illinois.edu/~elkishk2/ (under Code and Datasets). It is able to extract bi-grams that could be used as key words and it can also assign them into topics.
You might use the tf-idf representation of a given question, feed it to an SVM or neural net, and use that to predict 0/1 for each tag in your target set.  If there are too many possible classes (tags), it may be tricky to balance your data.