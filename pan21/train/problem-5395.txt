A very useful application of optimal control is found, e.g., in Markov decision processes: a dynamical system is modeled by a Markov chain which can be altered by using some admissible policies. Costs are given for transitions and/or controls and one is usually interested in finding a policy that minimizes the total/average/discounted cost for a finite/infinite temporal horizon. This can be accomplished, e.g., by formulating a suitable Hamilton-Jacobi-Bellman equation for the system and then solving it by means of dynamic programming (many other methods exist depending on the systems).
Hence a natural application is in stochastic optimization settings in which the dynamical system can be modeled as Markovian. A standard reference for optimal control is:
A good area to explore could be the theory of optimal control (i.e., controlling a system while minimizing some given cost function), which was mainly developed by Richard Bellman, together with the dynamic programming paradigm, that is now ubiquitous in computer science.