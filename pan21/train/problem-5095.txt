If I take the time to simulate a cluster using virtual box/hadoop/spark (for my learning experience, not performance), can/should I do the aggregations there and write the results to the distributed data store? 
Since deep learning can not be run on spark, does that mean I would need to copy the aggregated data back out to the local file system to use some of those techniques?
Clearly data set 1 will require joining weather and calendar data to date/time and aggregation across time and location. I know how to stuff it all into a MySQL database and do the aggregations and joins there, but I have been reading about spark and wondering.
I expect to be using python for file processing, scilearn, pylearn2 and word2vec for general exploration and training. R for getting a taste of the language.
1st project has 10,000 time series with 24 float data points each day for 10 years (876 million points). I will be creating a bunch of calendar and weather features for the data, then trying to forecast using a variety of machine learning techniques.
2nd is some 13 million rows of text data (several paragraphs for each row)  for classification. (currently in solr database)
In general I am looking for appropriate applications and insight into data flow from app to app to help me get to the part where I start trying various ML techniques.
For data set 2, I want to run the word2vec algorithm as found in the kaggle tutorial https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors. In that example, that is a deeplearning method, so I should just leave the data in the solr.. right?
Actually deep learning can be run in spark using h2o sparkling water feature.Also you can use h2o.deeplearning to run deeplearning on your data in cluster or single node.Spark is good for munging the data in cluster as it does so distributedly but in memory,otherwise h2o has limited functions for data munging and that too it can't distribute data munging.
Regarding data storage, from my experience CSV files or similar work best to load the data into both Pandas and Spark. There are Spark connectors for databases (e.g. Cassandra), but I am not sure about MySQL and Solr. For processed data and intermediate results (e.g. models) I would always use Python's serialization framework pickle, so you can directly get the object instances back into Python without the need to parse file formats and reinstantiate objects.