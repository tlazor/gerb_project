A single-port Mellanox 56Gbps card (8x PCIe 3.0) like the MCB191A-FCAT is less than 700 bucks, and a 2-meter copper direct attach cable is like 80 dollars.
Performance will generally blow 10GbE out of the water in all use cases. There are no downsides, unless you need to access the server from lots of different clients that can't all use InfiniBand (and even then, Mellanox' switches can bridge 10GbE and 40GbE to IB, but that is a bit more of an investment, of course).
Something of a tangent, but consider using InfiniBand instead of dual 10GbE links. You can get 56Gbps Infiniband cards quite cheap, or 100Gbps ones for not too much more, and on Linux it's easy to use NFS with RDMA over IB, which will give you extremely low latency and near theoretical line speed throughput (if your underlying storage can handle it). You don't need a switch, just two InfiniBand cards and a direct attach cable (or an InfiniBand fiber cable if you need longer distances).
For reference I recently built a small 'log capture' solution using four regular dual-xeon servers (HPE DL380 Gen9s in this case), each with 6 x NVMe drives, I used IP over Infiniband but those 25/40Gbps NICs would be the same and we're capturing up to 8GBps per server - works a treat.
We have pegged a 10G NIC dumping data to a Gluster cluster over their fuse client. It takes a little tuning bit you wouldn't believe the performance it can achieve since 3.0. 
25Gbps Ethernet is already borderline-mainstream while PCIe-base NVMe will lap up that traffic easily.