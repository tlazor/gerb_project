I don't use GestureListener, but rather track fingers in my own InputProcessor overrides.  This allows huge flexibility, and gives me a rich set of input data, without worrying about some of it being stolen by another listener.  I can't make out exactly what you're trying to accomplish, but by doing something similar, you can do just about anything.
It would be easy to implemnt a follower that just cached the screenX, screenY at each call into touchDragged(), and compared that cached value to the current value, so you could derive a detla-vector.
Then, using those state variables, and processing touchDragged() and touchUp(), I can tell when the user is dragging around, calculating a "heading vector" as the delta from the stored origin to the current screenX and screenY passed into touchDragged(). I can tell when another finger is tapped (which I treat as an "action button").
The designers did a good job on the interface, so with just a bit of creative calculation and keeping track (in class-level variables) of things, you can implement just about anything.
It's easy to implement a radius of "dead spot" around the origin to filter of superfluous jitter around the origin. By using the pythagorean theorem to calculate the distance from the origin to the current screenX, screenY, and ignoring values with a distance less than some value.
For instance, I implement an On-Screen virtual D-Pad.  Where I treat the first touch via overridden touchDown() to be the origin of the D-Pad, storing the origin and the "pointer" (finger id) in class-level vars.  When the class is initialized, the finger Id is set to -1, so I can easily check uninitialized state.
My current game doesn't utilze fixed-position controls, rather wherever the user wants to touch on the screen becomes the origin of the D-Pad until that finger is raised, then the origin becomes wherever they touch again (likely a different place than previously).  This is to avoid controls getting in the way of in-game action.