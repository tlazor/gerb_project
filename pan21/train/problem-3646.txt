One common technique is to perform gradient descent on each node's local subset of the data and then pass the updates back to a master node. The master aggregates incoming updates from across its workers and then sends out the updated weights to workers for continued gradient descent. When repeated numerous times, the error aggregated across all nodes eventually reaches a global minimum. 
The accuracy should be the same. Yes, the data is distributed, and yes on each node there is only a subset of the data, but that does not mean that the learning algorithm (for example gradient descent for linear regression) happens on only one node for only one of the subset/local copies of the data. 
While for some algorithm to heuristically merge results from distributed machines, there is no guarantee on quality of the results. 
For example, if the goal is to estimate the mean of the data. Each machine can get one mean value and the number of samples, then a reducer machine could get a weighted average of the means, which is the true mean of all data. 