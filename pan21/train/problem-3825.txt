In convolutional neural networks, we make convolutions of three channels (red, green, blue) with a filter of dimensions $k\times k\times 3$, like in the picture: 
Depending on data set and problem, a model might actually perform better by compressing the training data down to greyscale. Eg., a small data set might overfit more with colored images than greyscale.
Each filter consists of adjustable weights, and can learn to detect primitive features, like edges. The filter can be different for each channel: one for R, another for G, yet another for B.
Isn't it more practical to detect features on a gray-scale image where we just need a 2-dimensional convolution? I mean, somewhere in the pre-processing step, the image can be separated into a gray-scale part and some other construction containing only a color information. Then each part is processed by a different network. Will this be more efficient? Do such networks exist already?
The number of neurons and architecture of the model are hyperparameters. There's no reason you couldn't test greyscale architectures alongside 3-channel models.
If there's an edge, it will appear on each channel at the same place. The only case when this is not true is if we have lines that are purely red, green or blue which is possible only in synthetic images but not in real images. 
in medical images often colors are very vivid and in channels looks very different take a look at these contests data https://www.kaggle.com/c/human-protein-atlas-image-classification https://www.kaggle.com/c/data-science-bowl-2018 images on different chanels differ sometimes a lot, so if you have picture of bear, car or any "normal" picture probably in most cases you can get away with greyscale or analyzing just one chanel, so having convolutions over volume will produce better models
For real images these three filters will contain the same information which means that we have three times more weights, so we need more data to train the network.