Although I haven't had real experience with figuring out how much power to keep multiple machines running, one thing to keep in mind is to have enough power for maximum load.
If tripping the circuit breaker or overloading the generator is unacceptable, then I would think it would be a good idea to figure out a conservative estimate for power consumption -- find out maximum power consumption of each component and round values up.
i trying to see how much electricity is required to power 'x' number of computers. I know it's a vague thing because some computers draw more than others (eg. diff chipsets, HDD, vid cards, PSU's, etc). 
A rough guestimate that I would come up with for an "average" computer would be something along the line of 300 W for the machine and 100 W for the LCD, and definitely your mileage may vary.
While I don't have exact numbers, I have ran LAN parties with up to that many people in a conference room.
So, lets just assume is a mum-and-dad Dell computer with some average run of the mill stuff. Nothing fancy. 20" LCD's.
this is to help calculate the generator power required to keep around 'x' computers running in a LAN. The real figure is in the hundreds .. but i'm assuming i can just figure out the base cost for one machine and then multiple it by the number of seats.
We had a power board, which had it's own breaker. We had about 18A breakers in total, for 6480W, which works out to 324 watts per machine. Not really a lot for gaming (we blew a breaker once, but I don't think we had 20 people, more like 17 or 18).