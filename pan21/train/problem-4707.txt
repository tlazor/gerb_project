It's hard to debug without seeing code, but you may be reading the file to the memory, instead of processing it line by line using foreach $line (<FILE>). Also, if you add this to a variable or array, it's the same. Do all processing instead of the foreach. Even so, 100k emails should not be using 48Gb.
We have compromised and split the list into chunks of 10,000, but would like to figure out the root cause for future fixes.  This is a CentOS machine with Litespeed as the web server.
We have a midsized server with 48GB of RAM and are attempting to import a list of around 100,000 opt-in email subscribers to a new list management system written in Perl.  From my understanding, Perl doesn't have imposed memory limits like PHP, and yet we are continuously getting internal server errors when attempting to do the import.  When investigating the error logs, we see that the script ran out of memory.  Since perl doesn't have a setting to limit the memory usage (as far as I can tell) why are we getting these errors?  I doubt a small import like this is consuming 48GB of ram.