I have a game that takes input from a joystick. If you tilt the joystick right, the ship moves right linearly according to how much the joystick is tilted. So if you tilt the joystick twice as much the ship tilts twice as much. This feels fine in cases when you want to bank the ship through a steep turn (ie. lots of tilt). But for small tilts, the controls feel far too sensitive. How do game devs flatten/smooth the effects of controls? Do I just take the log of the tilt? Mostly, I want to flatten things in the middle of the range of possible tilts.
Usually, a joystick axis goes from 0 to 1, with 0 being no movement and 1 being full movement. So if you are holding the left thumbstick halfway to the right, it would be 0.5f, and if you are holding it three quarters of the way to the left it would be -0.75f.
Usually a joystick value comes to you in the [-1, 1] range.  You'll want to preserve the sign when raising it to a power, so you'd do something like this:
Here's an interactive graph where you can see how this function works when you adjust the value of p.  As you can see, the curve flattens out in the middle, meaning that a given joystick deflection produces a smaller response.
A common approach is to raise the values coming from the joystick to a power, for instance squaring them, which improves the sensitivity for small deflections but still allows you to access the whole range of movement with large deflections.
What I suggest is only registering movement if the value is bigger than some threshold. For example: