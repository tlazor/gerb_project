wget saves to a file by default; cURL by default outputs to STDOUT (meaning your screen).  You need to specify that it should be outputting to a file.  You can do this with the -o (aka --output) switch, or with shell redirection:
The slowness probably is due more to the output going to the terminal screen than your network speed; once you redirect the output to a file you should see download speeds roughly equivalent to what you got with wget.
I am having a bit of problem using cURL to download using FTP, that is to say I think it's being rather slow if working at all. I usually use wget an this one file which I downloaded earlier with wget started and finished under 10 minutes. The size was about 200 MB, a Linux distribution. I tried it later on with cURL, first time using it, and all I did was type in the FTP address after the cURL command as I would with wget. I started seeing the source code and everything, but it's been like an hour and it still hasn't finished, is this normal? My connection seems fine, as fast as it's supposed to be. I would appreciate if someone who uses cURL or knows about it extensively could explain the matter.