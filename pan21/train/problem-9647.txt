As far as ETL/ELT, there is AWS Glue as other posters have mentioned. And yes, there are a number of tools, some of which are free.  Amazon does have a DB Best Practices Guide, that might help you, too. One tip I've seen in other forums is to load your data as raw as possible and do the transformations in Redshift.  That would lead you to an ELT process.  With so many options, perhaps looking at a comparison of the 2 methods would help.  Here's a blog article from Panopoly explaining the differences, it might help you decide on a path. 
Since Redshift is a columnar database, storage and query performance will be different than RDBMS models.  Optimizing for a columnar database is also different.  Because there is usually less disk I/O and less data loaded from disk then queries are faster. 
In terms of the AWS blog post you reference, I take it you have looked at those recommendations and considered which options work best for your data for distribution, keys, cursors, workload management, etc. and have at least a good idea of the approach you would use.  I find it easier to work with a visual representation, you might consider a quick and dirty DB diagram showing how your existing tables would migrate to Redshift.  Covering the major ones to get a feel for how much data is going where. And I would certainly use the ODBC/JDBC drivers from Amazon, loading large amounts of data can be troublesome in any case, much less moving to a different DB type.