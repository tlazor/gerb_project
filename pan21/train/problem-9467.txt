You seem to look at raw access on a server. If you have a server which uses a lot of disk IO then having directly attached storage instead of storage via 100mbit Ethernet makes sense.
There are quite a lot of faster interfaces then 100mbit Ethernet. Maybe not on a home/SOHO version. But a enterprise SAN will offer things like FibreChannel, iSCSI, 10GBit Ethernet, etc etc.
As best as I can tell, SAN stands for Storage Area Network, and it basically means that you connect your servers to your disks using an ordinary IP network. I am utterly stunned that any sane person would think this is a good idea.
You can do this, but many don't, for better performance and reliabilty many have either dedicated networks just for Data Center Ethernet (for FCoE), IP (for iSCSI) or Fibre-Channel (FC) based communication to their array/s.
So, these are the drawbacks of using a SAN - massively increased cost, massively decreased performance, loss of scaling, increased complexity, and more possible points for the system to fail at. So what are the advantages?
And as you suggest it IS much easier to add, change and remove disks - if I have to use local disk I need to order the physical disk, have it installed and present it to the machine. With a SAN I just have one large block of disks already in place and just carve-up what I need as required, presenting it in seconds to the server as needed, when it's not needed the space goes back into the pool. Also failed disks are handled much more smoothly.
In short, no matter which way I look at this, it looks extremely stupid. In fact, it looks so obviously stupid that nobody would spend time on R&D making something so stupid. As I said above, this can only mean that I'm misunderstanding something somewhere - because nobody would do something this dumb.
I'm not saying they are the best thing since sliced bread and work for every situation, especially give the potential cost. You really need to read a book I think about SANs like this one, it will clear things up.
Additionally, SAN technology is apparently extremely expensive. And it seems reasonable to presume that setting up an entire IP network is vastly more complicated than just plugging a few drives into a cable.
Ultra-320 SCSI is actually only 2.560Gbps, while 1Gbps Ethernet (as often used for iSCSI) is therefore 2.56 times slower overall many people are happy to deal with this limitation as their bandwidth requirements fit within that profile. But SANs very often deal with MUCH faster protocols, 10Gbps, 40Gbps and 100Gbps Ethernet links are used for both FCoE and iSCSI and in the FC world most link speeds are 4Gbps, 8Gbps and 16Gbps - best yet the FC and FCoE protocols are specifically designed to handle storage data and degrade much more gracefully under high concurrent load than either regular Ethernet or SCSI/SAS. By the way nobody would ever really use 100Mbps Ethernet for iSCSI in a production environment, who uses 100Mbps Ethernet for anything these days anyway?
On the other hand, in 10 years of working here, I have needed to change disks... let me count... twice. So it's an event that happens roughly once every 5 years. So you're telling me that once every 5 years, the SAN is going to save me 5 minutes of work? And every second of every day it's going to make stuff 25x slower? And this is a good tradeoff?
As soon as you get any large number of servers, local storage quickly becomes to much management overhead and risk.
Now, with each server having a dedicated SCSI link, that means every time I add another server, I'm adding more bandwidth (and more disks). But with a shared SAN, every time I add a server I'm taking bandwidth away from the existing servers. The thing now gets slower as I add more hardware, not faster.
So SANs can very often be much faster than locally attached storage but the main reason that people use SAN is to ensure their services stay up. They do this using block-level sharing and cluster-aware file systems, more than one server can have concurrent access to individual blocks of data at the same time, not files like when using a NAS. This is how us Pros can have things like multi-node DB clusters and fault-tolerant VMs.
I think your issue is with your lack of experience at dealing with scale, I'm responsible for tens of thousands of disks, they die every day, but we have a very smooth process that involves global online spares that jump into place as required, meaning we can replace the physical disks during normal work hours - no need for midnight call-outs. If I just looked after a few standalone boxes then I wouldn't understand either but for businesses with important availability requirements, changing needs and large amounts of data to be reliably  stored then I'm afraid that SANs are the only thing thing that makes sense.
320 MB/s verses 12.5 MB/s. That's, let me see, roughly 25x slower. On paper. Before we add IP overhead. (Presumably SCSI has its own command overheads, but I'm guessing a SAN probably just tunnels SCSI commands over IP, rather than implementing a completely new disk protocol over IP.)
The one I keep hearing is that it makes it easier to add disks, or to move a disk from one server to another. Which sounds logical enough - presumably with a SAN you just gotta push a few buttons and the drive now belongs to a different server. That's a heck of a lot simpler than physically moving the drive (depending on exactly how your drives are connected).
No, again, add more interfaces to your storage volumes, and add more storage volumes. If you have a disk array with a dedicated SCSI link to a server, and you add another server with a dedicated SCSI link to the disk array, you have now split the bandwidth to each server in half (unless the storage unit can go twice as fast as the SCSI channel!).
So I have my server connected to its disks with an Ultra-320 SCSI link. That's 320 MB/s of bandwidth shared between all the disks on that server. And then I rip them off the SCSI link and plug them into a 100 Mbit/s Ethernet network with its piffling 12.5 MB/s of theoretical bandwidth. That's before you take into account any routing delays, IP overhead, and perhaps packet collisions. (The latter can usually be avoided.)
Yeah so use 10Gbps network fabric then. Add multiple 10Gig interfaces and bond them, have 40Gbps if you like. It's a simple answer. If you need more than 100Mbps of transfer throughput, don't use 100Mbps networking equipment.
Its quite rare that people run devices at 100% load all day and night, bandwidth is much less of an issue compared to seek times and latency. You seem to be caught up in what are merely architectural issues, that can be easily overcome.
If you had even 10 servers that ran 10 virtual machines each, you would want them stored on a SAN. If all data is kept locally on a server, when that server goes down, the data is now offline. SANs improve data availability through replication to other storage nodes. They also improve integrity with centralised snap shots, backups and checksums.
You post is so long, to be honest I'm not going to read it all (sorry), but yes, you are missing some points;
My employer decided to install a SAN. For various irrelevant reasons, this never actually happened. But I did some research to find out what a "SAN" is, and... I'm baffled. It looks like such an absurdly bad idea that I can only conclude that I've misunderstood how it works.
I guess if I was in charge of some huge datacenter with thousands of servers, keeping track of that much disk might be difficult, and having a SAN might make sense. Heck, if the servers are all virtualised, they'll all be as slow as hell anyway, so maybe the SAN won't even matter.
However, this does not match my situation at all. I have two servers with three disks each. It's not as if managing all that stuff is "difficult".