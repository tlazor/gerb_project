I have a ZFS pool made out of 6 RAIDZs.  One of the RAIDZ is degraded, due to loosing two disks in the single RAIDZ close enough together that ZFS wasn't able to recover from the first failure before the second disk failed.  Here is the output from "zpool status" shortly after reboot:
I think ZFS attempting to deal with two spare replacement operations is at the root of the memory consumption issue, so I want to remove one of the hot spares so ZFS can work on one at a time.  However, when I try to detach one of the spares, I get "cannot detach /dev/disk/by-id/scsi-SATA_ST3000DM001-1CH_W1F49M01: no valid replicas".  Perhaps I can use the -f option to force the operation, but it's not clear to me exactly what the result of that will be, so I wanted to see if anyone has any input before going forward.
When the first disk failed I replaced it with a hot spare and it began to resilver.  Before the resilver completed, a second disk failed, so I replaced the second disk with another hot spare.  Since then it will start to resilver, get about 50% done and then starts gobbling memory until it eats it all up and causes the OS to crash.
Upgrading the RAM on the server isn't a straightforward option at this point, and it's unclear to me that doing so would guarantee a solution.  I understand that there will be data loss at this stage, but if I can sacrifice the contents of this one RAIDZ to preserve the rest of the pool that is a perfectly acceptable outcome.  I am in the process of backing up the contents of this server to another server, but the memory consumption issue forces a reboot (or crash) every 48 hours or so, which interrupts my rsync backup, and restarting the rsync takes time (it can resume once it figures out where it left off, but that takes a very long time).
If I can get the system into a stable state where it can remain operational long enough for the backup to complete I plan to take it down for overhaul, but with the current conditions it's stuck in a bit of a recovery loop.  