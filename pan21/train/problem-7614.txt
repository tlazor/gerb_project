For this kind of thing I would use a database, and serve the files from the database.  You may be able to template the page so you don't have the full page in the database. 
Still, yes, you will want to use a nested directory structure (say 3+ levels deep), so that no single directory gets more than a few thousand files in it. Then, use mod_rewrite to convert requests to the actual physical file names, something like the following (but probably with more checks and logic):
Are you certain that all (or a majority of) the 50M files will be requested? If not, and if your problem domain allows it, you could consider taking a "lazy computation" approach. That is, only generate (and then cache) those files that are actually requested.
Finally, some filesystems are better at efficient handling of large numbers of files than others, so you may want to do some testing and benchmarking with a few candidates (e.g. ext4, xfs, jfs, reiserfs) before going into production.
Depending on how you are generating the pages, consider putting the source you generate the page from in the database and generate the pages as needed.  There are caching techniques that can be used to prevent the need to generate the page each time it is requested.