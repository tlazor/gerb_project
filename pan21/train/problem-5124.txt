Cost based optimizers all work via a variety of proprietary algorithms (or you can read them for open source databases), but they typically work by assigning a reference operation a value of 1. For example, in SQL Server an operation with a cost estimate of 1 takes 1/320th of a second on a reference computer under some developer's desk in Redmond. The costing is just a relative guess of how expensive a query will be. Many RDBMSes use this cost in establishing priority or, in the case of deadlocks, to kill off cheaper queries (they take less time to run again). But it's all just a guess based on the information that the query optimizer has at its disposal at the time the query is being run.
Peter is correct, the best you can hope for is running some benchmark queries in ideal scenarios and using those to base best guesses on. You have to deal with a lot of different points of contention in an RDBMS, so it's difficult to specifically determine how any given query will perform in the real world.