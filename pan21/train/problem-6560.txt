If you keep the results for sanity testing, any small change in the application may change the results dramatically. If 50% of your values are suddenly different, can you say it's due to an introduced bug, or if, due to some small floating point accuracy difference, the physics simulation just happens to act slightly differently?
How do you know if some values are wrong? Would a human watch every single iteration to make sure nothing funny happens? Or would you just set some arbitrary boundary values that are checked against ("no object may ever fly over 2 kilometers" or some such..).
Yes, it is possible to run a simulation of all angles and shot strengths, and to let the physics run through, and to get some values out. There's two slight problems with this, however.
More likely it's worth it to make some hand-crafted boundary case tests (the cases you'd expect to have problems; shooting straight up, straight right, max power, min power, etc), combined with random testing that can be left running overnight.
Second: Is it really sane to iterate through every single possible angle and shot power? The number of iterations required is probably immense, and since the physics simulation takes some time to execute. It is still probably possible to test them all, especially if you throw the problem to some cloud processing service, but whether it is cost-effective is another matter.
First: What would you consider a failure? If you are looking for crashes, that's a clear thing, but any other kind of tests would be harder to make.