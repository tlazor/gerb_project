Personally i am using RAID-0 with three disks (PATA about 28MiB/s and each SATA about 45MiB/s), so i get arround 105MiB/s (adding the PATA reaches the bottleneck since PCI-32 33MHz can not sustain the three HDDs bandwith 2xSATA+1xPATA plus all cards, and gain by adding PATA is only 15MiB/s, but all counts), also more, adding the PATA causes the SATAs to not be used at max bandwidth, so they work on relax, also the PATA works on relax way since bottleneck is at PCI bus, so the HDDs have some time to cold them selfes (something most people do not care about)... why i use so fast HDDs that they are doing nothing half of the time? just to not over heat them and no need to have them on refrigerated room! Also to enlarge their live really a lot!
Also i recomend using small HDDs fast enough and not to tend to use huge HDDs. It is much better to have 8 HDD of 120GiB, than 2 of 500GiB; but put a warning on having enough ports and enough power drain.
If done on ZFS, since stripes size is dynamic, if no compression at all, i can get the sum of bandwidth (28 PATA + 2*45 SATA + 25 USB = 142MiB/s)? No and no, again the limit is PCI BUS (105MiB/s because all other cards), so i get only 105MiB/s uncompressed data transfer... so no gain (on speed) by adding the USB one, something i knew before starting, PCI bus is the botleneck.
I know you are talking about RAID 1 (mirror), for writes Raid 1 needs to write to all the disks the same amount of info on each one, so speed is the slowest one, but on reads it can read different things form each disk (if configured for so) so it can go up to as much as Raid 0, so my tests on Raid 0 can make some light, and since i tested Raid 0 agaist ZFS, ... all is said.
After thinking a little and trying a lot of tweaking a lot i could add an USB 2.0 HDD to the zfs pool (i wanted in a way i can remove it after the tests, that is a lot hard)... basically i solved it by takeing apart all HDDs and put other ones i have on a desk without use, then do the test with ZFS on (all acting as dynamic stripping, zfs stripe size is dynamic):
For what i have read about ZFS (it allows compression on the fly, also encryption) it has also something really good that no RAID can do: Integrity check on the fly.
If i do that configuration on Raid0, i would get 25MiB/s*4=100MiB/s, so now the limit is not by PCI at 105MiB/s (video, audio, etc., cards use part of the bandwidth), it is by adding the really slow 2.0 USB max speed, it lowers (on Raid0) the speed of all (Raid0 needs all stripes of same size, so bandwith to all HDDs is the same, as high as the lowest one).
Creating a mirror spanning the whole disk is the simplest solution. Creating several smaller mdX devices is more work, but can be more flexible. 
Migrating to ZFS do not get things better? Oh, yes and no; since compresion is ON it gains a lot, now (with compressible data) i can sustain a near 105MiB/s (compressed data) and since compression is near 2.5:1 i can read/write real uncompressed data at more than 261MiB/s on best cases.
If you are unsure about any step, first make a full disk backup. Clonezilla is a free tool which can do this.
The USB bus will be the bottleneck. An average speed for USB2 is about 30 MB/second. Both disks are likely to be much faster than that.
No. If the disk fails it fails. It does not care what is written on it, or how you see that data on it.
The good part of ZFS is the native compression (no matter if you put files or put partitions, it compress the blocks) and the validation on data.
So having ZFS with some level of RAID is much better than mdadm, etc., but at a high cost on RAM used.
RAID must not be used, no matter how say another thing, if your data is really important, RAID does not warranty data is correctly written to disks (it can not know if data on a sector is correct or has been altered, no checksum is calculated, no hash, etc).
I recomend not to use DeDup on ZFS... it can requiere huge amount of RAM, more than 32GiB of ram on 1TiB disks.