I try not to think of my servers but more of the service they provide. My servers are down often, once a month at least for patches, while the service they provide is little if not zero - this is achieved via a combination of clustering and load-balancing, fairly basic tricks that ensure you can have your cake and still eat it :)
Likewise with a print server, what is the business impact if you take it down for 5 minutes every couple of hours?  Depending on the volume of printing and what's being printed, there could be no business impact at all - and yet it would be measured against your overall downtime!
As Dominic says, no such thing as a 'standard'. There's "what the organisation is willing to pay for" - improvements in uptime (e.g. to go from 99% availabilty to 99.9%) require improvements to at least one (and often several of) staff levels and knowledge, working practice, amount and configuration of equipment. If you invest with an eye to improving uptime then you can do it, but when you get to 99.999% availabilty you really are talking about having implemented things like clustering, with cluster members in geographically seperated datacentres. Do-able, but not cheap.
Uptime requirements will be different for different systems, and will be influenced by the business requirements.  It might be a great thing to day that you have 5 9s, but are you putting a lot of work into ensuring uptime during periods when the servers don't actually need to be up?
So no, absolute measurements of downtime according to some hypothetical standard are actually quite meaningless.  Better to measure in terms of actual business impact instead.
Example: you mention a file server as one of the types you're interested in.  Now, a day is 24 hours, and assuming that 8 hours of those are the working day, you could in theory get away with 66.66666... % downtime!