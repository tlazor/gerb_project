What I'd like to do is tell the OS to set the SATA links to 3Gb/s to start with on boot, since I don't think the motherboard's SATA bus can handle all 4 drives at 6Gb/s. I couldn't find an option in the Motherboard's BIOS to change the link speed.
There is nothing at runtime unfortunately (there is /sys/class/ata_link which contains read only infomation).
With regards to data loss, probably not -- but frankly since the SATA vendors clearly haven't honored the SATA spec correctly (or its buggy or whatever) then who knows.
I could even see in /var/log/syslog that the SATA controller tried to reset the link speed to 1.5 GB and still failed communicating with the disk. Without any configuration change, it worked absolutely flawlessly with the new cable attached.
After doing the above, I am now able to run IO intensive operations like copying virtual machine images from one part of the disk to another without getting any more WRITE FPDMA QUEUED errors.
As someone might stumble over this question just as I just did: A very simple solution to the problem could be to exchange the SATA cable. I moved an SSD from 2014 into a 2018-ish system and used the old, unshielded cable to connect it. I got exactly the errors and problems described here. They were all completely gone after I exchanged the cable to a modern shielded one.
Downgrading it to 3Gbps may not be possible this way, but it should be possible to downgrade the links to 1.5Gbps. Unless you've got 10k drives or those are SSDs, they won't be able to saturate the 150MB/s link anyway...
Most hard drives can be forced to negotiate lower speed using jumpers. Google for "Sata 1 jumpers your drive model"
When the errors occur, the Kernel/OS (CentOS 6.2) eventually resets the link several times, and when it continues to fail, then it changes the link speed to 3Gb/s. Once that happens, the errors stop for that drive for the remainder of the session.
I'm having an issue with a motherboard that claims to support 6Gb/s SATA transfer speeds, but when using 4 drives on it, in a software RAID 10 with heavy disk IO, some of the SATA links start throwing kernel errors, ie. ata1.00: failed command: WRITE FPDMA QUEUED. But it's a different drive each time, not always ata1, and running extended test on each disk individually produces no errors.
However at boot it seems you can setup parameters forcing values that you want. The documentation for this is here: http://www.kernel.org/doc/Documentation/kernel-parameters.txt