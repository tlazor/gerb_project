So it's still good to optimize out branching where reasonable, but I think it's one of the things good left to the optimization stage where you need to squeeze max performance out of your shaders.
If there is no divergence (i.e. all threads in a wave take the same branch) newer GPU's can skip all the work within the if-branch. If there's divergence, then code in both branches is executed, but thread execution mask basically defines which threads execute code in which branch (code in non-executed branches for threads are effectively NOPed out). This is basically the same as predicated branching but happens dynamically based on divergence.
Is this true?  Are there any other downsides? like perhaps older video cards still doing the if logic anyways, even though no pixels evaluate to true?
It's common knowledge that branching in a GPU program is costly because it may have to run both the if and else logic for every pixel being evaluated in the same wave, but only applying each result to the appropriate pixels.
In this case, it seems like it has the ability to be faster compared to branchless logic, for the case when no pixels need to do the work inside the if statement, and can instead skip past it as a group.
On GCN architecture at least the branching itself is basically free or at least very cheap (handled by a separate unit running parallel to ALU for example), but something to keep in mind is that branching also tends to increase GPR pressure, which in turn lowers the occupancy. Low occupancy means there can be less threads in flight at once which influences GPU's ability to hide memory latencies. This may or may not be an issue in your shaders depending on how they access memory, i.e. you should see performance increase along with increased occupancy in shaders with heavy memory access.
I was curious if branching was still a performance issue if there is only an if statement and no else statement?