That might not sound as much to use today, but in the era when the Europe to US connection a 1200 baud dial-in line (that is about 0.1KB/sec!) every little bit helped.
The smallest part of information in a computer is a bit. A bit is either true or false, ) or 1, high voltage or ground, ... 
The bits are grouped into small sets. For almost all modern computers in groups of eight. We call this a byte.
We send all 8 bits of a bit to a destination when you know that you only need 7 of them to represent the text? If you do things in a smart way you can save 1/8th bandwidth.
You already self answered, but I think I can do better than Apparently certain types of files need to be uploaded in binary. 
I can look that up in the ASCII table and I will discover that your computer would store that in four bytes containing this:
These days we define much more characters. We use UTF-16 and unicode, allowing chinese, japanese, right-to-left language etc etc. Back in ye old days we did not yet have support for this in common places.
ASCII is a set of 128 characters, numbered 0 to 127. You only need 7 bits for this. On ye old days this was all you needed for communication. Just the regular 26 letter in the western alfabet, the number 0 to 9 and some special codes sunch as 7: Ring the bell or beep.
You transferred binary files (e.g PDF's) in ASCII mode, which did not transmit all information. Thus the resulting files arrived mangled on the destination
To transfer anything but plain old text, use the 'bin' command on the FTP prompt or tick the 'bin' option of you use a GUI.
This makes it the preferred method of transferring text. However leaving out the first bit will break anything which is not text. Thus the FTP protocol was designed twith two options: ASCII mode (efficient for text), and BINary mode (transfer as it is).