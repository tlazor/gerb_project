Given a kernel function, you can also choose any definition of distance thats suits your feeling : weighted euclidian distance, $L^1$ norm, $L^{\infty}$ norm, earth mover's distance...
From the euclidian distance, you can derive many similarity meausures from kernel functions (polynomial, exponential, Matern, custom...), of which none is a priori better or worse than the gaussian kernel. It all depends on your data and what you expect.
Now, the gaussian kernel with euclidian distance is very common as it is quite intuitive, and provides useful properties such as smoothness.
Let's be precise. "Distance" has lots of meanings in data science, I think you're talking about Euclidean distance.
In Euclidian space where the axes are represented by $i, j, k$ vectors, three-dimensional space, the distance can be obtained by connecting the two points and finding the length of the connection. This space is used whenever the basis, each of directions, are independent. In other words, whenever it is needed to find the true distance, Euclidian distance can be employed if the features or variables, axes indeed, are independent. On the contrary, whenever the variables are correlated, the Euclidian distance cannot be employed, because the axes are not independent anymore. In such situations which is not rare, Mahalanobis can be utilised. Its form is like Gaussian distance. 