To reduce jitter to the minimum there is likely one ethernet bus on the ingress side of the encoder chip and another ethernet bus on the egress side of the encoder chip. The management ports run to ethernet ports on the supervising CPU.
From a networking perspective, this sounds easy. But from a programmatic perspective, you have to bind a specific conversation to a specific interface and a specific port (regardless of whether they are on the same physical layer 2 or layer 3 network). 
I'm no video coding expert, but what I assume it the case is that these encoders have dedicated ports for receiving the video stream and dedicated ports for sending the encoded stream. On the first set of ports you connect recording devices (e.g. IP based security cameras), on the second set you connect the network which needs to receive the encoded streams. The encoder sends and receives on all interfaces of course, the ports just have a different function, a bit like the difference between a LAN and a WAN port on a consumer grade network.
What you have isn't a generic networking device. It's basically a MPEG encoder chip with networking wrapped around it.
You may not even be able to arc up a TCP link on the input and output ports. A lot of studio video codecs only support the expected packets on the input (such as UDP containing MJPEG images) and only generate the encoded packets on the output (such as UDP containing MPEG-2). To allow IP to work they'll bridge the ethernet bus into the supervisory CPU so that the CPU can generate ARP and so on.
I am very confused with some equipment at work (video encoders). They have 6 Ethernet ports: 2 for management and 4 for data. But the data ports are labelled like 2 for data input and 2 for data output. I wonder why they should be different, if an Ethernet port trasmit and receive at the same time.
Likely this is done for user interface reasons. While it's physically, and logically possible to send and receive on any interface, you may gain some convenience by telling the end user which ports to use rather than letting them decide. For non-networking people this is easier than forcing them to reconfigure the software just because someone wants to move around the plugs and a sender is now a receiver. 
Also since real-time en/de-coding requires strict real-time processing, the software may optimize communication on an interface for real-time encoding or decoding (possibly both, but you can theoretically always gain some performance by optimizing for a single use). Even the OS could optimize each interface for a specific purpose (Google for real-time operating systems). A really high-end system could even optimize the hardware for a specific purpose.