After loading your normal maps, you construct a TBN matrix to transform from tangent space to e.g. world space before writing to the G-Buffer. So in essence, your normals in the G-Buffer are stored in world-space, not screen-space. With that you can apply lighting in world space, as far as I'm aware, Crytek & Epic do the lighting in world-space now.
I'm confused when you say "screenspace normals", Killzone uses view-space normals, they store the X & Y coordinate of the normal in FP16 format, and reconstruct Z using $z = sqrt(1.0 - Normal.x^2 - Normal.y2)$ a problem with that is that we lack the sign of Z, even in view-space the normals can point away from the camera, i.e. Z is not always positive.
I'm not sure what you mean by "screenspace-ness", but when writing any sort of data to the G-Buffer, you're essentially writing their current value, if the normals are in world-space, they will remain in world-space when written to the G-Buffer. When you sample the G-Buffer on the second pass, for each texel, you get an RGB color (assuming you're using RGB) that maps to your coordinate data.