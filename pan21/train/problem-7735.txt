In my humble opinion, just splitting the file in training and test set is not the best approach, as there might be bias in the accuracy calculation just because of the random way data were splitted. I would go with cross-validation in order to ensure that the model has been extensively tested. In cases with small amount of data, leave-one-out-cross-validation is often used in order to maximize the amount of available data for the training. You can read more on Cross validation and Leave-one-out-cross-validation (theory and python implementation) here.
The test set is there to show you how the model can perform to unknown cases and basically reassures that it is not overfitted. The selection of the "optimal" model needs to be made based on its performance on the test set and NOT on the training set, because what you need is the model with the best behavior in unknown cases.
When you have found the model with the best performance using cross-validation, I believe you can feel free to train it on the whole set of data before giving it to production.