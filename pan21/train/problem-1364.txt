But this started me thinking: What are they key metrics that would tell me a server has/has not got enough memory or would benefit from more?
We have a build server VM which previously had 4GB of RAM allocated but that has now been reduced to 2GB, partially as an experiment to see if it runs just as well with less memory.
Is it as simple as looking at "In Use" memory over time? Or should I look at "Available"? Or look at the amount of paging? Should I setup alerts and what on? I suppose I'm looking for some insights into analysing server memory usage.
If you don't need to keep free memory for spikes in usage you might get by just fine even if the machine is using up nearly all memory if the performance seems to be nearly identical (I wouldn't advise it, but that's just been my experience).
I've been watching the memory usage in the Task Manager during a build to monitor how much memory is "In use" and it has not risen above 90%...which makes me suspect it's running just fine.
I wouldn't say this is the best way to go about it, but what I have done on our VMs (Linux, but the idea is probably similar) is to look at swap (page file) usage, which I find tends to be a good indicator since you actually want to use RAM. RAM that isn't used is RAM that is wasted. If the kernel starts swapping (paging), then it's finding itself lacking memory at least part of the time.
However, if it's a performance problem is usually only going to be reliably-tested by benchmarks or telemetry of some sort. Perhaps you could keep track of metrics of some sort and test it out with 2GB and 4GB configurations and look for differences. Be sure that you're narrowing down memory as the only variable here, keeping everything else as similar as possible (user load, making sure caches have been warmed, etc.).