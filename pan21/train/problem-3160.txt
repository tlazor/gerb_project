If using a linear SVM, you can simply continue training the model. Say you train on the first 20k points, then receive an additional 20k points. Initialize the optimization using the first model, then train using the full set of 40k points. Online methods like stochastic gradient descent are a good way to train SVMs on large data sets. SGD processes a single point at a time, so dividing the data into blocks isn't necessary. If the full dataset is available, you'd probably get faster convergence by sweeping through it all instead of breaking it into blocks.
You could train multiple models on different subsets of the data, then ensemble them, as @Hobbes suggested. This would let you train the models in parallel on different machines. The function learned would be different than training a single model on the full data set.
The case isn't so straightforward for random forests because the individual trees choose hierarchical splits in a greedy manner. The splits chosen for the initial 20k points may not match what would have been chosen had the full 40k points been available. But, it's not easy to go back and revise them. However, a number of papers have proposed online variants of random forests that could be used in this setting. A search for 'incremental' or 'online' random forests should turn up the relevant results.
I'd venture a guess that you're not directly using a kernelized SVM on 1M data points. But, you could approximate one by using the kernel to perform a feature space mapping, then train a linear SVM in the feature space. For example, the Nystr√∂m technique approximates the mapping using a random subsample of data points. One option would be to learn the feature space mapping using all data, then incrementally train the linear SVM. An alternative would be to learn the mapping using only the first block of data. This is valid if all blocks are equally representative of the full data set and adding new data wouldn't change the mapping.
The answer depends on your motivation for breaking the data into blocks. It could range from 'use online training' to 'use ensemble methods' to 'don't break the data into blocks'. Here are a few options to consider.