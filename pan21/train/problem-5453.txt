The convergence will continue until "cable/IPTV" and "Internet" networks are indistinguishable.  One day your Internet Service Provider will offer to hook you up with the 10,000 channel YouTube broadcast, and movie studios will broadcast IPTV multicast to the world directly.  There still will not be anything to watch on broadcast and there will still be buffering on your personal programming because your favorite cat videos will never be popular enough to make it onto the broadcast schedule.
Watching TV on the Internet is the opposite in almost every regard.  It is a two way signal tailored to distributing data to individual subscribers on-demand.  Nobody gets the same signal at the same time, and everybody has individual needs.  It is fundamentally many signals to many people.
There are several reasons. First, cable/sattelite bandwidth is much higher than you maybe think. Even a quite old coax installation has a total bandwidth that is very competitive to what most average users have for LAN. Same goes for sattelite. Just consider that e.g. on Astra you have some 100+ transponders, each with several channels and each of these with 27.5k symbols per second. That's a huge amount of bandwidth.
This makes the prerequisites to both the local cable and provider infrastructure a lot less demanding since you only need to transmit a tiny fraction of the data, and you need not worry about getting replies from the client and handling these. The amount of data to be sent through the infrastructure (and also your local cable) depends solely on the number of channels being broadcast, not on the number of people watching them. Since the latter are around a million times as many as the former, that's big win.
"Cable" is fast at providing TV because it is a one way broadcast signal tailored to distributing TV channels to multiple subscribers simultaneously.  Everybody gets the same signal at the same time with basically no variation.  It is fundamentally one signal to many people.
Second, data is broadcast, that means it is sent out once to "everybody" (and to nobody in particular), and that's it. Internally, a cable network may have to copy around the raw bits here and there (e.g. to a sattelite, or to your local cable network), but whatever. Everything is just sent once, fire and forget, eat it or die. There is no protocol for transmission errors (well, there is FEC, but that doesn't count, it doesn't communicate back) or any such thing. You get it or you don't, and as time goes by, you care less and less whether you got it, too (the provider doesn't care at all to begin with).
The actual medium is not the same: TV distributed via dedicated coax works very differently and has a very different infrastructure compared to, say, watching Youtube or Netflix, or anything else over the public internet.
Plenty of good points in the other answers but here is a picture showing how Verizon achieves this over fiber (Verizon FIOS).  Every provider is going to have different mechanisms so this is only an example of one such approach.  The Internet data and the broadcast video come from two different sources.  They are added on close to the last mile by getting transmitted across a different wavelength.
Modern "cable" has changed this by adopting newer bidirectional digital infrastructure influenced by data networks to allow for cable Internet.  This further allows for new services like on-demand programming and set top box gaming.  This required fundamental changes in the cable network and the electrical signals carried on the cables.  Data networks have changed this by adopting multicast protocols to allow for large scale broadcast data to enable IPTV, on-demand programming, and set top box gaming.  This also required fundamental changes in the data network.
So the answer to your question is: You are comparing apples and oranges, your premise that they are similar is wrong.
Even if the coax is used to implement IP protocols, and TV content distributed on top of that, you still have total bandwidth control, and can use a dedicated multicast infrastructure (which doesn't work very well over the public internet).
There is no HTTPS server involved. No requests being processed, no pulling arbitrary blocks of data at haphazard times, no guarantees, no concurrency/scalability issues. No packets being dropped by intermediate routers, no congestion control (and if packets are lost, nobody cares).
At its core your assumption "over a coax cable in the same way" is false.  Cables simply carry an electrical signal.  This signal can be encoded many different ways each of which is tailored to a particular usage including encoding schemes that combine multiple uses into a single signal.