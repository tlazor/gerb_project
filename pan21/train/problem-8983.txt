In the image below, you would make four different splits on your training set, where blue would represent training cases and orange the validation cases. You run the same regression/classification on each of the four blue parts, and obtain predictions on each of the four orange parts. 
Chapter 5 of "Introduction to Statistical Learning" covers CV and bootstrap. I strongly recommend to read this chapter, since sampling methods are extremely relevant in practice.
Cross validation (CV) usually means that you split some training dataset in k pieces in order to generate different train/validation sets. By doing so you can see how well a model learns (and is able to make predictions) on different samples of a training dataset. 
Since sampling strategies can be relevant, it is advisable to let some tool like sklearn do the CV splits. Most tools/methods come with some CV options if relevant. 
There are different strategies to deal with data characteristics. In order to balance groups etc, you can use a stratified sampling strategy. See an sklearn implementation here.
During training and model tuning, your model should not see the test data! The idea is that you reserve a truly "exogenous" dataset (never used during training) to test how well your model does in the end. If you use your test set during training, information may leak from your test set to the model, and you cannot demonstrate exogenous validity of your model anymore (becasue information from the test set leaked to your model).