There should be some threshold for more. Is there any technique available which can suggest no. of bags for the type of data available. 
Fourth factor could be also the signal to noise ratio. In other words if you data is noisy enough you need more samples to surface to interesting signal in a stable way.
Your bag size could be found heuristically by starting with a roughly equal number of patterns in each bag ( n1 ). Increase n1 to a value half way between its current value and the maximum if you find that n1 is to low.  Conversely decrease n1 to between the current value and the minimum if you find it is to high.
Second reason if the sample itself is representative. Often in practice the sample is not purely a random sample. It contains various interdependencies which can lead to problems. For example if you collect data from multiple countries and you have more samples from some countries simply because of costs, but not related with the phenomena you study.
From the start I have to state that I am not aware about any paper regarding number of bags for bagged logistic regression. Therefore my assertions apply generally, for any bagged ensemble.
The short answer is no, and I do not think that if possible to be constructed. There are a couple of reasons that I will describe above.
The first reason is that it depends on the complexity of the joint probability you want to estimate. Technically any model you build for prediction purposes aims to estimate a conditional probability space on output variable giving the joint input variables. Doing bagging puts the problem if the estimations from selected samples does cover the that relation in all places. To exemplify, you can have one input categorical variable with 2 levels or with 10 levels. I think more estimations are needed for the later case.
Since finding the answer to your question is hard, and knowing it is close to similar with knowing the true model, there are things which can be follow in order to find a proper number of bags. What I recommend would be to build repeatedly models with increasing number of bags and see the error estimates. Usually when those estimates have a low variance and adding bags does improve the performance significantly than you can assume you need no more bags and bagging with your chosen model is not able to capture any other structure in your data.  
Generally speaking about component classifiers, overall classification is better when the decision rules of the component classifiers differ and provide complementary information.  
Third reason is that 20%. A typical bootstrap sample have the same size as original. Reducing the size is interesting because of performance gain or because of induced artificially variance which makes the samples "more independent". However estimating what is the effect of decreasing the sample size is again hard to estimate. 
Having said that, one methodology you might try for suggesting the number of bags is to create an arbitrary statistic that indicates which parts of your feature space is covered by a particular bag.  You could stop adding bags when you reach a satisfactory level of coverage of your entire feature space: when you have expert classifiers for each region of your feature space.  