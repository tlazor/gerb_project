Further, changing any config in CloudFront (including CNAME) results in 15-60 min of waiting while the changes are propagated to the edge servers. Also, not an ideal setup.
Does anyone have a good solution for moving from one CloudFront distribution to another or does everyone really just create their distribution and then never ever touch it again? 
The first option I attempted was to have two separate CloudFront Web Distributions, one for my current, or A, environment and one for my new, or B, environment.  I attempted to use a Route53 weighted routing policy where I added two records for my www.domain.com Route53 record, one pointing to CloudFront Distribution A with a weight of 1 and the other pointing to CloudFront Distribution B with a weight of 0.  The plan would be to change the weights when I want to move from distribution A to distribution B.  However, only one CloudFront distribution at a time can have the www.domain.com Alternate Domain Names (CNAMEs) registered or you get the following error:
My CloudFront distribution consists of one S3 origin for static content (javascript, etc) and a custom origin pointing to an AWS ELB.
CloudFront requires the CNAME in the distribution config to be unique across your entire account. So controlling blue/green via DNS to different distributions will not work. There is a hack rolling around that would use wild cards but that makes no guarantee that the correct files are served. Controlling blue/green via DNS and CloudFront is not feasible.
Under normal circumstances we don't make any changes to our CloudFront distribution at all.  We version our static content in the S3 origin by changing the name of the static content files in S3 and do rolling deployments to EC2 instances under the Elastic Load Balancer (ELB).  However, there are times when we need to test and make changes to the CloudFront distribution itself or have significant enough changes to our environment that we need to point to a new ELB in a new environment.
You cannot performs soak tests very well. I suppose it's possible to increase the TTL of index.html to a few hours or days (depending on your need), that would help ensure the clients will get new versions as their local cache expires.
In this blog post the author implements a-b testing via Lambda@Edge working off of the code in the AWS documentation (you can see their examples here: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html).
Basically what you do is you create a single Cloudfront distribution pointing to two different origins. Then you can use the Lambda@Edge to direct the traffic to either origin (via Cookies), and of course you can implement other things via the Lambda such as weighting the traffic or flipping it etc. It's also easy to add further origins and logic.
For apps that are already running, we have the build version built into the code and a build config json file on the root of the app. Then the app will occasionally request the json file, check the version, if it's out of date, prompt for a refresh.
Now configure CloudFront to use your bucket to serve the files. At this point, it all comes down to you cache settings. Since CloudFront takes forever, set the CacheControl header on our S3 objects. For index.html, we use 5 minutes, everything else, 1 day.
I have S3 and custom origins pointing to both my A and B environments and then I update the CloudFront Cache Behavior to point to the other origin when I want to move from one environment to another.  This is extremely messy because these updates take 15-60 minutes, there is no visibility into the progress of the update and depending on the nature of your change you may need to follow that up with a CloudFront Invalidation so you aren't serving cached content from the old environment along with new content.
When it comes time to switch, swap the S3 directory names. Within 5 minutes, the app will be live for all intents and purposes.