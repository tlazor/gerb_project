An Internet connection with just 1 Mbit/s bandwidth can connect you with millions of hosts without a problem. Note that these millions of hosts are also connected with you.
IP networks are based on packets. Packet networks don't allocate bandwidth for devices that are just connected. Packet networks (so to speak) allocate bandwidth ad hoc when there's a packet to be transported. You can connect 16 million nodes (10.0.0.0/8) all with each other using a 10 Mbit/s network (as long as there's extremely little traffic).
with /8 you have big broadcast domain and if a broadcast packet initials in your network to call all of possible node, their will be a bandwidth waste. 
An engineer and I were discussing that if our network has more network than actual needed would cause wasting bandwidth. For examples, if our company has only 200 devices are actually on a network, but our network is 10.0.0.0/8 then we are wasting bandwidth for those that we don't use. I don't think its true because there are only 200 devices on network then only 200 devices are using network bandwidth even you have more host in your network subnet than actual devices. I'm not sure if it's true. Anyone can explain to me?