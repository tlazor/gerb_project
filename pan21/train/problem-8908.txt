A lot of classification techniques also have a class weight argument that will help you focus on the minority class more.  It will penalize a miss classification of a true minority class, so your overall accucracy will suffer a little bit but you will start seeing more minority classes that are correctly classified.
I think subsampling (downsampling) is a popular method to control class imbalance at the base level, meaning it fixes the root of the problem.  So for all of your examples, randomly selecting 1,000 of the majority of the class each time would work.  You could even play around with making 10 models (10 folds of 1,000 majority vs the 1,000 minority) so you will use your whole data set.  You can use this method, but again you're kind of throwing away 9,000 samples unless you try some ensemble methods.  Easy fix, but tough to get an optimal model based on your data.
The degree to which you need to control for the class imbalance is based largely on your goal.  If you care about pure classification, then imbalance would affect the 50% probability cut off for most techniques, so I would consider downsampling.  If you only care about the order of the classifications (want positives generally more higher than negatives) and use a measure such as AUC, the class imbalance will only bias your probabilities, but the relative order should be decently stable for most techniques.
Logistic regression is nice for class imbalance because as long as you have  >500 of the minority class, the estimates of the parameters will be accurate enough and the only impact will be on the intercept, which can be corrected for if that is something you might want.  Logistic regression models the probabilities rather than just classes, so you can do more manual adjustments to suit your needs.