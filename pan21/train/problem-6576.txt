In my experience, giving weights to observations (if the algorithm in use supports it) generally works better for highly imbalanced  classification problems. Since, your are using RandomForests I would suggest you to try that.
SMOTE is not designed to work with severe data imbalance specially if you have wide variation within the minority class 
Balancing your dataset does not guarantee an even prediction split.  Imagine the case where your features cannot separate between positive and negative examples at all. In this case, even if you balance the dataset, you will learn a decision boundary that essentially randomly guesses on each example. You would therefore expect that your prediction error would mirror the distribution of the majority/minority classes.
I am trying to solve a classification problem on a highly imbalanced data set. I am using SMOTE to over sample the minority samples and down sample the majority ones. After creating a balanced data set, I applied the random forest model. But, the prediction error for the minority class is extremely high even after using a balanced data set. What could possibly be going wrong?