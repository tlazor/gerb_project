That syntax saves 50% of the runtime compared to the original query. If the original query takes 30 minutes to execute and does 400 million comparisons that kind of change can add up.
Complete script available here.  Just a reminder this is just an example demonstration of a technique, not a complete production solution.  If you do not feel this addresses your problem please post some more realistic sample data, anonymised if you wish which genuinely represents your use case.
In your example data set, #temp1 only contains individual tokens (no combined processes). If your data actually looks like that, you can use a string splitting function on #temp2.mdx and do an equi-join. I'm using my own string splitter that accepts VARCHAR(MAX) values. It's based on Jeff Moden's.
I have used some dummy Excel formulas in this simple demonstration of an approach to this problem.  This which will run on SQL Server 2017 or Azure SQL DB uses the new functions TRANSLATE and STRING_SPLIT for effective formula splitting.  The list of characters used for splitting could easily be extended.  This entire script (including dummy data generation) runs in about 5 seconds on my laptop whereas your original query ran for 7+ minutes on the same data.  Please work through the script and see if you can adapt the approach for your actual data.
Using <> and REPLACE is a really rough way of getting what you want. Using LIKE could be a bit better, but you'll have to deal with escaping any special characters in mdx.
On an otherwise not-busy 4 CPU server you could see the query run 4X faster compared to before. Putting everything together you might see a speedup of 8X. Perhaps your query which took 30 minutes will now take closer to 4 minutes.
Let's start with reducing the cost of the join predicate. Does temp1.mdx always fit in a VARCHAR(8000)? The documentation for [REPLACE][1] says the following:
This happens because parallel plans don't reduce costs on the inner side of nested loops. If you really need to improve performance you can consider tricking SQL Server into using a parallel plan. This type of query is very suited to parallelism despite the optimizer's costing model. I'm going to use the undocumented trace flag 8649 which is not safe for production use:
I don't know what your data set looks like, but on my machine this shaves off 33% of the runtime. You can do even better if you can use CHARINDEX, but that only works if #temp1.mdx can be cast as a VARCHAR(8000) without losing data.
So what can you do about it? You can improve performance by making the join predicate as cheap as possible to calculate (since you'll need to do it 400 million times) and by encouraging SQL Server to execute your query in parallel. The query optimizer is biased against parallel plans for queries of this type.
There may be some tricks you can employ around not selecting all columns, table definitions, and spools. Can't say more without more information.
So as if you might be getting wrong results. Setting that aside, the REPLACE function means that you need to process the entire #temp2.mdx string each time. You can't quit early if you find a match right away. Intuitively, it feels like the wrong function for this kind of task. You can use LIKE instead, but you need to be careful with automatic conversions to VARCHAR(MAX):
Therefore you must change your approach.  You mention you cannot split the string "because the formula is not as simple" - please post some more realistic formulas to represent this.
But you can make it perform better if you understand why it won't perform well. SQL Server has three options to physically implement that join: hash join, merge join, and loop join. Both hash join and merge join require at least one equality condition in the join that is always true, so those are ineligible for this query. If you try to force them you'll get an error. So you must do a loop join, but there's no way to create an index which you can use to do a seek on the inner side of the nested loop join. Essentially, you are requiring SQL Server to do a cross join of the two tables and to evaluate the join predicate once for each combination of rows. For your table sizes you'll need to calculate the predicate 400 million times.