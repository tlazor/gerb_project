You'll have at least one vSwitch that'll be handling all the vmkernel traffic, on that vSwitch, or any other vSwitch in fact, just create a new 'port group' and call it 'Internal' or whatever you want, then assign the private vNICs to it - it's that simple.
IF the machine is complaining that it doesn't have a gateway just assign an unused IP as the gateway IP. As long as all the traffic you are using is staying within the same subnet everything should work.
Note that this is only creating connectivity. This physical NIC does not have an IP address, nor should it. It is used only as a passthrough for the VMs to communicate. You do not need to set up a gateway for this. The VMs will be on the same local subnet as the physical hosts attached to that switch.
So at this point if you configure one of your VMs on the 192.168.0.x network and ping one of your physical hosts on that network, it should work.
Obviously if this is to work across hosts you'll need proper uplinks that allow the traffic to move from host to host and matching Port Groups but if this is for a single host then that's all you need to do ok.
Assign at least one physical NIC (pNIC) to your vSwitch. This NIC is how the VMs attached to the vSwitch (as well as any vKernel ports) get their connectivity out of the ESX box. Plug this NIC into a switch that your other physical servers are connected to. You now have connectivity.
When machines are on the same subnet they will send out a broadcast packetes and arp requests to all machines or devices on the subnet and the corresponding machine replys back and the communication starts.
If I understand you correctly you are just trying to setup a local network with a bunch of machines all on the same subnet? One of these being an ESXI server.
Technically if these machines and the virtual machines are all in the same subnet you don't need a gateway device for them to communicate. The gateway is only used when the machine is trying to connect with a device or resource that isn't on the same subnet.