This causes vast amounts of confusion to the uninitiated, because it all depends on the endian-ness, and the method (byte order) used to make bigger quantities out of smaller ones.
(when programming in C, never ASSUME that a char is 8 bits. Read your compiler manual, especially if doing embedded programming).
Generally high level languages hide this level of gruesomeness, but it can be important for things like overlay data structures, and, again, memory mapped device control registers.
Things get even more complicated when endian-ness comes into play, there are 2 common arrangements: little and big endian, this describes how byte are arranged to fit into a larger quantity (normally that machines native word size). So for example, on a 32 bit machine you might find the bytes arrange like this:
MOSTLY this kind of thing only comes up as a worry for the hardware designers. But if you have to write device drivers and things that talk to hardware that is through memory mapped registers, it becomes a big deal.
To the specific question: How are characters stored? The answer is: it depends. Frequently, ascii characters that fit in a byte are stored as a byte. Wide characters are frequently stored as 16 bit words. But unicode might implement wide characters or one of any number of coding systems, in which case characters might occupy anywhere from 1 to about 4 bytes, depending on the character.
On other architectures you might only have access to some other sized "blob" for the machine data type. For example on the TMS320C40 you can access 32 bit words, and 8 bit bytes are packed into these words. You can can pack the bytes in and out, but its quite a slow process requiring several machine instructions. 
Just think of your mass storage as a great big bunch of bytes, that can be accessed, and the machine will magically take care of it all. The common term used is to thing of files as a "stream", where you start at the start and the stream comes rolling in. (This conveniently ignores random access.) The smallest part you can break the file's stream into is a byte.
BUT dumping that same block from the same address and presenting as 16 bit (hex) integers might present as:
The convention in C of assume CHARACTER = BYTE is these days, misleading and misguided. Its best to thing of "char" is a synonym for "byte" - unless your machine / compiler tells otherwise (see above). GOOD C programs generally define a set of preferred types such as "UINT8" - unsigned 8 bit integer, "SINT8" - signed 8 bit integer, and so on, so that the program written becomes as independent as is sensibly possible from the peculiarities of the specific compiler and underlying hardware.
On a PC, addressing is by byte, and you can access a byte (8 bits), a word (16 bits), a double word (32 bits), and a quadword (64 bits).
Programs make use of this in all kinds of ways. So for example you will get CHARACTERS represented by bytes if they fit happily into plain ASCII (or even EBCDIC for those with long memories). The modern Unicode character systems may use Wide Characters (generally these are 16 bits), but there are many encoding systems for unicode. The Wikipedia page on Unicode is pretty instructive.
If a machine wants to store larger quantities (16 bit words, etc), then it may or may not do some level of transform to get that into the bytes that go to the storage.