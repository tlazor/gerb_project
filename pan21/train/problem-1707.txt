During development, a CPU will be designed to run at a certain speed, based on how much work the CPU is actually capable of carrying out.  If you raise or lower the clock speed, you're not actually changing the amount of work the CPU can accomplish, you're just messing with the efficiency ratio of it.
In many cases, doing a lot of work in one cycle could enable you to clear the entire pipeline. If successful, this means that your next cycle is going to be un-optimized because you have to fill the pipeline again, which can take some time.
The details get into a lot of physics and electrical engineering that I don't really understand, but remember that clock rate is not achieved by just naively adding input voltage to the processor and hoping for the best. At the very least, thermal profile is another necessary concern.
The clock speed is actually a (more or less) arbitrary decision made by the CPU designer, sometimes for good reasons (efficiency), sometimes for poor ones (advertising).
Let's say that a given CPU has a mixture of instructions that take between 1 and 100 nanoseconds (ns) to finish.  You could set the clock rate such that 1 "tick" is 100 ns (10 MHz), meaning every instruction would finish in exactly 1 tick.  However, if the instruction execution times are evenly distributed, this means that your computational units would be idle 50% of the time (the average execution speed would be 50ns, leaving the other 50ns of the tick idle).  If, on the other hand, you set your tick to be 10ns, the instructions would range between 1 and 10 ticks, but the unit would never be idle more than 9ns before the next instruction began, and the average idle would be 5ns.  Meaning your average idle time is down from 50% (average of 50ns out of every 100) to 9% (since average execution time is now 55ns (average execution of 50ns + average idle of 5ns)).
You could alternatively say, "Fetch the first number into a register. Then fetch the second number. Then add the least significant bits. Then add the second-least significant bit with the carry from before. Then add the third-least .... Then add the most significant bits. If there was a carry, set the overflow flag. Then write the result to memory." Now you have a huge number of steps. But each step can be absurdly fast. So you have low instructions per cycle (1/36 or so in this case). But your clock speed can be very high since each cycle only has a very small bit to do.
The actual specific trade-offs and cycle numbers are vastly different because modern CPUs are pipelined and overlap instructions. But the basic idea is correct.
A CPU can accomplish X work in a second.  If you use H clock speed and I IPC, we have I=X/H.  Changing H doesn't change X, but it inversely affects I.
For example, you could just say "add the two numbers". Now you only have one step. But that step has multiple parts and will take longer to do. So you have high instructions per cycle -- one in this case. But your clock speed can't be high because you have a lot to do in that cycle.
Here's a very simple (perhaps grossly oversimplified) explanation: Say you have a particular job to do, say add two 32-bit numbers. You can take two approaches. You can split it into a very large number of very small steps or you can split it into a small number of very large steps.
How long your computer takes to finish a particular task doesn't depend on the clock speed of the computer... it depends on how the computational units are designed and engineered.
To have both high instructions per cycle and a high clock speed, you'd have to divide a complex instruction into a very small number of very simple steps. But that can't be done because the instruction is complex.
I could design a very simplistic processor that processes one stage of one RISC instruction every cycle, and if this were the basis of my CPU, I could probably achieve a very, very high cycles per second due to the reduced complexity of what constitutes "a cycle".
I'm greatly oversimplifying this, but the important point to remember is that these terms are comparing apples to oranges. A "Cycle" is not a single unified unit of measurement that is the same across all processors, like a "second" is a unified measurement of time. Instead, a cycle represents a certain unit of work, which is defined somewhat arbitrarily but bounded by the complexity of the pipeline design and of course by physics.
(And before you cry about overclocking CPUs: this gives you two advantages that result in real-world speed gains: fast executing instructions (that take less than 1 cycle) end up with faster execution times, and all instructions have less idle time.  Both of these can in fact increase the amount of work your computer can perform, but you'll find that overclocking a CPU by X% doesn't always equal X% increase in work done when you benchmark it.)