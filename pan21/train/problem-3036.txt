You can then use that representation as input to you classification model of choice - SVM, Random Forest, Logistic Regression etc... but that is it's own problem to solve.
You could first convert the representation of the tokens using word embedding.  There are two publicly available models that are very popular: Word2Vec and GloVe.  Word embeddings can be very useful because they capture latent semantic and lexical information that is not usually available in vanilla BOW models.
There are tradeoffs to every type of feature engineering that you do.  Be aware of that and do your due diligence to evaluate what type of systematic effects (if any) this type of preprocessing will have on your result.
I don't now wether I got your question right. But if you count all words within a class, for example, the word "the" is counted everytime it appears. However, if you count the unique words the word "the" is counted once. This is why your counts differ from your plot. Each class can have a different number of unique words.
Next, using the word embeddings, take a mean over the set for each example in you data set.  That will balance the classes in some sense because you are essentially reducing the problem to an average embedding representation for each example.  Assuming that the number of examples per class are balance then your balance problem is solved.  Also consider doing stop-word filtering to remove useless terms.