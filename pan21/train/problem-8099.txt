Any #P problem, or even #P/poly, can be written as a polynomial: make a circuit out of NAND gates, write these as $1-xy$ where $x$ and $y$ are 0-1 valued integers, and sum over all inputs. This gives a polynomial in $\mathbb{Z}[x_1,...,x_n]$ for inputs of size $n$. The decision problem is testing whether this is 0.
A natural way to send a multivariate multilinear polynomial to a univariate is via the substitution $x_i \mapsto y^{2^i}$. The resulting polynomial is say $\sum_{i\in S} \alpha_i y^{a_i}$. This could be an exponential degree polynomial of course but let us go modulo $y^r - 1$ for a small range of $r$'s. Now an $r$ would be "bad" for a pair of monomials if $y^a$ and $y^b$ get mapped to the same monomial modulo $y^r - 1$. Or in other words, $r$ divides $a - b$. Thus as long as $r$ does not divide $\prod_{i,j\in S} (a_i - a_j)$, this wouldn't happen. Hence it is sufficient to run over a polynomial range of $r$'s. Thus, it suffices to evaluate the polynomial at some roots of unities and we can figure out of the polynomial is zero or not. 
more concretely, lets consider a polynomial in $\mathbb F_2[x]$. We know that $x^2=x$ in $\mathbb F_2$, so if we assume that any polynomial is already in a reduced form when given as an input, we are left simply considering one of : $0,1,x,x+1$ and accordingly evaluating any of these polynomials at either of $0$ or $1$ takes at most 2 arithmetic operations.
Over $\mathbb{C}$, testing for zero and evaluation is "almost" the same in the following sense: Assume you have a decision tree which tests whether some irreducible polynomial $f$ is nonzero.
I'm going to venture the idea that evaluating a polynomial $q(x)$ in $\mathbb F_p$ for fixed prime $p$ (or any finite field extension thereof, and with the coefficients restricted to the same field) will fit your criterion.
time. Let $h_1,\dots,h_m$ be the polynomials that are tested along the common prefix of the two path. Since $V(f)$ is closed, all elements that lie in $V(f)$ and reach $v$ also lie in $V(h_m)$. Therefore, if $f(x) = 0$, then one of the $h_i$ vanishes on $x$. We apply Hilbert's Nullstellensatz to $h_1 \cdots h_m$ and get that $f g = h_1 \cdots h_m$ for some polynomial $g$ that's coprime to $f$. In short, while we are not computing $f$, when deciding whether $f(x) = 0$, we have to compute $fg$ for some coprime $g$. 
In certain special cases, we have 'black-box' tests for identity testing where you can test if a polynomial is zero or not by just evaluating it at a predefined set of points. A simple example of this is if the polynomial is 'sparse' (just has $n^{O(1)}$ monomials). To make the exposition simpler, lets assume the polynomial is multilinear (each monomial is a product of distinct variables). 
We are working over $\mathbb{C}$, therefore we can only test for equality but we do not have "<". That is the important difference to the second example in the question!
I believe that a similar "constant time via fixed number of arithmetic operations" statement applies more generally for $\mathbb F_q$ where $q=p^n$ where $p$ is prime. note that if $n$ isn't fixed, this statement no longer is valid
Typically, evaluating a polynomial at certain values is easier than identity testing, especially when the representation of the polynomial is via a circuit (some succinct representation). However, there are lots of randomized identity testing algorithms (Schwarz-Zippel being the most straight-forward) that works on just evaluations. 
There has been more progress in black-box identity testing algorithms. Right now, most of then stand at restricted depth 3 circuits (sum of products of sums of variables). (FWIW) Some of this is mentioned in more details in Chapter 3 and 4 of my M.Sc thesis. And there has been further improvements by Saxena and Seshadri recently as well. 
Now take the typical path, i.e., the path taken by almost all inputs (we always follow the "$\not=$"-branch). Furthermore, take the typical path of all elements in the variety 