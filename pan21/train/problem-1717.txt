The following STACS '97 paper might be interesting for your case: The Complexity of Generating Test Instances.
Before [AKS], Primality had coRP and RP algorithms (Miller-Rabin for coRP, Adleman-Huang for RP). A natural zero error extension would be to run both simultaneously until you push the error down to $\frac{1}{n^c}$ or so, and then run the brute-force algorithm. The expected running time would then remain $\text{polylog}(n)$.
Abstract: Recently, Watanabe proposed a new framework for testing the correctness and average case behavior of algorithms that purport to solve a given NP search problem efficiently on average. The idea is to randomly generate certified instances in a way that resembles the underlying distribution. We discuss this approach and show that test instances can be generated for every NP search problem with non-adaptive queries to an NP oracle. Further, we introduce Las Vegas as well as Monte Carlo types of test instance generators and show that these generators can be used to find out whether an algorithm is correct and efficient on average. In fact, it is not hard to construct Monte Carlo generators for all RP search problems as well as Las Vegas generators for all ZPP search problems. On the other hand, we prove that Monte Carlo generators can only exist for problems in co-AM.
Of course, there are polynomial-time interior-point algorithms, so it's not exactly what you're looking for.
There is a string matching algorithm (Nebel, 2006) that uses probabilistic ideas. I do know wether this is the fastest approach existing, though, and you apparently need some samples for training.
How about Kelner and Spielman's randomized polynomial-time simplex algorithm? It finds the optimal vertex of a linear program. No deterministic simplex algorithm is known which is proven to run in polynomial time, and for many of them, pathological instances can be constructed that make the algorithm take exponential time.