Write-back caching on a controller can give you a performance win at the expense of creating some (reasonably unlikely but possible) failure modes where data could be corrupted.  The biggest performance win for this is on highly random access loads.  If you want to do this, consider putting the logs on a separate controller and disabling write-back caching on the log volumes.  The logs will then have better data integrity and a single failure cannot take out both the log and data volumes.  This allows you to restore from a backup and roll forward from the logs.
I have seen an instance some years ago where this was done on LFS file system on a Baan installation on a HP/UX box.  The system had persistent performance and data corruption issues that went undiagnosed until someone worked out that the file systems were formatted with LFS.
I've seen to have remembered that a tweaked FreeBSD will give you a bit more performance as opposed to other OS. Though I am sure that this info is outdated and probably a myth in the first place.
But having the pg___xlog on a software ramdisk is not an option: If you lose the contents of the pg_xlog directory postgres will not start. (But there exists hardware ramdisks with battery backup that might be of interest.)
RAID-5 volumes incur more write overhead than RAID-10 volumes, so a busy database with lots of write traffic will perform better (often much better) on a RAID-10.  Logs should be put physically separate disk volumes to the data.  If your database is large and mostly read only (e.g. a data warehouse) there may be a case to put it on RAID-5 volumes if this does not unduly slow down the load process.
IMHO: Use the filesytem you are most comfortable with for the database files. Move the pg_xlog (with a symlink, see documentation) to the fastest possible device you have. 
Database management systems implement their own journalling through the database logs, so installing such a DBMS on a journalled file system degrades performance through two mechanisms:
Volumes holding database files will normally have a small number of large files.  DBMS servers will normally have a setting that configures how many blocks are read in a single I/O.  Smaller numbers would be appropriate for high volume transaction processing systems as they would minimise caching of redundant data.  Larger numbers would be appropriate for systems such as data warehouses that did a lot of sequetial reads.  If possible, tune your file system allocation block size to be the same size as the multi-block read that the DBMS is set to.
I had a small test program that created 50 threads, where every thread inserted 1000 (or if it was 10000) rows into the same table.
Some database management systems can work off raw disk partitions.  This gives varying degrees of performance gain, typically less so on a modern system with lots of memory.  On older systems with less space to cache file system metadata the savings on disk I/O were quite significant.  Raw partitions make the system harder to manage, but provide the best performance available.