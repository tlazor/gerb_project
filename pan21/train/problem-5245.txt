To me, the fastest way to analyze tabular data (rows and columns) from the same or different Excel files is Tableau.
Bonus trick: once you're happy with your view (columns and rows) you can download it as a CSV. Now you have exactly the data you want in CSV. 
Create the base columns you are using for your final headers. Loop through the headers of each file and match them to the base column with the most common amount of characters. 
Simplified: load your Excel files and create a key to join (e.g. a new column called 'axel temp font'). Join on this key. Once you've done this successfully you can bring the columns and rows (called measures in Tableau) into your view to analyze.
It is difficult to give you a good answer without knowing the dataset. However this is how I would approach the problem: 
I see three main ways of comparing columns: automated comparisons of column names (e.g. regex,  Levenshtein distance), comparing content (e.g. compare mean and standard deviation of the data for the column; if the mean value of a column is 10,000 then it probably isn't recording Front Axial Temperature), and manual comparison. You can combine these, for instance clustering on column names and content, then manually looking at the contents of each cluster. The smaller the number of different column names, the more you can rely on manual examination. You may also be able to get other metadata sources, such as looking for documentation for whatever process generated the files.