To give a comparison, for RAID5 you need two drive read and two drive write. You read parity and old data, that is enough to recalculate parity for new data, then write new parity and new data back.
On par they work only in massive sequential writing, where you write complete stripes, so RAID6 doesn't need to read anything, just calculate syndroms for each complete stripe and push everything down onto drives.
Degraded RAID10 works almost like optimal; degraded RAID5 and RAID6 generally for reading of some blocks need to bother all the drives to recover data from other strides and syndroms, so they became very slow. Under recovery pressure they become even slower and again bother not onle one paired drive, but all other drives.
Look: to update a stride in RAID10 case you have only two drives involved, and no reading (you just writing the same data on both). To update a stride in RAID6 setup, you have to read other stride from the same stripe, calculate syndroms and write on three disks (data and updated syndroms), so you bother all disks.
For 6 disks situation is similar: for RAID10 you still have to bother two disks, for RAID6 you have to read all drivers except three (one you updating now and two with syndroms) to load full stripe, to be able to calculate new syndroms. Then you write onto three drives - updated data and two syndroms. So one stride update still bothers all drives in the array.
For reading all RAIDs in optimal state works like RAID0, because all drives could be read in parallel.