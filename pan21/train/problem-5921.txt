Once you have each individual element tuned, then you look at things like load balancing.  In the case of having dozens of servers, using load balancing to share resources better is a viable approach.  In the case of a small number of servers as seems to be the case here, it is better to think of load balancing as strictly a failover/availability measure, not something to use to compensate for performance issues.
In some cases optimizing the web server layer can be helpful.  Likely that is not a concern for you based on what I can see here.  (Technically, even tuning the "web server" layer has multiple places to optimize - e.g. do you put a proxy in front of it, separate static content, that kind of thing.  But I think you're not in a place to worry about that yet.)
For a CMS like Drupal, you're almost certainly using the database for everything, in which case you likely need far more physical disks to have the speed you need.  The specifics of that, unfortunately, are dependent on a number of variables including whether your site is mostly reads, mostly writes, or a combination, as well as how much data is being transferred in requests (if everyone gets the same front page, that's one transfer and things are likely cached vs. if everyone logs in and gets individual content, the data access will be all over the place).  You should start by optimizing your database server as much as possible, which will require looking at documentation specific to your database software and hardware.
There are two layers here it appears:  the web server layer and the database server layer.  Load balancing web servers will not, for example, help if it is your database server that is having performance issues.
You want to make sure each layer is configured to properly serve what is needed.  In this case the first poster's comment about disk performance is likely spot on.  Disk space is not sufficient for selecting a storage configuration, generally IOPS or throughput is the most important part.  Also, in the case of SATA vs. SAS, you want to use SAS disks because the storage interface supports more simultaneous connections.
Conversely, if the servers are basically idle but the site is slow, then the fact the servers are idle is irrelevant - there is still something non-optimal in the mix.
First rule:  if a site works fine for an end user, backend statistics are irrelevant.  They are useful for capacity planning but if a server is running 100% CPU and the site works fine, technically you do not have a performance problem.  You have a "no excess capacity if something goes wrong you're in trouble" kind of problem, but not a problem in the moment.
So basically, tune your stack outward from the data.  The information you gain from that process will likely guide you in the process of determining how or whether to implement a specific load balancing solution.
Bunch of things going on here.  In the general category of performance tuning, there are multiple levels/steps in the process.  You seem to be skipping to the last step first.  Generally this is how I approach it in my work: