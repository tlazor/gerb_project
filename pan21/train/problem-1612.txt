Also your local disk will always be faster than your SAN in your setup, because even a SATA disk will have a maximum of 3Gb/s bandwidth, so your SAN will never match the speed of your local disks. You probably are also using ethernet instead of fibre which is also not help performance.
Which load-balancing policy are you using for your iSCSI NICs - in almost every situation one device talking to another device - each using any of the forms of bonding/etherchannel/teaming/whatever will only ever use one of however many links are available to talk - i.e. never more than 1Gbps, which by the way is almost never fully available, especially with iSCSI due to all the encapsulation involved.
Multipathing may be causing your issue.  Are you able, and have you tried, to disable multipathing and just have one 1Gb connection to your SAN?  VMware may be path thrashing when put under load because of a bad link, or a delay in packet delivery...   
You use a SAN not only because of the speed, but to have a central managed place where you can put all your importante data and make sure a suitable RAID level is being applied. There are also certain features like replication which is one of the advantages of having a SAN.
That SAN hardware is certified for Vmware so get your support to look into it. Common causes for bad performance are overload the interface of the SAN hardware, because if you have multiple connections to the same SAN not all can be served at the maximum speed. 
BTW, your maximum throughput with a 1Gb link is going to be ~30MBytes/sec if your SAN and ESXi host were the only two devices on that link...