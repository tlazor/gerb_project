Some time ago for a small static website we used a fully readonly filesystem: CD-ROM. It was highly cached by the kernel, so the speed was sufficient. 
This is an extremely open ended question, and the best answer is really "It depends." What you do to protect your site depends on a lot of factors.
Are you hosting a site that is more dynamic and allows user content, like a blog, wiki or forum? Then you have a lot more to be concerned about - picking a software package that has a good record for security, keeping it up to date when patches are released, and following guides for configuring it securely. Rename the administrator account and use strong passwords to start.
You really haven't provided enough information for someone to  give you details on how to specifically help you, though. 
Are you on a shared hosting plan, VPS, or dedicated host? If you're on a VPS or dedicated host you're responsible for that machine's security - meaning configuring a firewall, host based IDS, locking down any open ports and using strong authentication, keeping your patches up to date, etc. If you're on a shared host - do they have a good record for security? 
You have to understand that there is no such thing as security, just like in real life. Everything breaks, everything can be exploited, an attacker just has to be smarter than the code author(s). The idea is to layer your approaches that it makes the attack infeasible to the attacker without outspending what the data is worth.
Do you have a brochure-ware static HTML site that's only updated through FTP transfers of the latest revs of the files?  Then you need to stop using FTP, use SFTP/SCP and key-based authentication and disable password authentication. 
Seriously, though? Use the latest up-to-date software that has at least been run though an external code auditing review, keep it up-to-date as new releases are released. Don't use unauthorized, un-audited third party software for above software. If you run php/perl/python/ruby run it through the same process as above. Static pages can't be exploited, but the server still can. If you have remote access to this server, limit it as best as possible via firewall rules. You could also setup with a company that does remote website scrapping and compares your current page to last known good one. There are several free and several commercial.