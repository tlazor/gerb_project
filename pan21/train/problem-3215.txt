No, there have been improvements in how it handles the situation but MongoDB will currently just use all available space until there is none left - you need to monitor your disk space independently.  
This completely depends on your data requirements and how many documents you intend to insert.  If you intend to have 5GB of documents then you should be fine, even with overheads for replication (oplog is 5% of free space) and storage (there is always an empty file pre-allocated for each database).  If you plan to have 10-12GB of data (and remember you have to store indexes, journal, logs as well) then I would go for a larger disk.  
Finally, get some idea of your working set size for your database (per shard) - that will dictate how much RAM you need, which is really the key to selecting instance size on EC2 for MongoDB.  You may have enough with 8GB, but if not you will see significant performance hits for hitting disk.
Generally I would recommend reading through the whitepaper and following the guidelines therein - you'll find recommendations for Linux settings (readahead, hugepages etc.), storage, pIOPS and more.  Also worth checking out are the Production Notes - some duplication, but it's updated more often than a whitepaper.
Since you say you plan to grow to 1TB in a year then you will probably exceed 20GB inside a month and need to increase disk anyway, hence it will probably be easier to go for 100GB immediately.  At 1TB in a year, assuming constant growth, that will only give you about 1 month of room (1TB per year ~= 83GB per month).
Never use micro instances for anything in production - in particular do not use them for config servers.  Your config servers are critical for the operation of a sharded cluster.  But, no need to take my word for it - see page 6 of the updated Amazon whitepaper: