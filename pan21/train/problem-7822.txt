So, if your system enters OOM phase, consequences for Java processes could be that they get killed instantaneously - which wouldn't get you 'Out of memory' messages in java logs you are observing.
If the situation occurs that actual memory usage grows over the amount of RAM that system can provide, kernel will activate OOM killer which will try to kill processes to free memory. It will usually kill the youngest processes consuming large amounts of RAM, but you cannot depend on it. It can (and will) cause havoc. You can modify affinity of OOM to kill specific processes by adjusting /proc//oom_adj (for example if you want to avoid situation where OOM kills database or some other large RAM [ab]user).
Setting both Xmx and Xms to same value will prevent heap resize, but that doesn't mean java process will start using all the memory all at once at the startup. It will allocate as much as it needs VIRT memory, but resident data set won't grow up to Xms but will stay as low as needed.
In terms of virtual memory: kernel will promise (overcommit) to java process as much as it asks for (Xmx + some additional), but all that memory won't get allocated immediately. Amount needed for current data will be allocated only, and you can see how much by observing resident set size (non-swapped physical memory a task has used). To see the VIRT and RSS sizes you can run the following command:
Overcommit is by default enabled on Linux in heuristic mode. That means that kernel will usually allow overcommit - meaning that will promise more memory to all of the processes requesting it then it actually can deliver, in a hope that processes will never actually start using all the memory at the same time. Maybe overcommit is disabled on your server, you can check it by running:
By all probability, errors you are observing are indication that program running under java virtual machine process lacks heap space. Try by increasing the Xmx setting and re-test your app.