This is nicely covered in the answers to the How to complete a git clone for a big project on an unstable connection? Stack Overflow question.
Can I request the server to not compress the objects to a pack file while git cloning. If it depends on server, does github and aosp support this?
I noticed most of the projects are downloaded with the remote compressing of objects into a pack file and then simply getting the pack file and extracting the pack contents.
I have also noticed very few projects actually getting downloaded without the remote compressing objects into pack file. For these I don't have the .pack and .idx files. But many individual folders and files inside those folders are created inside the objects folder. I have a few questions regarding this approach.
Since you mentioned you can't/don't want to use the workarounds that @anr mentioned were listed in https://stackoverflow.com/a/3957733/2732969 (have the repo owner create a bundle of the repo and put that as a regular file on an HTTP server, clone to a shallower depth) your options are limited...
It does not matter how you are doing the clone (shallow or normal) at the time of writing (27th June 2018) resumable cloning is NOT supported by git. If the cloning process is interrupted nicely git will throw the in-progress repo away and if it is interrupted abruptly you will have to throw the repo away by removing it (as it will be corrupt) and start the clone again.
With this, I get .pack and .idx files in .repo/objects/pack/ folder. There aren't any other folders inside objects folder. And when downloading this pack file, if connection breaks , then it is not resumable .
Will this clone resume when interrupted ? I'm not asking about single file object resume capability. But when interrupted and attempted to redownload, will the already downloaded files be skipped ?? And how is this affected with normal clone and shallow clone ?