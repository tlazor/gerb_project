I should think the testserver isn't bothered by the location the requests are coming from, and I assume that testserver and server1 and server2 are not reaching their bandwidth cap (page isn't that heavy).
Also, it depends how many apache workers are currently started. If there are 10 running, they just have to server the page. If you start 20 parallel connections, 10 more workes have to be started, which uses resources (or the requests have to wait).
When you run 1 process, the whole cpu is just working for that one process. (ok, and some for the kernel). When you run 2 processes, the kernel must switch between them, and on every switch it has to store some data to ram, and then retrieve it, which causes some overhead, so - more processes - more overhead.
There are some types of load where increased concurrency leads to performance drop. First come to my mind is sequential HDD read - you will get best overall performance if you have one thread of reading large file.
Also 10 requests is too few to make any conclusions. Proper testing require you to monitor system in process and identify warm-up period for your system, when your load factors are stabilized. After warm-up you can run actual tests, and statistically research it's results to be sure that they are valid.
This question bothers me, and I do not think I am going to find the answer myself, so I thought it might be best to look for help.