A fast solution: If you use SKLearn's Random Forest, I highly suggest "class_weight" => “balanced”. 
Personally, if the metrics will just be used by you to evaluate the model, I would use the sensitivity and specificity within each class to evaluate the model, in which case, I care less about the balance of the classes in the test data as long as I have enough of both to be representative. I can account for the prior probabilities of the classes to evaluate the performance of the model.
That said, it sounds like your test set is drawn independently of the training data.  If you are going to balance the training data set, why not draw one balanced data set from the raw data and then split the training and test data?  This will give you very similar class populations in both data sets without necessarily having to do any extra work.
Your model could have very dissimilar performances at detecting "0" or "1" samples without you noticing if the performance is not high enough.
There are several ways of balancing classes. Either you can increase the number of samples of minority class or decrease the number of samples of majority class.
I would include more samples in the training and subsample "0" (to keep balance of "0" and "1" in each step) during the training and would evaluate the model with a balanced test set. 
Once you have balanced your classes, for a start you take 80% of it as train and test it on remaining 20%. In your case 4519 samples are 1 and 18921 are 0 so lets say you upsample 4519 and now you have 15,000 samples of class 1. You will take 80% of (15000+18921) and train your model. Test it on remaining 20%.
Recall becomes important when you are let's say identifying fraudulent transactions out of several millions. You will have very few classes labelled as 1 (fraudulent). So even you simply label all the classes in your test data as 0 you will be more than 99% accurate but you should ask yourself the question "Out of all the actual classes with label 1 how many did i get right from the trained model". That is what recall gives you.
And really, it depends on the audience for, and interpretability of the model metrics, which is the thrust of your second question:
On the other hand, if the metrics will be used to describe predictive power to a non-technical audience, say upper management, I would want to be able to discuss the overall accuracy, for which, I would want a reasonably balanced test set.