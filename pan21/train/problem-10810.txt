I do not know exactly why... but the system become more responsive with hyperthreading off (on my i5 notebook and massive Xeon servers with 60+ cores). I guess that has to do with per-cpu caches, per-cpu memory allocation, cpu scheduler allocation and process priorities complex iterations. I think the benefits of hyperthreading is outweight by the complexity of making cpu schedulers that know how to use it.
To disable hyperthreading I include a script on machine /etc/rc.local. It is not exaclty clean, but is easy to install, independent of cpu architecture and should work on any modern linux distribution.
In the first scenario the hyperthreading is uselles, the background tasks will use expensive context switches because I maxed out hyperthreading with the normal processing. The second is unaceptable because up to 50% of my cpu power gets prioritized to the background tasks.
Linux kernel information and controls can be accessed as files in /sys directory on modern linux distributions. For example:
For me, the problem with hyperthreading is: If I start as many cpu-intensive threads as I have logical cores, I will have fast context switches for the cpu intensive tasks, but expensive ones for the background tasks since the hyperthreading totally consumed by the cpu intensive tasks. On the other hand, if I start as many cpu-intensive threads as I have physical cores I will have no context switches to those tasks and fast context switches for the background tasks. Seems good, but the background tasks will found free logical processors and will run almost imediatedly. It is like they are realtime performace (nice -20). 
The "cpu-intensive" tasks I am talking about are artificial intelligence data mining and authorization servers (my work). Blender rendering in cheap computers and clusters (to sketch my future house). 