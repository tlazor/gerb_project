You need an Amazon AWS account to store your files on S3. Then a simple command can run your backup, even incremential or as a sync solution, e.g.:
rsnapshot.org or writing your own using the techniques from http://blog.interlinked.org/tutorials/rsync_time_machine.html
In order to then create a backup, I'd recommend running the tar command in your question on the remote machine so you still have an archive per-day, but your bandwidth usage is the lowest possible. You can include SQL database dumps (not the raw tables!) in your rsync backup, but ensure they don't end up in the public facing directories!
--link-dest= allows you to hardlink so that your new backup contains hardlinks to the unchanged files, and the actual file for something that has changed since the last backup.  Only the changed files are transferred after the initial backup.  Deleting a previous generation doesn't affect any files in later backups and only removes the hardlinks.  Disk space taken amounts to the full backup + the generational changes.  Ideally, you could keep many weeks/months of backups depending on available space and how much of your dataset changes.
How do you backup your site? I am especially interested when you have a large site (20GB+) with thousands of files? 