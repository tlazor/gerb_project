There are lots of clever ways to extend the Levenshtein distance to give a fuller picture. A brief intro to a pretty useful module (for python) called 'Fuzzy Wuzzy' is here by the team at SeatGeek.
It's in memory, so you probably don't want to use it to match datasets which are above about 100k rows.
A couple things you can do is partial string similarity (if you have different length strings, say m & n with m < n), then you only match for m characters. You can also separate the string into tokens (individual words) and look at how sets of tokens match or arrange them alphabetically and order them.
I've also written a similar project specific to UK addresses, but this assumes you have access to Addressbase Premium.  This one isn't in memory, so has been used against the 100m or so UK addresses. See here:
If you want to get this going quickly I'd reccommend using libpostal to normalise your addresses and then feed them into my generic fuzzymatcher (pip install fuzzymatcher).
I've written a generic probabalistic fuzzy matcher in Python which will do a reasonable job of matching any type of data:
As you are using R you might want to look into the stringdist package and the Jaro-Winkler distance metric that can be used in the calculations. This was developed at the U.S. Census Bureau for linking .