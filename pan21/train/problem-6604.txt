While 8 bits is almost enough precision to display an image, it is definitely not enough precision to compute an image. At various points while computing an image, at least 32 bits of precision is required. 
HDR refers to the act of rendering to images which have higher than 8-bit precision. In contemporary TV videogames, 16-bit precision is the norm, and this may be "enough" in videogames for years to come.
This is easy to fix by using floating point values for the red/green/blue values - but now you have the problem of how to display that on a graphics device that only handles a fixed number of discrete values per channel (eg. 256). So the second part of the problem is how to map your floating point values back to the limited range. The trivial solution is to scale all the values proportionately into the discrete range, but this would mean that 1 very bright pixel could make the rest of the screen black, etc. Sometimes this is what you want, sometimes it's not - see CiscoIPPhone's tone mapping link for examples of how you can approach this.
This is all fine if you just render bitmaps (well, most of the time), but if you use them as textures in a 3D scene it is a different story. If you want to model interaction with light correctly, you should use linear light calculations throughout the rendering pipeline. This means 
There has been a trend in realtime 3D graphics toward keeping all graphics in >8 bit precision until the last possible moment, at which time the >8 bits of red are downsampled to 8 bits, and so forth for green and blue.
Computers traditionally represented each pixel on screen as only 24 bits in memory: 8 for red, 8 for green, and 8 for blue. This is almost enough bits that a human wouldn't notice if you added more, and the 8-bit byte is very convenient for microprocessors, so that's what stuck.
When you make this change to an existing scene, with existing artwork, lights, etc, you probably have to fix a lot of your light intensities and textures, because they were chosen to look nice when rendering with non-linear light. So it is not a feature you can just "turn on" and expect everything to look better just like that.
This is why pixel shaders compute colors in 32-bit precision, even when you are rendering to an 8-bit precision image. Otherwise, you couldn't for example divide a value by 1000, and then later multiply it by 1000, because dividing any 8-bit value by 1000 results in zero. 
The monitor you are looking at produces light as a function of the input pixels. You might expect that a pixel with value 255 will produce (approximately) 255 times more light than a pixel with value 1. This is not the case. With a standard monitor gamma of 2.3, it is 255 ^ 2.3 times brighter, or about 340000!
It's generally not your textures that need to be stored in a new format - it's when the lighting is applied to them that you need to be able to accommodate larger values. Obviously however if you have light sources baked into a texture - eg. a starry background - you might want a higher resolution format there. Or just have the shader scale the values of such materials up when the time comes to render them.
Technically, HDR merely means using a larger range of possible values for your graphics. Usually you're restricted to 256 discrete values for the red, green, and blue channels, which means that if you have 2 items, one twice as bright as the other, and a 3rd which is 10,000 times brighter than the first, there's no way you can represent all 3 in the same scene correctly - you either make the bright object only 256x brighter than the first instead, or you make both the dull objects completely black (losing the contrast between them) and then the bright object is infinitely brighter than them both.
Everybody producing content (camera manifacturers) knows this, or (if you're a designer) you implicitly compensate for it. 