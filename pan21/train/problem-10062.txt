You should only be doing backups on SECONDARY nodes. The PRIMARY already has enough work performing inserts and writes. If your read preference is PRIMARY or PRIMARY Preferred, mongodump will compete with all find() and findOne() operations.
If you want to make a snapshot of your replica sets as backup you can use the MongoDB Cloud Manager Backup. 
If you do not change the SECONDARY into a hidden member and you launch a mongodump against it, all queries to that node will slow each other down for sure.
Alternatively you can use the OpsManager, it has similar functions as the Cloud Manager but requirieres an Enterprise Advanced subscription.
If your read preference is SECONDARY or SECONDARY Preferred, you need to designate one of the SECONDARY nodes as a hidden member. This should make this node not be selected to perform any queries but would still just perform replication. This makes a hidden node a prime candidate for doing backups.
I also run a three node replica set.  We back up the entire instance, not just one database, but conceptually there's no real difference.  We use a scripted mongodump pointing to a secondary replica and writing out to a network storage location.  Do you have any performance metrics on how long your backup usually takes?  Also, the CPU, memory, and network speed of the replicas will affect the time to back up and performance, so the impact of your 30 gig DB backup will depend heavily on these specs.  Also, if you have readable secondaries that will impact backups depending on read only traffic to them during dump operations if you backup from a secondary replica.  In any case dumping from the replica will isolate your production primary from the network load of the dump operation.
Also you can use mongodump to backup your data. However, for replica sets, the MongoDB Cloud Manager or OpsManger should eb your first choise.