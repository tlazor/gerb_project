Cloning/Mirroring solutions such as dd may not be able to provide incremental backup. If you want to have a backup at different points, you need to use a tool like rsync or a complete backup solution like backuppc.
Don't use dd or dump - in addition to no error checking, there can be portability issues with these too. cpio and tar have been around for a long time, tar can tricky when bitflips / corruption occurs. I've been using afio for some time - it allows individual files to be compressed (rather than tar compression where the output stream is compressed).
If it were me and there was a need to minimise downtime I'd break a disk out of the mirror (if there's a DBMS running on it then stop it first, break the mirror and restart to get a consistent snapshot). At most you're looknig at a few seconds of downtime. Then remount the filesystem from this disk somewhere else for backup. It can then be hot-joined back into the raid set. 
How much downtime can you afford? (if you want low downtime then you really must start planing for unscheduled outages - meaning you need at least one other machine).
There are ways to maintain the service when creating a consistent snapshot - but these are specific to the DBMS being used.
You can use Clonezilla to make a image of a disk or partition compress it and upload it to a FTP or windows share. Read the documentation, because the software is very flexible.
As for getting the data off the hard disk....for a DBMS, use the DBMS tools for generating a backup - a file image may not be portable across different versions of the DBMS. Every DBMS I've come across will allow you to run 2 instances on the same machine (as long as they are using different data files / network sockets). 