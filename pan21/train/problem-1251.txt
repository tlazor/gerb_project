The cons of this setup is that if a split-brain happens (it will), it can be difficult to fix since data may exist on both nodes.
Basically, we have two storage LVM pools on top of two MDADM Raid 1 arrays. They're exported to DRBD to do block level replication. Then, you add LVM on top of DRBD to enable snapshots of the VMs and direct filesystem access to the VMs. Why two?
When choosing an HA solution, you'll need to decide what level of downtime (if any) is acceptable. This will affect the complexity of your setup.
For the highest availability setup without shared storage, you'd have to use DRBD in primary-primary role. This will require a STONITH device to kill off a non-responding node. IP based battery backup devices can generally handle this function efficiently. Pacemaker and corosync can handle bringing up the VMs and managing the resources.
The original idea was to create a DRBD resource for each VM, so that the machines could be moved between hosts depending on load and not have one host sitting idle. Administering it was a pain, so two DRBD resources of 200GB each was a good compromise. That way, r0 can be primary on node1 and r1 can be primary on node2. If node1 fails, we run our "make master" script on node2 and it handles the LVM mappings, setting DRBD primary for those resources and telling virsh to startup all the nodes. On an SSD array, I can bring down a dozen VMs and bring them up on node2 in about 2-3 minutes.