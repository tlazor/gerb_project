Till you hit the so-called the curse of dimensionality for your exercise. Although I am not yet sure what features/num_samples ratio exactly causes the curse of dimensionality! What I have gathered so far about the curse of dimensionality has been very subjective. 
returns array([ 2.00000000e+00, -1.30768001e-15]), meaning that the coefficient of the new feature (the one with random values) was practically set to $0$.
Adding noise is not the same as changing the dimension of the feature space. If the data is linearly separable in the original feature space, it will be also separable although you add an extra random feature. Take a look at figure 1 and figure 2. Figure depicts the scatter plot (var1_1 vs var1_1) of a linear separable data in a one dimensional feature space. Figure 2 depicts the scatter plot of the same feature space with an extra random feature, now the dimension is 2, but the data is still linearly separable. You only have to look at the projection of the data in the var1_1 axis.
It depends on what is understood as noise, since a noise source can be interpreted as any way of corrupting/altering the data.
If you want to evaluate the robustness of your prediction model against noise, I will take option 1, since it not straightforward to derive what kind of noise to apply in the feature space. If you are working with images, you can blur them or if you are dealing with audio files, you can add white gaussian noise, or another kind of noise source, for example another mixing the original audio files with other sound sources.
Figure 2:Scatter plot of separable data in a 2-D feature space, which is the same as the previous space plus an extra random feature
Then, if you try plotting y against x, you'll see that the values don't lie on a perfectly straight line, but rather they deviate from it slightly (and randomly).
In addition to what Lupacante conceptually and nicely showed such that the added feature(s) has(have) to be informative for the model otherwise it can get ignored by majority of models (perhaps easily by regularized models), I would like add that you also increase the dimensionally of the feature space synthetically using many simple mathematical expressions as well. Concertedly, let's say your only feature (column) is: 
If now i want to introduce some noise in this dataset, is it correct to add another feature with random values to my dataset ?
The problem with adding an extra feature with random values is that, if it's uninformative (as it likely is, given that its values are all random), it might get ignored by your classifier.