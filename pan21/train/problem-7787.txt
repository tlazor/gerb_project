Is there any implications of storing such a huge number of files? Will the filesystem be victim to fragmentation?
Fragmentation ocures when a file already on disk is changed such that it becomes larger than it previously was, and no file smaller than your cluster size should end up fragmented. to correctly answer your question, we need to know your cluster size, the percentage of the drive in use, and whether the files get changed after the initial allocation of disk space. 
The access speed decreases significantly. It is a common pain in Java projects, where you have tons of relatively small files. It is more efficient to store them in one single archive, the access is faster in spite of the fact that they have to be unpacked.
Ext4 in particular doesn't require defragmentation the same way windows file systems do because of the way it allocates space to files, so as long as your partition is not 90% full, I wouldn't worry about that.
I'll add that you should watch for the maximum number of files your filesystem can handle. Files (on ext4 which you use) consume inodes. The inode table is statically allocated when the filesystem is created and the only way to increase it is to recreate the filesystem.
I'm working on an archival project and currently I'm storing 514600 images (ranging from 200kb to 2mb) on my 4TB NAS drive. The drive itself is formatted as ext4 and I'm writing the files over the network via cifs/smb.
IO operations on many small files vs a few large files are much much slower, due to all the files system overhead (and scan/seek time), so if you consider that degraded performance, then yes. it really depends more on whether you are accessing 5000 of them at once, or just a few at a time. 
You work on an archival project, so you should probably use ZIP or TAR/GZIP. If your files are already packed, you may use just TAR to stick them together into one bigger file. Find some reasonable "unit of packing" - so you may easily locate all your files later.