What the heck are you talking about? Not sure where you got your information from, but you should certainly discard that source. Nothing from what you assume here is actually correct.
Second, if using one file per table is advantageous, why does create table only give me the option to allocate the table to a file group and not to a specific logical file? This would require me to create a separate file group for every file in my scenario, which suggests to me that perhaps SQL Server isn't envisioning the advantages I am assuming would come from doing what I'm proposing.
Lastly, when it comes to squeezing every drop of performance out of a Sql Server, I refer you to the following chart (provided my Microsoft):
Any potential optimizations that could be made from an application perspective easily dwarf any possible optimizations at a hardware / database configuration level... so focus your attention appropriately.
As I really want to squeeze every last drop of performance out of the hardware I have available (about 64GB of RAM, a very fast SSD and 16 cores), I was thinking of allowing each table to have its own file so that no matter if I'm joining on 2, 3, 4, 5 or more tables, each table will always be read using a separate thread and the structure of each file will be closely aligned with the table contents, which would hopefully minimise fragmentation and make it faster for SQL Server to add to the contents of any given table.
One caveat, I'm stuck on SQL Server 2008 R2 Web Edition. Which means I can't use automatic horizontal partitioning, which rules that out as a performance enhancement.
Going through all these presentations you will quickly notice that they all focus on writes since this is where SSDs performance comes into picture. Your post wording is almost entirely about reads, which is a different topic. If reads are your pain point then you should be talking about RAM, not about SSDs, and about proper indexing and querying strategies. 
I'm creating a database in which there will be around 30 tables, with every table containing tens of millions of rows and each table containing a single important column and a primary/foreign key column in order to maximise query efficiency in the face of heavy updates and insertions and make heavy use of clustered indexes. Two of the tables will contain variable-length textual data, with one of them containing hundreds of millions of rows but the rest will contain only numeric data.
Brent also has a nice presentation on the topic: SQL on SSDs: Hot and Crazy Love and there are more out there. 
My first suggestion would be to not make any assumptions about performance without doing load testing against both configurations.
As others have noted, there is no direct benefit from one file per table; here's a great synopsis from Steve Jones on how this myth originated: http://www.sqlservercentral.com/blogs/steve_jones/2009/10/13/sql-server-legend-data-files-and-threads/
If you want to read a good discussion of SSD performance for SQL Server there are several blog series out there. As usually, Paul Randal's one is the top read: 
You may also want to investigate a partitioned view which I believe is supported by 2008 Web Edition.  There are some tricks to coding against a partitioned view, but you can mimic a lot of the functionality of partitioned tables relatively easily.
My guess from having seen such configurations (that make sense on paper) in the past would be that having each table on a separate file wouldn't have a measurable positive impact for performance... and that the additional complexity would offset any performance gains even if they were measurable.
Will using one file per table actually maximise performance, or am I overlooking built-in SQL Server engine characteristics that would make doing so redundant?