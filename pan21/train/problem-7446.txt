At one point, there were two On-Demand and two Spot instances running. The system had just scaled down because the CPU load of the five servers became very low (around 35%). With the four servers, the CPU load went up and after a few minutes briefly crossed the 70% mark (maybe there was a very minor traffic boost at that time).
That is my expectation, but I did not find much details in the documentation to confirm it. Can someone shed some light on how the new Target Tracking scaling works in details, and how to apply it to a hybrid setup with Spot and On-Demand instances?
- Correct, the min and max are the bounds for the desired (it can't go below the min or above the max)
In my production service, I am using two separate auto-scaling groups to support hybrid auto-scaling with a mixture of Spot and On-Demand instances. What I want is that my CPU usage should not exceed 70%, and it should use Spot instances whenever possible but fall back to On-Demand instances if necessary.
The system decided conservatively to scale up again, but as both auto-scaling groups made the decision independently at the same time, two instances were started (one Spot and one On-Demand instance). At this point, there were now six servers running. After a while it scaled down again and finally reached a setup of running four instances.
1) If there is no spot capacity in any of the availability zones you selected for any of the instance types you selected, it will NOT fallback to on demand automatically.  So if you have it setup for 50% spot and 50% On-Demand, and the desired is 10, with no spot availability, you'll just have 5 On-Demand instances.  If you had enough different instance types selected I'd imagine this wouldn't be an issue, but who knows.
2) Most of the load balances on their use round robin or something like it for distributing connections to instances, so if there's 1 fast instance and 1 slow one, they'll both receive the same amount of connections and the slow one will eventually get bogged down
- Correct, the 'desired capacity' is what the scaling policy changes to make instances be terminated or launched
My assumption is that it should help to prevent that scenario I described. I would expect that the On-Demand group scales down earlier than the Spot group (which is desirable, anyway, as they are more costly). And I expect that the Spot instances scale up sooner, which should protect against unnecessary upscaling from the On-Demand group.
- AWS doesn't say exactly how it works, but it will create two CloudWatch alarms for each target tracking policy, one for scaling out and one for scaling in, you can check the thresholds on those to see when they'll be triggered
One thing you should look into is a new feature where you can mix Spot and On-Demand in a single AutoScaling Group now.  You can also have multiple instance types in one AutoScaling Group at a time.  So you could theoretically have a single group with a bunch of different backup instance types selected, using the 2 cheapest spot instances at any given time with the others as fallbacks if those two run our of spot capacity.  
- It would eventually, the spot instances would be terminated, which would cause increased load on the On-Demand instances, which would cause them to scale.
First, I set both Auto-Scaling groups (Spot and On-Demand) to use Target Tracking for 70% CPU load and set the minimum size of both groups to one. The traffic on my service is quite predictable (no sudden boost, more traffic during the day, few traffic during the night).
https://aws.amazon.com/blogs/aws/new-ec2-auto-scaling-groups-with-multiple-instance-types-purchase-options/