Noncritical data, logs, rollback datars and such can more or less go on "vulnerable" filesystems (memfs, fast-io-fs w/o journaling/preflight/precommit caching) while actual data files (depending on the flavor of db) are nicely spread on things as cheap as a well-constructed zpool.
They very often go on boilerplate recommendations that in the long run are rarely optimal.  Not because they don't know what they're doing -- but because they're putting trust in a middle-of-the-road doc pushed out by $vendor as "designed to annoy the fewest # of customers / resulting in less help calls".
They're infinitely smarter than the server admin when it comes to constructing a complex query that doesnt do a bazillion table-scans, but the hardware/server admin is infinitely smarter just the same in allocating proper resources, (assuming he has a gray hair or four). :)
My "ISO Layer 8" advice is this : Approach the DBA and say, hey, I'm not gonna tell you you're wrong, and in return, you're not gonna tell me how to architect my systems :)
My short answer: It's all about the spindles, and it's all about spindles, and as of the past few years, your choice of filesystem and how much ram that filesystem can suck up.
I've had awesome success segregating dbdata / logs / transactionals amongst (a) different physical controllers, (b) using tweaked filesystem params (specifically, and this is a big one, matching the params of your db writes/reads/commits to the same size sector/block that the fs was constructed under) and (c) "Choosing my poison".  
If you want to direct-msg me, feel free;  But keep in mind -- There's no global ideal config.  The number of rows, the anticipated scans, the cardinality and efficiency of your keys/indexes, queries/sec, full tablescans per interval, etc, all play into it.  It's a tough game to play.
The previous poster is absolutely right in that trans logs are sequential, and can/should go on volumes made for fast write, not read (and arguably not necessarily "stability",) such as a big stripe.  The responder ("here's the rub") is also in the right : Disk contention is nasty if the data isn't in ram or in disk-cache, and without serious (pronounced "long, monotonous, tedious and prone to guessing error") analyses, you absolutely should try to avoid mixing disks with databases that function differently.  (e.g. High trans read, and low trans bigdata write -- totally fubar's any cache strategy).