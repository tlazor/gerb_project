While it's possible to design computers that have different parts running at different independent speeds, arbitration of resources often requires being able to quickly decide which request to service first, which in turn requires knowing whether any other request might have come in soon enough to win priority.  Deciding such things, most of the time, is pretty simple.  Something like a "quiz buzzer" circuit could be implemented with as few as two transistors.  The problem is that making quick decisions that are reliably unambiguous is hard.  The only practical way to do that in many cases is to use a decide called a "synchronizer", which can avoid ambiguities but introduces a two-cycle delay.  One could design a caching controller which would reliably arbitrate among two systems with separate clocks if one were willing to tolerate a two-cycle delay on every operation to determine who won arbitration.  Such an approach would be less than useful, however, if one would like a cache to respond immediately to requests in the absence of contention, since even uncontested requests would still have a two-cycle delay.
Graphics is an example of a task that we have found massive parallel work to be efficient.  The GPU is optimized to do the kind of operations that we want to do graphics (but it isn't limited to that).
And someone here said mobile devices have multi-cpus with different speeds. That's just not true. Its not a central processing unit if it is not the unit of central processing; no matter what the manufacturer says it is or is not. in that case [not a cpu] its just a "support package".
The reason common systems have cores at the same speed is a simple math problem. Input and output timing (with optimizations) based on a single set of constants (which are scalable = multipliable by a number of units).
Unless we were extremely concerned about power consumption, it would make no sense to accept all the cost associated with an additional core and not get as much performance out of that core as possible. The maximum clock speed is determined largely by the fabrication process, and the entire chip is made by the same process. So what would the advantage be to making some of the cores slower than the fabrication process supported?
Now, mobile devices do this for a different reason.  They have low-power cores that are significantly slower, but use significantly less power per unit of compute as well.  This lets them stretch battery life much longer when not doing CPU intensive tasks.  Here we have a different kind of "large payoff"; not performance, but power efficiency.  It still takes a lot of work on the part of the OS and possibly application writer to get this to work right; only the large payoff made it worth it.
They have (set of) a CPU(s), with 1-72 threads active at once, and a (set of) GPU(s), with 16-7168 computing units.
It is arguably much easier to write code for a CPU, because massively parallel code is harder to get right.  So only when the payoff is large is it worth trading single-core performance for multi-core situations.  GPUs give a large payoff when used properly.
In general, trading one core at X FLOPS for three cores at X/2 FLOPS is not worth it; but trading one core at X FLOPS for one hundred cores at X/5 FLOPS is very much worth it.
Running everything off a common clock avoids the need for synchronization, which in turn avoids a two-cycle communications delay every time it's necessary to pass information or control signals between clock domains.
We already have cores that can slow down to save power. What would be the point to limiting their peak performance?
When programming for this, you generate very different code for the CPU and for the GPU.  Lots of work is done to divide the workload, so that the GPU gets tasks that are best done on the GPU, and the CPU gets tasks that are best done on the CPU.