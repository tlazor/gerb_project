We used this theorem to construct a combinatorial object that we used for proving the hardness of approximating Set-Cover. 
In retrospect, this may be obvious, but I've always been fond of Steele, Yao, and Ben-Or's application of the Oleinik-Petrovsky/Milnor/Thom theorem (bounding the Betti numbers of real semi-algebraic sets) to prove lower bounds in the algebraic decision tree and algebraic computation tree models of computation.
Another beautiful idea: Yao's idea to use minimax principles and the proof that mixed games have an equilibium (essentially linear programming duality) to show lower bounds on randomized algorithms (by instead constructing a distribution over inputs to a deterministic algorithm). 
e.g., in proving lower bounds for locally decodeable codes (see Katz and Trevisan), in Raz's proof of the parallel repetition theorem, in communication complexity (see, for example, the thread of work on compression of communication, e.g., the relatively recent work of Barak, Braverman, Chen and Rao, and the references there), and so much more work. 
There are many such examples. When I first learned complexity theory, I found it surprising that basic theorems about roots of polynomials (such as the Schwartz-Zippel-DeMillo-Lipton Lemma) had anything to do with the question of whether interactive proofs can simulate polynomial space ($IP = PSPACE$). Of course, those properties of polynomials had already been used in prior work, and nowadays the use of "polynomializing" computations has become quite standard in complexity theory.
Noga used algebraic topology fixed-point theorems to prove the "Necklace Splitting Theorem": if you have a necklace with beads of t types and you want to divide parts of it between b people so each gets the same number of beads from each type (assume b divides t), you can always do that by cutting the necklace in at most (b-1)t places. 