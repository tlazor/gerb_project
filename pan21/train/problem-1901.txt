I will be using the 2nd option, setting up Caching solution in front of Load Balancer. I am going to handle the Single point of Failure problem with adding one more node having standby and using something like HAProxy to keep checking the health of the machine. If first Caching machine went down, then HAProxy will automatically move the IP and move the traffic to the other Caching Standby Machine and hence the Single point of failure will be handled.
Also, in my scenario, I will be using something like Varnish, instead of Nginx+php, which I will suggest you too, incase your application is not dependent on Nginx. Varnish will have it's own built in Load Balancer, hence you will skip the AWS Load Balancer too.
I've set up this stack many times on AWS.  Option #1 has always worked well for me and is the one I normally choose because it's the simplest.  I'm curious to know how much traffic you're dealing with where the less-than-ideal initial cache hits are an issue?  I'm serving a few million pageviews a month on a pair of m1.small instances and they're barely scratched.