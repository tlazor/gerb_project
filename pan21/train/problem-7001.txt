Without knowing a lot more about your data and the specifics of what you're doing, that's about the best I can offer for suggestions.
If I/O is the problem and memory is not an issue, the use -S for the first sort to make sure everything stays in memory. Also you may want to use lzop every time you write to disk (--compress-program=lzop): Disks are often the limiting factor so lzopping on the fly can give you extra speed. Or you could make a RAM disk and set -T to that dir.
In searching around, I found lots of references to academic papers and one commercial product called Nsort. I don't know anything about it other than that their web site claims that:
[Note: I'm not an expert on sorting, so someone smarter than me may be able to point out errors in my logic, or suggestions to improve on these.]
Additionally, sorting doesn't paralellize as easily as compression because there's no proximity guarantees.  In other words, with compression/decompression, you can break the input into discrete chunks, and operate on them each separately and independently.  After each chunk is processed, they're simply concatenated together.  With sorting, you've got multiple steps involved because you can't just concatenate the results (unless you do some preprocessing), you have to merge the results (because an entry at the beginning of the 60GB could end up adjacent to an entry at the end of the 60GB, after sorting).
Hrm.  You're going to run into a few issues here, I think.  First of all, your input data is going to have a big impact on sorting performance (different algorithms perform better or worse depending on the distribution of the input).  However, a bigger problem up front is simply that 60GB is a lot of data.
GNU sort has -m which can probably help you out. Let us assume you have 200 .gz-files that you want to sort and combine. Then you could use GNU Parallel to do: