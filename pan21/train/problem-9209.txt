And finally, if all else fails, make it a scheduled downtime and serve static pages off a CDN if you have to. In the case of *nix hosts, 99% of the updates don't even require restarts aside from the daemons affected, so there's that too.
The point of clustering is to have room for growth. That is, you'd want enough spare capacity to handle a server suddenly dropping out of the pool - planned or unplanned. That said, you'd also want to do this during off-peak hours as well to reduce the chance of overload.
The "proper" way of doing updates is to take a server out of the operating pool, update it, then put it back in after the maintenance period. It's as easy as it sounds and often times you can find tools to do this for you. For example, in the use case of a website, typically you'd have a cluster sitting behind a load balancer (like haproxy, or even an appliance) and just taking the server offline is enough to re-balance the load.
I searched around and I was surprised that I couldn't find an answer to this question. My assumption is that you have multiple servers. Normally they both will be doing their specific take (for the rest of this I will assume a simple website). Now lets say server A & B need updates. Do you update server A while server B keeps pushing out the webpage and then when server A is okay you update server B? This seems like it would work in small scale but seems horrible in large scale due to the fact that you'd need twice the power that you normally have. When dealing with a large number of servers do you update small sections at a time? I thought the problem with this would be if server A can't work alongside server B C D E or F any-longer that's not that bad. But when you start updating you slowly lose this small percentage. 