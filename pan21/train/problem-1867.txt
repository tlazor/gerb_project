One other point - it may be that you don't get a significant gain from compression so it isn't worth turning out. Best practice to check the compression gain before enabling using the sp_estimate_data_compression_savings stored-proc.
The "sysobjects" view includes some but not all of the indexes.  We need "sysindexes" instead.  Thanks to the anonymous poster at aspfaq.com for this index insight.  We also want to ignore user-defined functions.
As an aside, be careful with enabling everything to be compressed. The data is compressed in memory and decompressed every time it is accessed. For an OLTP system with lots of changes and memory-resident data, compression is not suitable as you'll burn more CPU for no gain in IOs. For data that is read-occasionally, like a data warehouse, it's much more suitable because you get a big trade-off in reduced IOs against the extra CPU. Compression is a data warehousing feature, not an OLTP feature. Not sure if this applies to you, but worth pointing out just-in-case, and for others reading the thread.
I'd also mention that you should check to see whether or not the table is compressed before rebuilding it.
I'm a little late to the party, but here's a version that uses DMVs rather than the deprecated system tables and allows for arbitrary schema names. It enables or disables row or page compression on all heaps, clustered indexes and nonclustered indexes (including all partitioned tables) in the current database:
You should probably look to handle new tables as well, so you don't need to run this batch on a regular basis.  I detailed a method for automatically compressing new tables in this blog post.
I've just used the Works With SQL Server Tool to test after compressing using the a_hardin-splattne script.  The test failed because several indexes were not compressed.