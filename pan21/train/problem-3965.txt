I currently have a system setup where a client FTPs me files which triggers inotify (via Linux kernel notifications) to trigger a parse to take action on those files. The problem I'm running into is the parser is currently hitting I/O capacity on one EC2 instance and I'd like to add additional nodes to work on splitting the file load. The client unfortunately can only upload via FTP. This leaves me wondering how can I have another instance, that doesn't share the EBS volume that the files are being dropped on, work on that file.
If your client is able to access your ftp by DNS, rather than IP, it seems that the easiest solution might be to put an ELB in front of several ftp instances, so that you can scale horizontally.  
Can't you have a script to scp those file to the other node when inotify find there are new files being ftp-ed to your head node?
Is there currently an AWS solution that will keep my client who uses FTP not touched (besides maybe an IP change which is fine) and allow me to have multiple EC2 instances access the filesystem?
Then, if you need all of the ftp'd files to end up in one place when you are done processing, you can use S3 or any number of solutions to persistently store the processed files in a single location.