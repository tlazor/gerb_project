Trends for a certain "row" is easy as well as investigating how the row looked at a certain point in time.
There are two ways to run the deltas -- forward or backward.  Going 'forward', you would start with the original (complete) snapshot, then apply deltas until the desired time.  Going 'backward' has the advantage that the most recent snapshot is complete.  The going backward 'subtracts' off the changes.
So this creates the following problem: averting the need to store an endless amount of tables. I have read in various posts on Stack Exchange that it's bad architecture to have millions of tables in a database, and with such a design performance will suffer. So here are the two possible architectures I can think of:
In the load process each row is compared with the current row and if nothing has changed it is ignored. If something has changed the current row's end_time is set to now() and a new row is inserted with begin_time now().
Since you say "for years", it is probably wise to take complete snapshots every, say, day.  Then finding a particular 30-second second snapshot won't involve more than 2880 deltas.  This obviously leads to a speed/space tradeoff -- full snapshots are bulky, but infrequent snapshots leads to long 'reconstruction' times.
Until I see the actual queries, I will advise against PARTITIONing since it is usually of no performance benefit.
The link about table size limits is missing one number:  64TB is the limit for one non-partitioned InnoDB table.
I suggest looking into storing only the "deltas".  When some piece of the snapshot does not change at all, the delta is empty, and you can store nothing.
For the sake of it I'll describe an approach not mentioned above. It is typically used for temporal data. Not sure it will fit your needs, but here it goes. The idea is to have a copy of your original with two additional attributes, begin_time and end_time:
In addition, if I did not need the actual data to be query-able, I'd further 7zip the stream before saving it to the DB. No reason to just waste space.   
For reconstructing a snapshot at some point in the past, the processing is costly -- you need to walk through the versions, applying the deltas as you go.
Rather than "snapshotting", use a TRIGGER to build an "audit trail".  This is similar the "deltas" I mentioned, but it is better in that it is continuous, not "every 30 seconds".  The case I remember had over a billion rows in the audit trail; each row had (approximately) the timestamp, table name, PRIMARY KEY, and a compressed JSON blob of all the columns for that rows.  Your needs may be better served by some variant of that.
The z dimension will be time, and I would like to store as much as possible. We are hoping for approximately 1TB of total stored data with modest performance. In other words, we are looking to take a snapshot of one MySQL Table, every 30 seconds, for years. It will be the same table every time, and will be about 10 columns {x} by 1000 rows {y} (approximately 50KB). So if you will, we want to take a snapshot of a 50KB table every 30 seconds.
I am looking to create a large 3 dimensional database in MySQL. The structure will basically be a standard MySQL table, with a time component / dimension added. See the following analogy:
I would not store this data in MySQL but in a NoSQL system as json. This kind of problem is terrific for that architecture. Much more scale able than MySQL. 