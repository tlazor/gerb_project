This alone reduces the burden on system resources, but the fact that a lot of the time the GPU and CPU are talking only to each other, via extremely high-bandwidth, low-latency connections and caches on the die, means a lot of the traffic doesn't even leave the socket.
FSB and Northbridge are obsolete technologies. All modern systems work on the basis of a point-to-point interconnect (Intel's QuickPath Interconnect, AMD's HyperTransport etc.) which doesn't have a 'hub' like a Northbridge or a bus with limited global bandwidth as was the case with FSB.
On older chipsets (P4 era, some core2duo's) you are right. These are bandwidth starved and integrating a GPU on the same die as the CPU will reduce CPU performance. 
Note that memory access might not limit a CPU too much thanks to its caches. But it can limit the on-board GPU die. Hence the 4th level cache on Intel chips with Iris graphics. Ditto for AMD where higher speeds RAM means much better APU-gaming performance.
One theoretical way in which an integrated GPU might reduce the performance of the CPU is due to thermal limiting. If the GPU forces the die to reach high temperatures then the CPU may be throttled to reduce temperatures. Other than that I doubt an integrated GPU can measurably reduce the CPU's performance.
Consider the Intel HD Graphics. The video card resides inside the cpu die and uses ordinary system RAM. Therefore, it uses up Front Side Bus and Northbridge bandwidth. Therefore, accessing the main memory - the biggest bottleneck in modern computing, becomes event slower.
On more modern systems there no longer is a FSB (see @Lunatik's answer which is very good in that regard) and CPU performance decrease due to shared bandwidth to the memory should be minimal.