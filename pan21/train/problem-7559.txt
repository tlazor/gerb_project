Your question is sensible. The way in which posterior probability is calculated in the classical Naive Bayes classifier (in sklearn) is like  summation of the conditional probabilities of the all the features in the dataset. Even though the features are treated as conditionally independent, to learn the classification probability all the features are always used in this setup. Once the model has been learned you still all those features to calculate the posterior for a new observation. The conditional independence is just an assumption that is taken to make the statistics and math obey the rules and work.
But slightly modifying the way in which the posterior is calculated you can use Bayesian approach to make predictions even with the absence of certain features. Using Bayesian approach to make predictions in the absence of certain features is still an ongoing work. You may want to have a look at this paper in which Bayesian approach is applied to astronomy to do classification with missing values.
Naive Bayes can be easily implemented in python as it is a simple calculation. The sklearn Naive Bayes does not support predictions with missing values and can be tricky to implement to what you are thinking. Maybe writing your own piece of code would be better :)