We have generates a control server that has all databases of the Organisation as well as their RTO and SLA and run a backup based on how much data has changed as well as the servers last known IO data. 
What we do is we have scripted the restore scripts to a SQL Job on the given server and have the job pool the transaction log and measure changes. when performing a backup we generate a restore script (locally and remotely) and whenever we have a change in RTO/RPO/SLA we update the server. If wen need to recover (in-place or hot swap/recovery server) we execute the recovery script that get's the appropriate backups Full, differential and transactional for a point in time or simple recovery.  
The best strategy depends on your RPO-Recovery Point Objective, as well as the RTO - Recovery Time Objective aka how fast must you restore it as well as how much data are you allowed to loose. 
You might actually split a database in file groups that have different SLA's where 1 table get's backed up more frequently than others. Also when working with large databases (we have those that are several TB) you are not able to backup a whole database as the backup it self will take more time than that your infrastructure can cope with... a daily backup will fail if backing up takes more than a day… 
I have... a long time ago written a concept that addresses this, it also contains some scripts that will likely still work when changing paths, however it assumes password protecting backups, you might need to remove that as later versions of SQL server dropped that support. 
We have to many databases for any other approach, also we have to high a financial exposure if "manually" doing things. DBA 1 and DBA 2 might do things differently also how can you consistently implement something on "all" servers when you get a security update or worse.. new idea in managemnt…    