Something easy and simple is to get the absolute difference of each point from the two time-series and then use those values to decide.
Another way is to use a Granger Causality Test, which is used (among other things) to find if two time-series are statistically different.
For example, you can get the average, standard deviation, maximum and minimum of the absolute differences and see in what limits you can safely accept the deployment.
What I am looking for is to automate this decision making. Basically, compare the error data during the push with last weeks (or any other time frame) and decide if the two error trends are similar and to what degree they are similar.
When we deploy any new version of a web service, we keep monitoring it (while deploying to live) to make sure it is not introducing any new errors. For this, we just visually compare with last weeks trend of errors (within the same time frame) and if they look similar, we approve the new build, or if the number of errors seems to increase, we decide to rollback.