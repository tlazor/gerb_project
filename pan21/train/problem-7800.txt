You might take a look at (Hotelling, 1933) which defines PCA as a orthogonal projection into a lower dimensional subspace maximizing the variance of the projected data. (Bishop 2006) might be easier to come by and contains a derivation of PCA based on this definition in chapter 12. 
Information Geometry is a rigorous framework which combines statistical inference, information theory and differential geometry.
agreed this is not studied in general so much. part of the theoretical challenge/ difficulty is its highly dataset- and algorithm-specific (in the sense that different datasets & ML techniques are more or less sensitive to the "dimension-distortion tradeoff"). often dimensionality reduction is regarded as a tunable/ optimizable parameter along with all the other ML parameters and some quality metric intrinsic to the problem (also measuring learning success/ "generalization") is used to determine how much to apply. typically the performance of the ML algorithm declines if either too little or too much dimensionality reduction is applied.
however re "formal analysis/ tradeoffs of dimensionality reduction," newly emerging, try this large paper, which looks at how the Johnson-Lindenstrauss reduction affects datapoint proximity wrt a distance metric & lays out a more general theoretical framework. 60pp
From an algorithmic perspective, the tradeoff is clear: lower dimensionality yields faster runtimes and reduced storage space, but compromises precision. I call this the dimension-distortion tradeoff.
The problem of dimensionality reduction and PCA has been formulated information-geometricaly as follows (with emphasis on information):
From a statistical/information-theoretic perspective, the situation seems less clear. It is commonly believed that dimensionality reduction (PCA in particular) has a denoising effect and thus should actually improve the performance. On the other hand, dimensionality reduction does discard information, which might cause the performance to degrade. Thus, one must address the statistical question: is the information I'm discarding noise or potentially useful (and even if useful, might the benefits of lower dimension still outweigh the losses)?
This is less of a question and more of a "here's my take let me know if you agree" (so I guess it might turn into a big-list?).
Dimensionality reduction refers to a collection of techniques that input data and return a lower-dimensional version, with some distortion. PCA and Johnson-Lindenstrauss are the most common examples.
moreover, sometimes mostly PCA alone without further learning algorithms applied on top of this data was enough as an effective/ top performing prediction algorithm. in this case one could say that most of the statistical trends in the data were apparently "identified" by PCA and what was "left over" (the "residual") was either not substantial or noisy. this contest had a $1M prize awarded to collaborating teams and eventually Netflix decided to employ many of these techniques in their live production system.
Are there other formal analyses of the statistical benefits of dimensionality reduction? Perhaps other tradeoffs besides those mentioned above?
There appear to be very few formal analyses of the statistical benefits of dimensionality reduction. One that I'm aware of is in our 
as mentioned in the comments, one great applied case study in this type of big data problem & the relevance of PCA/ dimensionality reduction with major statistical/ scientific analysis is in the Netflix prize for movie ratings prediction. prior to the contest, PCA type approaches were somewhat rarely applied to human ratings predictions. however they were found to be extremely effective in this contest combined with careful tuning/ conditioning. the winning solutions used large "blends" of many different techniques but PCA related algorithms composed many of them. 
Specificaly the concept of dual connections and dualy-flat spaces. It has been shown that various statistical inference methods (e.g maximum likelihood, EM algorithm etc.) can be formulated in terms of dualy-flat spaces in a statistical manifold (reference "Methods of Information Geometry", Amari, Nagaoka). The new formulation allows a unified view of previously un-related methods and algorithms.