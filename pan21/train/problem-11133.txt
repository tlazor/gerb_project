When benchmarking it is typical to try to "blow out the cache" by using a data-set at least twice the size of the RAM+cache.  This can help get more worst-case performance numbers, but really help you with realistic numbers.
Sadly, for realistic information about performance, you really don't have much option but to create something that will simulate your specific use-case with your specific data-set.  Ideally you'd also want to age the file-system before running this benchmark, by loading it with data simulating normal use over time.  A freshly "mkfs"ed file-system may respond quite differently from one that has a bunch of other data and has files created and deleted in directories.
In other words, if this system is going to be a web server, load up your pages and data and applications and get a reasonably representative set of URLs to run through siege or ab.  If it's a database server, load up a production database and run your typical queries against it, etc...
However, as far as a quick but not very realistic comparison, tools like bonnie++ can provide good numbers.  I usually have problems with Bonnie++ giving me good numbers for the random I/O section, because it tends to run with way, way too small a data-set, so pay attention to the options for controlling that.