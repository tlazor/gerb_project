This becomes an issue in many domains where detection of the minority class is particularly important. For example, in medical diagnoses, we may be willing to sacrifice overall accuracy (because of more false positives) in order to have a better true positive rate.
Relative imbalance, on the other hand, does not go away with more data. You have a relative imbalance when the prior probability of certain classes is much larger than that of some other class or classes. For example, you will always have 10 times as many examples of class 1 than examples of class 2, because class 1 occurs 10 times as often.
Overall, you should also be careful to pick the right evaluation metric. Evaluating your model using accuracy may lead you to believe that your model is performing very well when in fact it is classifying everything into the majority class. There are many metrics you may use, each of which have their pros and cons. The area under the ROC curve (AUC) is a common metric which gives you a general idea of your model's performance averaged over different class misclassification costs. If you can plot the ROC curve for multiple models and notice that one curve dominates the others across the entire width of the plot, then that is the clearest sign that you have a winner. There's a whole chapter on the subject, if you're interested, in the book Imbalanced Learning: Foundations, Algorithms, and Applications.
Absolute imbalance/rarity occurs when, while you have plenty of data from some classes, you have only a few examples of some other classes (or subconcept of a class). In this case, the issue is that there may not be enough data for the learning algorithm to learn the minority class. In the example you give, with 100 examples of the minority class, depending on the nature of your data, you might have this problem. If you expect to have more data in the future, however, absolute rarity should eventually no longer be an issue.
In short, it depends on your domain. Are you OK with optimizing for overall accuracy, or is it more important to have comparable performances across the classes? If you choose the latter, then there are a few things you may try:
Most learning algorithms for classification optimize for accuracy, or something similar like RMSE. This means that, when solving the real classification problem is hard enough, and the data is strongly imbalanced towards one class, the model may resort to predicting the majority class whenever there's a doubt. Recall for the majority class may be great, but not so much for the minority class.
The problem you face is commonly called the class imbalance and has been the subject of quite a bit of research. Here's a literature review, if you're interested: He, H., & Garcia, E. A. (2008). Learning from imbalanced data