So, if you're halving your grid frequency on each axis at each quadtree level (1/4 as many triangles, 4x the area each), then you'll want to drop one level of detail each time you double your distance. Or conversely, anytime you halve your distance to a triangle, you should double the grid frequency. Something like...
Ideally you'd have to make your own experiments and find the right balance between quality, not noticing that the asset is changing shape and performance.
Now obviously since the depth falloff is continuous but your LOD transitions are discrete jumps constrained to chunk boundaries, you'll have some judgement calls to make about where to put your transition point within this range or how to round up or down to favour visuals, performance, consistency, etc. But thinking about it in terms of pixels per triangle should give you a solid metric to use in making and evaluating those decisions.
Let's take a look at the terrain you show at 1m away, which covers an area of about C on-screen pixels. As this triangle recedes to distance D away, it will cover an area of about \$\frac C {D^2}\$.
To avoid either extreme, we want to replace our terrain chunks with LOD versions with a triangle density appropriate to their distance, so that the rendered image has a roughly even density of pixels per triangle in both near and far terrain.
There is no "one-size-fits-all" formula for Level Of Detail calculations. A game where assets are frequently travelling from the far distance to directly in front of the camera will need a different formula than a 2D sidescroller game that just happens to use 3D assets.
One way to think about the problem is in terms of visible triangle density. How many pixels does a typical triangle span?
Also I'm not sure "splitting" an asset is the way to go, as usually games use separate assets for each Level Of Detail asset, and swap between them.
If your triangles are all the same size, then as they recede into the distance, they start taking up fewer and fewer pixels, until you end up with many distant triangles all drawing to the same pixel - lots of redundant calculations for no visible benefit. Or on the other side, as they get closer to the camera, the triangles get bigger and bigger until their pointy edges are obvious.
So when you get near to the chunk it splits, and then when getting near to the splited chunk it does the same again and over (quad tree). But what is the formula for calculating the distance when it should split depending on the distance? For smaller chunks, smaller distance?