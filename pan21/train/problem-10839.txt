When p > n, the LASSO model can only sustain up to n variables (this can be proven using linear algebra, the rank of the data matrix in particular), leaving at least p - n variables out (some that might be predictive, consider a model where you use LASSO and all your variables are predictive). This is not necessairly a bad thing though, depending on your use case. 
This is really strange since I cannot think of a good reason why Lasso should be a problem in high dimensional data. In fact since coefficients can be shrunken to zero under Lasso (which is not the case for Ridge or Elastic Net), Lasso is a good option for feature selection. Maybe the comment refers to exactly this point, that Lasso may „kick out“ variables?
Here is a related post which may be of interest: https://stats.stackexchange.com/questions/373275/for-high-dimensional-data-does-it-make-sense-to-do-feature-selection-before-run
In general, I find LASSO to choose different sets of variables that change quite a bit depending on the training set, especially for small datasets and even worse when p > n. Again, not necessarily a problem but you might look to not using explicit feature selection and instead opting for ridge regression. Alternatively, the elastic net is a balance between the two methods.
How and why exactly does Lasso behave "erratically" when no. of features > no. of training instances or when features are strongly correlated? Does it have to do with redundant features?
For the colinearity problem, if two variables are highly correlated (that is, they contain the same "information" in some sense) then LASSO will tend to pick one of the two randomly and drop the other one. Again, might not be an issue but can lead to "erratic" behavior in which the variables selected change quite a bit depending on the training set.