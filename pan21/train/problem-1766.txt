If they already have excesss CPU time to cover your needs then by all means take it, but most environments I work with there isn't 21+Ghz left over to spare (again presuming your application will fully utilize the processor you specified with a full 8 threads).  It's certainly a different story if this is a singlethreaded application that would only take up 2.6ghz at max.
It really depends on how much CPU time it's going to eat up.  The problem for a vmware solution is that if this eats up one XEON 5550 CPU (potentially 4 physical and 4 virtual  cores worth of CPU time) it's fairly expensive to dedicate 1 full CPU.  Saving from Vmware comes from sharing the load since it's assumed that the system that was virtualized is not going to be using the CPU 100% of the time.  If IT is willing to buy a better CPU than the 5500 to cover the vmware overhead then sure.  Otherwise I'd take the money they would spend on vmware licenses and you should easily be able to buy better/faster procs (take a look at the intel xeon 5492, it's older but has a faster clock speed -about .6 ghz faster base processor price goes up by a whopping 535 dollars)
However, for the IT department this would be a much better solution: no new server to maintain; no expense; no space used in their racks; significantly lower additional power consumption.
From your own point of view, VM will not produce any advantage. As long as your IT folks understand the purpose of that VM and allocate CPU and memory resources to that VM equal to what you'd have with a physical server, you will see pretty much the same result. 
Another factor to consider is vendor support.  If your software vendor will not support you in a virtual environment that should factor in to your decision.
Given that the machine is paid for by a grant, and that the time to process impacts your research, go with a dedicated machine.  You don't want web surfing to impact your results, or vice versa.  