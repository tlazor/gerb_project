Both disks have GRUB installed and the "rescue" system.  It should be possible, in an emergency, to boot some sort of Linux system from one of the two disks, in order to fix a problem and get the server going again.
The BIOS on my new server has a feature where you can hit F8 during booting, and then pick a boot device.  I have used this to test that I can boot with GRUB from either of my two hard disks.
A "rescue" system is installed in partition 5, also a plain ext3 partition.  This is a complete bootable Debian, and in fact was the first thing I installed; I installed the rest of the disk from that system.
4Gb is a good amount for the number of hosts you're talking about, but its so easy to setup additional hosts for misc uses that I bet you'll bump up against that limit before too long.
(I'm not actually sure that XFS is much better than ext3 for a many-small-files setup, but as I said, I did all this in a hurry after my old server died.)
We've had a positive experience with KVM managed by virt-manager, connecting with qemu over ssh.  It's been very easy to set up, configure, modify, destroy, and just all-around play with guest OSs.  This option also works with Xen, so you can use the same commands with either hypervisor.
I really recommend putting a tiny "rescue" system at the end of your hard disk.  Then, don't even mount that system in your main system, so that berserk processes (like rm -rf /) can't clobber it. There are many problems that can be easily solved by booting a working system, mounting the volume with the damaged system, and then fixing something.
I then turned to the older tools, the "xen-tools" stuff; in particular, "xen-create-image".  Because my users all have a Maildir setup (one file per each email) instead of an mbox setup (one file per email folder), I tried to use ReiserFS.  xen-create-image created the image just fine, but it wouldn't boot.  I decided to use XFS, and that worked.
The other comments about attackers reusing passwords to gain access to your other hosts is something to worry about.  My answer: use ssh keys only, and make sure you don't allow ssh key forwarding from your ssh client.  (this would allow a hostile root user on a host you log into to reuse your session keys to attach to another host as you)  Also, if you have different root passwords per box, you should think about setting your cli prompt or color scheme per host to remind you which host you are on so you don't accidentally type a different host's root password.
My old server didn't even have X11 installed.  I decided to install a GNOME desktop on the new server, hoping I could use cool GUI tools like virt-manager.  I discovered that 4 GB is not very big for a modern GNOME install; everything fits but there isn't much free space.  If I were starting over, I'd give 10 GB for the Dom0 OS on /dev/md0.  If I really get crunched for space, I can probably move /usr/bin into a new volume made under LVM.
The Dom0 is installed on a RAID volume, but not LVM.  I read some comments about some kernels having difficulty booting from LVM, so I just kept things simple.
In my opinion, virtualization is a cool technique which doesn't fit everywhere, and going virtual just because everyone does isn't a good idea (again, in my opinion). 
At first I tried to use the "libvirt" tools for Xen, such as "virt-manager".  Based on my experience, I must say that "libvirt" is half-baked in Debian 5.0 Lenny, and I do not recommend it.