There are lots of bad examples on the Internet.  Doing a git pull to distribute code is fabulous for development, but prone to all sorts of issues in practice.  It should not be used for production deploys without careful consideration of the negative consequences.
There are some source control systems that bill themselves as artifact management platforms as well (for example Perforce/Helix) and they try to support the mode of work. But git is especially bad at that. You would have to do a lot of work to make git support both development of code and management of artifacts efficiently. I'd suggest to not do that and use a different system for each. JFrog Artifactory for example is a great tool for that. Delivering code in containers and using docker hub for artifact store is another excellent way to do it. Even a bunch of upload/download scripts against S3 are better than storing your artifacts in git, as you might actively slow down the developer experience if you don't setup your solution exactly right.
My question: is method 1 really the best way to deploy a NodeJS application to a production server? If yes, what is the reason?
Surprisingly, most of the examples I found on the internet uses method 1. This method requires the production server to have public internet access which may not be possible for many cases.
We have an internal CI/CD server running private GitLab. It has the source code of our NodeJS project. We would like to run CI/CD and have it deploys to our on-premises production server. (Let's assume we are using the master branch).
I'm not sure what format npm uses for artifacts, but it is also probably a good idea to back them up off of the CI/CD server and use a dedicated set of machines for serving up the artifacts.  If you spin up 100 machines at once would your CI/CD server handle it or fall over?  Isolating the artifact serving will avoid your CI/CD server being swamped and useless until the deploy is over.