Damas-Milner type inference is proven complete for exponential time, and there are easily constructed cases with exponential blowup in the size of a result. Nonetheless, on most real-world inputs it behaves in an effectively linear fashion.
Pairing heaps, from [1] - they implement heaps, where insert and merge have O(log n) amortized complexity, but are conjectured to be O(1). In practice, they are extremely efficient, especially for users of merge.
A great and little-recognized example of this phenomenon is graph isomorphism.  The best known algorithm takes something like $O(2^{(\sqrt{(n log n)})})$ time, but graph isomorphism tends to be solvable quite quickly in practice.  
Groebner bases.  The worst-case running time is doubly-exponential (in the number of variables).  In practice however, especially for well-structured problems, the F4 and F5 algorithms are effective (i.e. terminate quite quickly).  It is still an active area of research to figure out what the proper conjecture even should be regarding the average or expected running time.  It is conjectured that it is related, somehow, to the volume of the Newton polytope of the underlying ideal.
I just discovered them just today while reading Sec. 5.5 of C. Okasaki's book "Purely functional data structures", so I thought I should share info about them.
I don't know if there's a formal result on average/smoothed complexity of the problem, but I remember reading that one existed - maybe someone else can chime in pointing out a formal result.  Certainly, there's a good deal of experimental evidence and a lot of fast solvers.  I'm also curious if this property extends to other members of the GI-complete family.
[1] Fredman, M. L., Sedgewick, R., Sleator, D. D., and Tarjan, R. E. 1986. The pairing heap: a new form of self-adjusting heap. Algorithmica 1, 1 (Jan. 1986), 111-129. DOI= http://dx.doi.org/10.1007/BF01840439 