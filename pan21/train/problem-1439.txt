That said (and the reason this is an answer rather than a comment), if you don't have control of the schema (normalization) or infrastructure (caching) to implement those changes... look at sys.dm_db_index_usage_stats.last_user_update.  This should be a relatively inexpensive query you can use to check the last insert/update/delete for the table.  Wrap your expensive query with the cheap one so that you only refresh from the table if the data has changed.  It's effectively a poor man's cache.
Note: there is an edge case that when the server is restarted it will not have a last_user_update until the first insert/update/delete occurs.
You should still optimize your query using the other answers provided, and perhaps including a where clause so that you only inspect new records added since the last refresh and then merge the results in your application code.  
This sounds like you have a table where each entry has a port, but only from a small pool of ports. In this case it is usually good practice to create a second table that contains every port once and link it via foreign key. Then you can query this much smaller table.
Besides, the question title says "Alternatives to running query for rarely changed data", so I assume that this large table doesn't change often anyway. I think, indexed view would be perfect here.
However, if you don't want to change up your database architecture, you could create a new table with just the name of the port (e.g.: CREATE TABLE portnames (name varchar(50));). Then you fill it with content from your first table (INSERT INTO portnames (SELECT DISTINCT PortName FROM Ports);). Now you can query this table instead! Remember, if you want to keep it updated you have to recreate (or truncate/insert) it everytime you add an entry to the first table .
Blasphemy to propose an application change in a database forum... but the ultimate database optimization is to not use the database at all.
In essence it is a cache, which is maintained by the engine automatically behind the scenes. Indexed view is stored on disk and updated automatically when the underlying table changes.
You mention running the query can take 10-15 seconds, but suggest "running some script at certain intervals... to offload querying".  This stands out as meeting two very different requirements:
This also makes it impossible to insert a row with a misspelled port, since it has to be linked to an existing row in the second table.
You need to determine the requirement, specifically your target latency for a new distinct PortName to appear in the UI dropdown.
So, updates, deletes and inserts into the main table would become somewhat slower, but querying the indexed view would be instant, because it will not scan 10M rows of the main table. In any case, the engine is smart enough not to scan the whole 10M row table when it is updated to adjust the values stored in the indexed view.
The other comments and answers by Flourid, RBarryYoung, and DForck42 have already highlighted that this looks like a normalization issue.  You should not be querying millions of rows to pull out just 80 unique records.  Normalize the data first so you can populate the drop downs from the 80 rows.  Then apply indexing as a performance enhancement if warranted.  Then apply caching so that you can populate the drop downs from the cache if warranted.
Somehow, nobody mentioned an indexed view. A very brief intro to the indexed views can be found at What You Can (and Canâ€™t) Do With Indexed Views.