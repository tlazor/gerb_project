The motivation for 2D anisotropic filtering is to prefilter the texture function over the screen area it falls under in a more accurate way than doing a box filter in texture space (usually, anisotropic filtering does several box filters in texture space).
New developments in volume rendering are happening all the time, and in any case there are so many variously suboptimal ways you could try to generalize 2D anisotropic filtering, the doubt expressed by the specification makes sense to me:
I am not aware of hardware support for 3D anisotropic filtering.  I could be wrong about its existence though.  I believe it has been tried.
Do you want absorption?  Emission?  (You need to do some kind of exponential to integrate.)  Do you want scattering?  (You need to do some kind of recursion or approximating blur to integrate).  And this doesn't even get into what kind of data the 3D texture stores.  Does this texture mean opacity, or does it mean density?  Is it diffuse or is it emission?  All of these need to be integrated differently.
Do modern GPUs support anisotropic filtering for 3D textures? If yes, how can one use it? The OpenGL spec doesn't seem to be very precise on this. From this link:
The best analogue I can think of would be doing some sort of 3D texture lookup anisotropically along the viewing ray.  This isn't quite a direct analogy.  Anyway, the problem with this is that when you're doing volume rendering, you actually care about how this integration is done a lot more than in the 2D case.