(I don't think you're actually computing accuracy in either case.)  It is extremely surprising to me that a LinearRegression model does so well; most of your variables seem categorical, even the dependent location_id.  Unless there's something predictive in the way the ids are actually assigned?  How many unique values does location_id have?
After fixing that, the feature ranking gives you some valuable insight to a problem, and depending on your needs you might drop low-performing variables, etc.
Is the location_id the location of the user, or the job (assuming I've gotten the context right)?  In either case, if you have many copies of the user/job and happen to split them across the training and test sets, then you may just be leaking information that way: the model learns the mapping user(/job)->location, and happens to be able to apply that to nearly every row in the test set.  (That still doesn't make much sense for LinearRegression, but could in the GBM.)  This is pretty similar to what @ShamitVerma has said, but doesn't rely on an interpretable mapping, just that the train/test split doesn't properly separate users/jobs.
 So of course that comes out as the "most important," and the other features' importance scores are probably mostly meaningless.