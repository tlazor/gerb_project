The code below is indeed longer than I would like to believe I'm allowed to post here, but it is also, hopefully, rather simple to read. Thank you for trying to help me.
First of all, this has been quite a journey to solve. I had to ask for a friend's help who, in turn, presented me to a friend of his. The solution is something that appears somewhat intuitive to me, but, nevertheless, was not at all on my radar.
Despite this little trick, it is still very hard for the NN to get a good fit because it's still one row, so, now, if we use ADAM (which dynamically reduces the learning rate), it might get stuck in local minima and never come out. The last tip then is to use incredibly high learning rates, in my case, between 0.2 and 0.3. The learning process will not be stable either and, thus, lastly, it's necessary to use an absurd amount of epochs (20,000) with callbacks to register the most optimal weights. This way, my friend's friend solution got us to 99.8% accuracy.
Despite the good performance, quite frankly, he and I are both in agreement that the most reliable and fast solution to this problem is actually to use a Decision Tree type of algorithm.
Basically, since we only have one row of data to look at, the fit is going to be too sharp for the NN, so it will most likely always plateau and stick to spitting out only zeros. To make things easier for the NN, the suggested solution is to apply some noise to our data, so that the system won't think that it needs to get a very sharp fit in order to do its job. 
This classification problem is apparently simple and I have no idea why it's not working, perhaps I'm doing a conceptual mistake. I'm trying to make a predictor which will classify minutes on a clock as 0 for no requests or 1 for a request in that time instant. So far, the model can't seem to learn past giving everything 0's or putting zeros before all the requests happen. The best case scenario I've got so far is:
I've been trying to implement all the things I've learned related to Neural Networks and my code works in the following manner:
Other techniques that have also been tried are Dropout, Gradient Clipping (for exploding gradients) and extremely small learning rates with ADAM (on the order of $10^{-5}$). Basically, I just can't go much further than an accuracy of around 73.33%.