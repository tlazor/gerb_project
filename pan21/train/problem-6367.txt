It is true, that over time, people get used to better quality. For example: the HiFi-norm from the 70's is certainly not up to current standards; we're used to better sound. So it may well be that in the future my remarks that most people don't see the difference  will be laughed at.
On one hand, there is the file-format. If you use JPG, it is 8 bit, don't bother. You may also want to make sure that your videocard is capable of 10 bits per color.Most videos are also 8 bit/color. So, unless you've got all the file formats (tiff16, CR2, etc.) and the software and the hardware in place, don't bother.
There are new 10 bit monitors coming into daylight these days. What is their advantage over 8 bit, if all (or almost all) sources are 24bit not 30 bit.
Secondly is the problem of your eye. Theoretically, your eyes see around 10 000 000 colors (sources differ a bit on this). The theoretical limit would be around  24 bits/color. But you need to have a well-trained eye for that. Many people don't notice if you zero-out the lowest bit in an image. Also, monitors do not display the full range of color values that the eye can see. Black is seldom black. That means that there is only a sub-set of the 10 million colors that you see that is displayed by the monitor.
Even Photoshop has 10bit support... but you still can describe one channel with 0-255 (8bit) not 0-1023 (10bit)