In your case, I'd be sure that the par files are stored separately from the dataset so that an event can't ruin both sets of data at once. Also, generating the archive files in the first place is a CPU intensive function... at 100% it took an otherwise idle 2GHz server 18.8 seconds to create par files for a single 3.7MB file
par2 seems the most commonly used for this, a lot of people use it when writing DVDs or CDs where the data will eventually degrade but it isn't likely that the entire disc would be rendered unusable all at once.  Leaving aside the mathematics behind it, it works by virtually splitting the files into "blocks", then creating par2 recovery files based on those blocks.  To recover corrupt data, the system needs to have as many unique blocks of recovery files as there are bad blocks of data in order to recover any of it (ie, if you have 9 blocks of par files and 10 blocks of bad data, nothing can be done at all).
For CDs and DVDs, people produce recovery sets with high redundancy and burn the set of blocks to multiple discs, expecting it to be unlikely that a given block would become corrupt on every single CD.  With 100% redundancy, the original file can be recreated from the par files alone but the par files will take twice the disk space as the original data (plus overhead).