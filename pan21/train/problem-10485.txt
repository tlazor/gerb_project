I'm not 100% sure I understand from the question, but it sounds like the Docker solution would be to go from having an (physical?) appliance with an OS and your app installed on it, to having an appliance with an OS and Docker on it, running a single container with your app in it. That doesn't obviate the need to update the OS in the host, and it adds a layer of complexity (and more updates to contend with, as you'll now have to keep Docker and the OS patched) with no readily-apparent benefit as far as the specific areas mentioned in the question are concerned.
The main downside is you need the host OS and some sort of orchestration.  There are lots of choices for that, but don't underestimate the amount of work it takes to evaluate one, put it in place, and maintain it.
First and foremost, you get repeatability.  You can create a Dockerfile, debug in a container on your developer machine, run tests on a continuous integration server, and then in your final product, and you know it will behave the same in all those environments.  No forgetting a dependency that a developer had installed on their machine.  Also, your developers don't have to use Ubuntu at their desk.  Important to keep us Arch Linux users happy :-)
Considerations you would need to deal with include data storage as containers don't retain changes on restart, so you would want a data volume. There are probably a lot more considerations you would have as well once you dig into it. The system that I am currently working with (all docker-based) has been in development for over a year now and we are still finding areas in which we need to make changes to the container, configurations, etc. 
It will be like ""only upgrade one package"", talking about only retrieve a new version of the container, much better that dealing with debian packages ;)
Second, for your upgrade scenario, you can have multiple versions pulled to a machine at once.  If you do a docker pull myapp:2.0 while 1.0 is running, you can swap to 2.0 extremely quickly.  Much faster than doing a full OS upgrade would normally take.  If you use an orchestrator with multiple instances of microservices, you can even do rolling upgrades that don't interrupt service at all.
Docker sounds reasonable to me, as you could make and test changes to the container in house, and then depending on your release process, restart the containers always pulling :latest or something similar which would provide a tested upgrade.
You could provide a URL with all tags/version of the container you have made and the clients will read that URL to see if there is a new version of the container.
Even you could give the users the possibility to choose which version from the available they want to use (if you want to give that possibility).
I've worked with Docker a long time.  The platform independence is nice, but it's not what I consider most useful about Docker.
If you use a microservices model, Docker also provides sandboxes that can limit the amount of damage attackers can do in the event of an exploit.  Instead of gaining control over an entire machine, they are only gaining control over one container.
However, if you're talking about going from a virtual appliance to a Docker container, that could potentially smooth things out for you, but it also adds Docker as a dependency for your product; you're shutting out anyone who isn't using Docker and doesn't want to add it to their stack just to use your product. You could continue to support those that don't/won't use Docker by continuing to ship the (now "legacy") virtual appliance as before, but now you've just doubled your workload because you have two distributions to support instead of one.
You can store personal files/settings on local and you will never lost that information in upgrades and you will ensure that what you have made and tested will work for everyone in the same way.