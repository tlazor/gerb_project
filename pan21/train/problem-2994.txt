Another information theory concept that might be useful is the entropy rate of a process.  ER is a quantification of the amount of information contained in a process.  Depending on your problem, you might be able to compute the ER for each process and then use those values as realizations of a feature that can be grouped with a clustering algorithm.
An example would be that I create sequence of numbers/bit/letters from a known PDF (X) and you receive a distorted version of those values for which you also know the PDF (Y).  The mutual information is the number of bits that Y communicates about X which can be thought of as a type of correlation.
In the case of time varying PDFs (ie stochastic processes), information theory would just treat the ensemble of rvs as one joint rv and then calculate the mutual information of the joint PDFs.  If the PDFs are iid, then significant simplifications are possible.  Joint Gaussian PDFs also make things easier.
This is similar to the fundamental information theory problem that Shannon explored.  In that domain, it is framed this way: given two rvs, X and Y, what information does X convey about Y?