Trigger a thread dump or two on each of the servers.  You will likely find one of the servers has a thread running which is not running on the other servers.   Also check memory utilization via the console.  I have seen WebLogic get into a garbage collection loop when there isn't enough memory.
I would also turn on garbage collection logging and see if GC times correlate with perceived lag times.
I'm not an expert of cluster or Bea, but in performance analisis problems there isn't only CPU. What are the data about memory, disk and network? The tools to get data about are top (cpu and memory, with many details and also per process), vmstat (memory, cpu, disk), sar (sysstat package on linux, with all possible data and historic recordings).
Is the load balancing completely round robin or is it doing stickiness based on IP or cookie?  You could have some kind of user traffic that sticks to one server and moves upon restart - especially if another one of your servers is calling an app on the cluster.  So cross check it against actual hits to the server.
You may also have a race condition in the app that certain operations get it in a loop.  For that you could take a thread dump (kill -3 pid) and pull it out of your stdout log and run something like Samurai on it to see what up.