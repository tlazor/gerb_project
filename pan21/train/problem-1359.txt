Bit of an odd question maybe, but I am considering setting up a server at home and I want to host a couple of websites, a nextcloud, and a plex server. My websites are pretty simple and this way I'll save quite a bit of money on hosting and dropbox fees etc. I bought an old Dell R710 and looking at the power it uses it's around 350 Watts. This adds up to around 3000 kwh on a yearly basis assuming it's up 24/7. 
Anyone knows how much energy these things consume? In an era where we should be less wastefull with our resources I feel this actually should be mentioned with online products. We know bitcoin uses the same amount of energy as some small countries, and Google explains on their website how much energy a search costs but that's about it for me.
The ability to size and resize a cloud server appropriately is a key benefit and driver of efficiency - you only pay for what you use (and therefore someone else can use the rest). That goal of paying only for what you use can be achieved to some extent locally with a screwdriver :-). Removing the second CPU would still leave you with probably 6 cores but save 50-100watts from the idle power consumption. Both CPUs are rated upto 120 watts but the amount they draw depends on how hard they are working. Another big power consumer can be the storage. Your server supports up to 8 disks - how many do you have/need? Reducing the number of disks in the server, or switching out spindle disks for SSD's would reduce power consumption. Obviously theres a trade off here between additional costs, running cost and available system resources but its worth considering for a greener option.
Given your server is upto 10 years old, and potentially contains 2x 6 core CPUs @2.4-3.3Ghz, upto 288GB of RAM and upto 8 spindle disks, the question you need to ask yourself is does that seem like overkill? If the answer is yes, its not going to be efficient. Having a big beefy server doing little or nothing will be eating power, heating the room and not a lot else. In the cloud the underlying host would still be doing other useful work as well as eating power and heating up a data center somewhere - theres no contest. 
Cloud generally means virtualised, which means multiple virtual servers inside a hypervisor on a single physical server somewhere. By virtualising many severs onto a single physical host cloud providers get much better utilisation of their hardware ("cutting up" a larger server into many smaller virtual servers) and require fewer total servers therefore less hardware and less power to do it. This means its very difficult to compare a physical server to a cloud server and probably why you cant find any useful comparison. Cloud hosts could calculate power usage, or at least give an upper and lower value based on usage but it would probably expose information they'd rather keep private.
In terms of establishing a running cost of your physical server, your best bet is to just measure it ideally when under a realistic load. You can buy a cheap "energy usage monitor" that you plugin inbetween the server and the wall socket and it will tell you how much power is being used. The 350 watt on the PSU is the maximum draw, the actual usage will be lower and depends on installed hardware / operating system / power management and of course the server config and utilisation.
Hosters won't make any statements because someone hosting a popular site is going to have a very different usage profile than someone just hosting a bunch of family pictures. With hosting at home don't forget that you probably need to look into some kind of dynamic DNS solution.
If your concerned with just energy efficiency/usage the cloud will always win for light workloads due to the economy of scales involved and how densely packed cloud servers are on the physical hardware. By contrast your hardware is relatively old and inefficient and will probably not be running fully utilised?
During the winter times, if your home has some modern heating systems (with thermostats), then during the cold months, your server will almost certainly be greener than the cloud solution, because whatever your server would consume will be offset by the saved energy by not using the heater. This also needs to be taken into account. So, probably the greenest way is to use the cloud during the summer and home server during the winter.
You simply won't know unless you know how the hoster does it's just. If you are concerned a simple vhost or just a website host might be what you're looking for. Your calculation is also wrong. It's unlikely that there will be a draw of 350 watts at all times.
Aside from the fact that this is pretty expensive (I don't mind this actually since  the fees for websites and dropbox would be around the same), I feel this is quite wastefull in terms of energy usage. Therefore I'm wondering what the energy consumption of hosting a website or a cloud is. I can't find anything on it online. My most important argument for not hosting everything at home would be if it was much more efficient to host everything somewhere else.