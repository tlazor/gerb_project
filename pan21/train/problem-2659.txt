I would like to train a neural network for named entity recognition to tag an unlabeled dataset of texts. The generated labels will then be checked via a crowdsourcing platform. The goal is to annotate the dataset. Therefore, the neural net should find all possible entities in the text, i.e. have high recall rather than precision.
What would be the best way to train a neural network for high recall, i.e. assigning lower cost to false positives than to false negatives? Could the loss function be changed from negative log likelihood to something else to encourage high recall?
This way you would get more samples predicted to class $1$, which would increase the class' recall (at the cost of its precision though). 
A network's output is the probability it gives that a sample belongs to a class, let's say class $0$ or class $1$. The output of the net would look like this $(0.53, 0.47)$. This means that the net gives a $53\%$ probability of the sample being class $0$ and a $47\%$ probability of it being class $1$. Normally we check which class has the highest probability and consider that to be the network's prediction. However, you could set a threshold over which the sample would be considered to be class $1$.