If I understand you correctly, your question is how to get your data into a model? Here is a brief example using R. The figure shows how the data is formated when I read it into R. It is one column, containing the label (this will be your y) and one column containing the event (this will be the text from which you make a bag of words). Make sure text is lower case and contains no special characters. Maybe remove stopwords, do stemming or prune your vocabulary. My text here is well formated for my task.
In R you can declare the column type to a factor and plug the factor in a regression model. Alternatively, you can simply "recode" the type to numbers, say 0=accident and 1=crash (etc). Any model should be able to digest these numbers which indicate your "classes" to be predicted. Don't forget to split your data into a train and test set.
Here is a good guide to glmnet and some documentation for R. You can do the same thing in Python by the way. 
The next step is to generate a bag of words or n-grams from event (I think this should be doable for you based on online examples).   
Once you have your labels (y) and your bag of words (x) you can start with some model(s). In another answer, a Keras model was proposed. I think this is an option, but probably a somewhat over-engineered solution. An alternative would be to use "normal" Logit with regularization (lasso or ridge). The reason for lasso/ridge is that features (aka columns in you bag of words) are "shrunken" automatically if they do not contribute much to a good prediction. This usually improves fit. 
Well, this is just a bonkers example of mine (not fine tuned). However, if you check different alpha values [0,1] you may get decent results for your task.