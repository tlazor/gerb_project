If you do see performance issues, I'd remove 30% of the VMs from LUN 1 and move them to LUN 2. Then start filling up LUN 2 in the same manner. Go to LUN 3 if necessary... does that make sense? The idea is to achieve maximum VM density on a given LUN along with a roughly 30% overhead for fudge room.
For example, if you had only one LUN, you'd only be using one active controller at a time. The other would sit idle as it would have nothing to do.
One final note. If you have any kind of servers in a failover pair/cluster or load balanced, you're going to want them in different datastores. You don't want the datastore to fail, and then be left with no servers. Admittedly, if you lose the array, you've lost everything, but this is why we backup, right?
If you had two LUNs, but one was much busier than the other, you'd be using both controllers but not equally. As you add more LUNs the controllers share the workload more evenly.
I would also make a pair of "high performance" LUNs for any heavy hitting VMs. Again, one per controller to share the workload.
Each volume (LUN) has its own queue depth, so to avoid IO contention, a lot of implementations use more smaller LUNs. That said, you can make a datastore span LUNs quite easily. The disadvantage of larger (and fewer) VMWare datastores is, as far as I know, that you can run into some limits on the number of VMs that could be on simultaneously. 
Based on your explanations above, you should be fine with 1 datastore. 60 vm's over 3 hosts isn't that bad (20:1). I would, however, recommend you upgrade the HBA's to 8Gb if financially possible on at least one host provided your fiber switch is an 8Gb switch at a minimum.
You can do 1 datastore/server, and there are some instances I do that myself, but I only do that with SAN replication on special servers. Managing 87 different datastores over 9 servers for vMotion gets confusing when you set it up! The majority of my vm's are in shared datastores with 5-10 servers on them depending on how much space they need.
Another consideration is controller performance. I don't know your SAN specifically, but most if not all SANs required that a LUN is owned by a single controller at a time. You want to have enough LUNs on the system so that you can balance out your workload between controllers.
That being said, I'd make at least 2 if not 3 datastores on the array. 1 datastore per host, with all servers accessing the others for vMotion. I don't know much about the IBM array; with EMC I would create a single RAID10 group with 3 LUNs for each datastore. With this setup, the host with the 8Gb HBAs would be great for your higher I/O systems.
Your VMs are all relatively the same in terms of performance requirements. I'd create two LUNs, one per controller. Start putting VMs on the first LUN, and measure your I/O latency and queue depth over time as the VMs settle in. Don't use the second LUN yet. Continue filling up LUN 1. You will either reach a point where you start to see performance indicators that the LUN is full, or you will have migrated half of your VMs to that LUN, and still have maintained performance.
You didn't specify how many VMs you have or what they're going to be doing. Even without that information, I'd avoid one making one big LUN for blocksize/performance, contention and flexibility reasons. 