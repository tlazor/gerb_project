A comment:  I recently attended a talk by Bruce Reed with the title Catching a Drunk Miscreant, which was joint work with Natasha Komorov and Peter Winkler.  If you can get a hold of the results from this work, maybe that might help you in some direction.
In general, they prove an upper bound on the number of steps a cop need in a general graph to be able to catch a robber, when we know the robber moves at random along the edges.
it does not relate it to random graph theory although you sketch out such a connection in your comments. (it seems like there stack distance could be related to markov chain mixing?)
it appears that you are interested in modelling cache performance or optimization algorithms by considering cache requests as nodes of a graph and the edges as transitions between adjacent requests. have not seen papers that study the structure of this graph. it does appear to provably not to be a purely random graph in real applications due to the success of caches in practice and what is referred to as spatial and temporal locality in the above slides. ie some kind of "clustering" as joe sketches out in his answer. 
from Q&A with you in the comments you seem to be interested in studying something defined as the stack distance in this set of slides, On the mathematical modelling of caches
it has some empirical analysis via benchmarks. it says in general there is "no known measurement of locality" of cache requests and then proposes stack distance as such a measure.