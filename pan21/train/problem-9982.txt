At a guess, you're thrashing the filesystem metadata cache memory in Linux, so that blocks for one page of the directory file are being expunged while you're still using another part, only to hit that page of the cache again when the next file is deleted.  Linux performance tuning isn't my area, but /proc/sys/{vm,fs}/ probably contain something relevant.
Would it be possible to backup all of the other files from this file system to a temporary storage location, reformat the partition, and then restore the files?
Alright this has been covered in various ways in the rest of the thread but I thought I would throw in my two cents. The performance culprit in your case is probably readdir. You are getting back a list of files that are not necessarily in any way sequential on disk which is causing disk access all over the place when you unlink. The files are small enough that the unlink operation probably doesn't jump around too much zeroing out the space. If you readdir and then sort by ascending inode you would probably get better performance. So readdir into ram (sort by inode) -> unlink -> profit.
If you can afford downtime, you might consider turning on the dir_index feature.  It switches the directory index from linear to something far more optimal for deletion in large directories (hashed b-trees).  tune2fs -O dir_index ... followed by e2fsck -D would work.  However, while I'm confident this would help before there are problems, I don't know how the conversion (e2fsck with the -D) performs when dealing with an existing v.large directory.  Backups + suck-it-and-see.
Inode is a rough approximation here I think .. but basing on your use case it might be fairly accurate...
Created 100,000 512-byte files in a directory (dd and /dev/urandom in a loop); forgot to time it, but it took roughly 15 minutes to create those files.
My preferred option is the newfs approach, already suggested.  The basic problem is, again as already noted, the linear scan to handle deletion is problematic.
rm -rf should be near optimal for a local filesystem (NFS would be different).  But at millions of files, 36 bytes per filename and 4 per inode (a guess, not checking value for ext3), that's 40 * millions, to be kept in RAM just for the directory.