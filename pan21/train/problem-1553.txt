As ewwhite already correctly stated, most non-streaming applications primarily perform non-sequential disk operations, which is why IOPS matter in addition to theoretical peak throughput.
When a coworker and I first installed SSDs in our development systems to replace the HDDs we'd previously been using, we ran some performance measurements on them that really highlighted why this matters:
Our compile times improved much more dramatically than a simple comparison of the maximum throughputs would have suggested. Builds that previously took over 30 minutes now finished in about a minute, since the disk I/O during a large build consists of reading and writing lots of separate source files which aren't individually very large and may be scattered physically all over the disk.
As you can clearly see from the example, just listing a max throughput for each device would give an extremely inaccurate picture of how they compare. The SSD is only about 6-7x as fast as the HDD when reading large files sequentially, but it's over 100x as fast when reading small chunks of data from different parts of the disk. Of course, with HDDs, this limitation is largely due to the fact that HDDs must physically move the r/w head to the desired track and then wait for the desired data to spin under the head, while SSDs have no physical parts to move.
While ewwhite's answer is completely correct, I wanted to provide some more concrete numbers just to help put why the difference matters in perspective.
By providing both throughput and IOPS numbers, you can get a much better idea of how a given workload will perform on a given storage device. If you're just streaming large amounts of data that isn't fragmented, you'll get pretty close to the maximum throughput. However, if you're doing a lot of small reads and/or writes that are not stored sequentially on the disk, you'll be limited by IOPS.