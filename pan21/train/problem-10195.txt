In NLU, you mainly deal with semantics, meanings and relation between words. For example imagine a Google search query. When I search "who was the president of US when Italy won the world cup last time?". If Google wants to rank pages for me based on NLP then many things might come up including these words and their close words (which of course helps me find the answer in related webpages). But if google needs to deliver the precise answer (which is still not easy!), it needs to understand what I say as you see an example here. For this, they use a Knowledge Base (for Google, it's called Knowledge Graph) which is the answer I have to your question.
You have many options depending on the level of complexity and creativity you may have. Among all, I would suggest going through "Natural Language Understanding" techniques.
Not so precise but practically so to speak, NLP is about processing text and taking each token/word/phrase as an object and learning different models based on appearance of these objects and answering different questions. Here you do not really work with what they mean. Even when you do sentiment analysis (which seems to be a semantic concept) what you really do is counting tokens and seeing their labels in training data and predict the sentiment of the new one. 
You can make a knowledge base for your corpus if there is enough text. For this, you can extract patterns of desired phrases manually and through this patterns search all occurrences of your desired sentences (e.g. "|MEDICINE| is used for |DISEASE|") and all occurrences of your desired objects (e.g. "Tumor"). Hearst Patterns can be used for such purpose as you see in this paper. Then connecting things though a graph gives you an option for understanding that the similar pattern is happening. For synonyms and related words you can easily use existing Knowledge Bases which are a lot.