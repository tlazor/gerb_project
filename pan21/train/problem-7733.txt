You'll need to be careful when trying to store large files, and not use perlbal as your mogstored, but it'll work. You could also split up the large files into several big chunks like mogilefs likes to do, but just keeping large files around is easier on your client code.
Google doesn't just store a lot of data on a lot of servers, they have written software that distributes the load and storage automatically, while recovering from hardware faults transparently (like BigTable and Map/Reduce).
If you don't need to manage this yourself then there are lots of content distribution networks (akamai etc.) who will hold and resend your files for you, they're usually geographically disperse and have excellent resilience. Your site would then simply need to point at these files to allow your users to access them, your job would then essentially be done.
Content Distribution Networks (large clusters of servers at geographically distinct locations) are the key for any data and/or traffic intensive web-scale app.
If you have a need to store and distribute these files yourselves then at the very least you need a big block of fast and resilient storage (such as a large SAN/NAS by EMC/HP/NetApp etc.), some web servers tuned to deliver video (my own personal speciality :) ) and some big caching load-balancers such as Zeus's ZXTM. Of course at this point you have a single point of failure (the data centre) so if up-time is important you should consider one or more additional data centres and distribute your data and load around.
I would probably store them using lots of servers with something like mogileFS. This way each file is stored on more than one server, so a server failing will not lose your data.
Facebook recently switched from a combination of A and B to C for their photo storage as it grew to unmanageable proportions.