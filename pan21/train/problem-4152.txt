Also If you have any word in the development of the software you run on the server you can add an extra parameter for every table, which keeps tracks of the changes. So based on that you instruct your backup script to dump only the tables which were changed since the last backup.
As you guessed s3cmd is much more reliable as it was few years ago and a lot of people are using it, including me without any problem. Also amazon S3 doesn't charge for uploading data in, so money factor is not a problem, but definitively you want to avoid unnecessary transfer which most of the time occur with database backups.
I had the same problem with MySQL, because unfortunately doesn't support incremental backup. This is why I wrote an bash script which for every database dump a table in a different file. After that I compress it and zdiff with the previous copy, ignoring the last 2-3 lines (where mysqldump writes the current date). If there is no difference between the files I don't sync the content in the cloud. The downside of this approach is the complexity of the solution, which add extra steps when restoring the data. 