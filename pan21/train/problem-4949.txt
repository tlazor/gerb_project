If that is true, then it would be easy to distinguish between a truly bad drive, and a good one that has simply had the misfortune of an incorrect checksum. Unfortunately I'm old enough to have stopped believing in fairy tales.
However I wonder how reliable this is. Perhaps a bit is stuck on 0, and appears as "good" when everything is zeroed out, but in real life use it has 50/50 chance of working correctly. Or perhaps a bit always gets set together with its neighbor, etc.
What made SpinRite so good was its ability to reverse engineer the error-checking in its early days.
However no one can really know what is going on without disassembling and examining the hard drive firmware.
I've got an idea about an utility similar to memtest that would do the same to a HDD (writing different patterns, possibly several times, and then reading them back to check), but I also wonder if there is a point. What if the guys on the other forum are right after all?
Most physical data mediums have a significant error rate that is hidden using things like forward error correction codes, etc.  You might have thousands of errors on your hard drive now, but since the drive writes redundant data for each sector, it never reports an error and you never know about it.  
I use spinrite to check hard drive for defect. So far I'm happy with it. As it disable any error correcting in hdd so that the real bad block will appear. 
What could be happening is that the drive firmware is "remapping" the bad sector behind the scenes.  Modern hard drives have spare sectors they keep around for this purpose.  There is a limit of them and SMART data can tell you how close you are to running out on some drives.
What SpinRite does is basically ask the HDD if its able to read the data multiple times.  If the hdd is able to read it, SpinRite moves the data to a different sector, this allows the hdd to mark the previous sector as bad eventually and this is how its able to recover data for you.
Basically your typicall hdd is able to determine if its able to read the data by doing error checking.  If it encounters an error its able to correct the errors to a certain number of bits.
I don't know much about the physics behind bad blocks on a HDD, but I've often seen that a "bad" block turns to good when you overwrite it with something (anything, zeroes will do). I've even written my own tool that does exactly that. Windows command-line format utility with the /P:1 switch will do the same. I figure that in this case it was just a CRC error (the know that the drive does store checksums with each of its sectors)
Besides the fact this program already exists ( SpinRite ) it sounds like you don't know enough about the inner workings of a HDD to even write this utility.
On another internet forum the prevailing opinion seems to be that there are two types of bad blocks - "soft" bad blocks (which are indeed checksum errors, and can be safely corrected with no impact on further drive longevity) and "hard" bad blocks that are micro-fissures in the drive plates, and will make the sector always unreadable, and no amount of overwriting will help that. Also, the drive can be expected to rapidly deteriorate in quality.
When a typical mechanical hdd encounters what it believes to be a bad sector it does one of several different things.  The author behind SpinRite often talks about some of the features within a program he wrote called SpinRite.
So either physical unrecoverable defects of the platters themselfs or known recoverable defects or better described the fact a platter can and will be 99.999% perfect.