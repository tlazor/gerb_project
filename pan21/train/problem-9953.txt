Data-fusion is the most probable way to "fight" with all of the data sources. However, additionally, I would strongly suggest imposing an ontology and some set of rules. 
Also, before data-fusion, the Accelerometer from wrist data readings should be carefully pre-processed. I would strongly suggest finding a nice way how to remove the bias(left handed/right-handed people) and how to remove the DC component(that is the overall gravity of the Earth gravitational field, remove this bias as well)
-proximity beacons (to understand which room the user is at any moment) based on this sensor data you can introduce localization, decision trees may the most suitable way to go here while imposing some clear manually added rules(if the person is in this room, then goes into this room, etc). Basically, before doing the overall data fusion the localization may be really helpful in increasing the accuracy because you can introduce the ontology with specific activities that can be done in some of the rooms. (The person is not expected to be running in the kitchen or the bathroom for example).