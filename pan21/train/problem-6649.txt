the initial rsync suggestion is great for server to server copy and could be used for local copying too, the thing that stood out - is your script adding dates to files which means you wish to be able to look back on same file X amount days ago,  doing rsync version control visit http://www.howtoforge.com/backing-up-with-rsync-and-managing-previous-versions-history
The 2nd method will continue on creating /tmp/backup-date folders no easy way of managing this unless you wrote another script to monitor them that did something like
• Your script runs four new processes while setting basename, dir, name, and ext. To avoid all those separate processes, use various shell parameter expansions as below.  (Eg, when f=/home/tx.7/xyz.axi.pan, these produce xyz.axi.pan, /home/tx.7, xyz.axi, and pan, respectively.  Note, these expansions should work for all the files in your directories as listed by find, but if used in other scripts will stumble when given names like . or / or some other edge cases.)
So, your question is good until "I intend to run this as a cron job, every 30 minutes or so." :-) I am not that bash expert myself, to others might point out the potential weaknesses of your approach.
where filescript represents a separate script that does the stuff found inside your current for loop.  Of course you can also add -newer Mark2 to the find command:
Is this robust? It seems to "lock up" without error messages sometimes, and I can't explain or understand why. What am I doing wrong?
Implementing solutions to problems like this is good for exercise purposes. It is good for learning.
• The find $2 lists all the files and directories in $2 and below, but presumably the bulk of those files will have been treated already in previous runs.  For example, to avoid processing files older than second-previous run, in each run write a time-marker file and use -newer:
I would expect using -exec to be faster than using the shell for loop, but it might be worthwhile to run a timing test.
The alternative if files are being backed up for version control purposes would be to use something like svn(subversion) or git to check in/out files this way there is an external process managing changes.
using top method produced 1 file for all content and then you could use logrotate to rotate tar files after 10 backups or something. so you don't have backup's filling up disk
But it is almost never good to re-invent the wheel in production systems. Please have a look at well-established data synchronization software such as rsync and perform some research around it and other data synchronization/backup/snapshot techniques.
It should basically mirror a directory structure somewhere, and then copy files over there when they've been altered. The ghost files should then be named filename.YYYYMMDD-HHMMSS.ext (the included date/time being the time of last modification).
• The for f in $(find $2) structure is likely to produce a large list of file names and then process it.  That list need not be stored if you instead use (eg)
This is what I came up with - it seems to work already, sort of. but since I've never written shell scripts before, I suspect I might be doing some things fundamentally wrong and inefficiënt.
• For clarity, I prefer at the start of a script like this to copy all of the $1, $2, $3 parameters to named variables, as you did for the first of them.
The previous answers addressed some alternative ways of accomplishing the  backup and versioning goals; in this answer I'll comment on three or four possible improvements to your script.