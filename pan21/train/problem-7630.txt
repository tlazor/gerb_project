First of all, let us note that you use MMAPv1 as your storage engine, simply because it is the only available option in MongoDB 2.6
But let us look a bit further. So, the first file to be allocated has 64MB, and this is where the local database is saved. As per the rule that there is always a pristine datafile, a new one gets allocated with 128MB (the size of the new file always gets doubled until 2GB are reached).
When a lot of deletions happen after this, you can have several GB of allocated datafiles which are unused.
To remedy the problem of datafile fragmentation, you can use the compact command, which more or less exactly does what you'd expect from it. Make sure you read and understand its documentation before using it.
So we now have 192MB allocated, albeit it may well be that we just have a few kB saved so far. Now, let us assume that you store 4 documents with a max size of 16MB. The first three ones would fit into the 64MB data file, bit for the last one, there would be a few KB too less space.
Now, you want to add a file with the max size of 16MB. In a worst case scenario, albeit there is plenty of space available, mongod can not find a place in the existing datafiles where it can accommodate said document, writing it to the pristine datafile and allocates a new one. So this behavior alone can leave you with almost 4GB of datafiles(almost 2GB of the old preallocated plus 2GB of the newly allocated)  with only a single document. In your case, that would be 40% of the total size of the datafiles unused.
It has to be said that while the quota of unused space is quite big in your case, this levels out for bigger data. Let me put it that way: you are on the lower, less space efficient end of the scale.
Some people suggest to use the repair command to reclaim disk space. While this is possible, personally I strongly advice against that. It comes with some serious drawbacks:
Now, let us assume you have a running application, you create a lot of documents, you delete a lot of documents, all with an average size of lets say 10MB.
Imho, leave it as it is. While the quota of unused space is horrible at the moment, it will get better over time. And the speed of the increased disk usage in relation to the data size will slow down drastically. However, as always: Keep your eyes on it, since your use cases may be different from the (more or less) standard behavior described here.
Another option to reduce datafile fragmentation is to shut down a secondary, delete the contents of its dbpath and start it again. It will then perform a resync of the data from the primary, writing the data as a contiguous stream into the data files. After the sync is finished, repeat the process for the second secondary (if existing). If that is done, have the primary step down and repeat the process on the machine of the former primary. However, this reduces or (in case of a 3 member replica set with an arbiter) eliminates the redundancy.
The reason behind that is that under certain conditions, the datafile needs to be filled with zeroes to allocate the space â€“ which takes quite some time. So, for performance, you want this work to be finished when you need the datafile.
To be honest with you: not too much when it comes to preallocation. With 4GB of data, you will (almost) always have a 2GB datafile preallocated, resulting in roughly 30% of unused, but preallocated and necessary space (mongod will cause problems if it can not preallocate a datafile).
Now, our former pristine datafile holds data, a new datafile of 256MB gets allocated. The total size of our data files now is 256+128+64 = 448MB, albeit only 64MB plus a few KB are saved (setting aside the oplog at the moment).
Let us start small. Let us assume we just started our MongoDb replica set. File allocation has a rule:
This is mainly due to two reasons: The fact that datafiles get preallocated (which is a requirement for the inner workings of MMAPv1) and the fact that the datafiles are fragmented (though documents are never!). Let us have a look at the details of those two reasons.