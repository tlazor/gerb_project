I had a similar experience with a newly registered domain name -- you can safely ignore this behavior. It won't have any sustainable impact on your rankings.
It's not possible to tell from a single URL whether this is practical or not but the first thing I'd be looking at is adding some part of the URL to the robots.txt file.
Google's claim to fame is to crawl the Internet and discern relevant content that provides value to searchers. In doing so, Google relies heavily on inbound links from other websites as a sort of "vote of confidence" about your site. Provided there are links on other website  floating about the Net, Google will follow the links in search of content to index.
In a perfect world, Google would remember receiving the 404 error and never crawl those link again. Unfortunately, GoogleBot is complex and ever-changing so it's hard to guess what might happen. 
If you don't want that Google accesses your site then you can block the IP range. In this case no page will be indexed.
I suspect that the previous owner of your domain name (prior to your registration) has inbound links elsewhere to content authored some time go. Now that you've taken custody of the domain name and the content no longer exists, Google gets a 404 error.
GoogleBot is known to try URL's that existed some time in the past.  For example, I recently did a complete overhaul of my website.  Old URL's that were indexed in the past are still getting hit (404) by Googlebot months later.  I know for a fact my website does not internally use those URL's in any way.  Some are linked by external sites; some are not even linked externally.
You may want to use Google Webmaster Tools, if you are not already.  You can use the tools, to see what was indexed and what gave a 404.  You can also see what pages are linked to from what external locations.