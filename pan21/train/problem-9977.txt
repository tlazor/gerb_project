I would get a Wattmeter and measure the actual power usage under max load. Modern wattmeters come with an array of functions, providing peak amp, average amp, total energy usage, etc.
Suppose a power outlet has a rating of 20 Amps.  That means it can supply at most 20 A.  A server has two power supplies (i.e., one is redundant) that has 10 A written on each.  My understanding from other posts is that this still means that the server draws at most 10 A.  The second power supply is really just for backup.  This is despite the fact that the maximum current is written on each power supply.
Assuming what I've said above (cobbled together from others' questions) is correct, is going over the outlet's power rating the general practice?  If so, then the million dollar question is how much over is "safe"?  Or there is no way to know other than to measure the power consumption (i.e., # of Watts) during normal use and see how many Amps are being drawn by the users?
If I plug a power bar into the aforementioned power outlet, does that mean that I can support at most two such servers?
I've looked through questions from others here and things have cleared up a lot.  But I still have a couple of lingering questions.
E.g. My machine has a 800W power supply, but it doesn't draw that much. I bought a meter when I decided to get a UPS. It turns out, powering my machine at idle load + one monitor is only 96 watts. Under load, it has never exceeded 250.
Other questions here seem to imply "No".  That's because I should really look at the actual power consumption (i.e., Watts = Voltage * Amperes).  I can believe that, but isn't that unsafe and a bit subjective?  The load can very and if you go over and plug 3, 4, ... 10 Amp servers, you're really betting that the load won't be high for the connected servers -- high enough to draw over 10 Amps.
Computers draw very different amounts of power depending on whether they are idle or are under load. You could estimate the maximum power used by each of the components (CPU, motherboard, hard drives, graphics card, etc), but it's probably best just to measure it yourself.
Of course, assuming that your fuse will melt at 10A, you don't want your electric loads to be anywhere close to that. I'd use 60% as a reasonable safety margin.
I think why I'm puzzled is if this was a toaster, a heater, a hair dryer, etc., it's a lot easier to predict.  Since there is the problem of server load, it seems I'm "forced" to either go over the rating on the power outlet or have an electrician install more power outlets.
I am looking after a few servers -- a small number and not worth calling a "data center".  As the servers aren't located in a "proper" computer room, I am wondering whether or not the power outlets can support what we have.