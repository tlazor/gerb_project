Interestingly enough, conventional neural networks were inspired by our own, actually cat's, biology. Hubel and Wiesel conducted experiments on the visual cortex of cats, and they realized that light was perceived by stacks of optic fields. This is what inspired convolutional layer and a deeper architecture.
Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it reduces the number of parameters. The reduction is possible because it takes advantage of feature locality, exactly what @ncasas writes.
ConvNets work because they exploit feature locality. They do it at different granularities, therefore being able to model hierarchically higher level features. They are translation invariant thanks to pooling units. They are not rotation-invariant per se, but they usually converge to filters that are rotated versions of the same filters, hence supporting rotated inputs.
Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm.
This is pure mathematics. A neural network, at the end of the day, is a big mathematical function. And the deeper the network, the bigger the function it represents. And by bigger, I obviously mean high-dimensional. The features learned are more sophisticated because they are the results of more sophisticated functions.
One should never forget the other components in a typical ConvNet. The convolution filters pick out the spatial invariant features, like edges and circles. These features are quantified in a pooling layer which follows the C layer. Finally, they are fed into (usually) multiple fully connected layers (fc). Credit must be given to these fully connected layers which are nothing more than what you find in any ordinary MLP.
I know of no other neural architecture that profits from feature locality in the same sense as ConvNets do.
Is it known why convolutional neural networks always end up learning increasingly sophisticated features as we go up the layers?