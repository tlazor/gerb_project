Increase the logging level of sshd. Copy the sshd_config file to a new one on a different port, and start it with  sshd -d -f.  Open any host firewall to listen on this new port. If you can get authentication working on the debugging sshd, move whatever changes you make into the regular service sshd.
Confirm what the restore time objective is for this box. It may be faster to build a new host and restore any important data from backup.
Personally, I do not partition data volumes, not after the initial OS install. Create LVM physical volumes on entire disks instead.  In this case, that could be: 
From your rescue boot, try to get your original partitions (especially /) up and running. You'll need to get very comfortable with mdadm (see the man pages and https://en.wikipedia.org/wiki/Mdadm, as well as other RAID HOWTOs, the Linux SAG, etc.)
I booted a rescue image of my hoster, created the partitions correctly and put the /dev/md4p1 into the fstab file as /home. But I am still not able to SSH into my server. 
If you want to try to fix it by yourself, keep in mind that you already broke it and you might put your RAID configuration past the point of no return, if it's not there already.
Assuming you don't have backups and you can't just go with a full re-install, your best bet is probably to get your hoster to try to fix it for you, even if you have to hire them. They have people who deal with broken RAID arrays all the time and it will probably be a lot easier for them in person than for you remotely.
After reboot , you can have a ssh access , and i will recommend you to backup all files and rebuild a new one .
Don't even try to get your server up and running yet. Stay in Rescue and try to understand how all of your disks, RAID arrays, and possibly LVM are configured. Do not try to fix anything, because it's very easy to accidentally overwrite a partition table.
Your choices at this point are "not good" and "really bad", because it sounds like you might not have enough expertise to troubleshoot and fix this remotely without making things worse. Working only within a rescue mode, without being able to see what's happening on screen when booting up, is somewhat like working with your eyes closed.
Once you think you fully understand how it was originally configured, you might be able to get the server up and booting off its / filesystem. This is presumably a different md device than the home (/dev/md4) that you theoretically broke, but since you actually can't boot the server anymore, it's extremely likely that this one is broken as well. :(
My Debian root server has 3 RAID-1 (they were preconfigured). One of them is a 1980GB drive (/dev/md4) mounted as /home. I tried to split it, to make a second partition for my databases.  But as I found out it didn't had a partition table. 
I tried to create a new partition on it with fdisk and it has accidentally overwritten the old one. I tried to reboot, but now I'm not able to SSH into the server anymore.
Also, unfortunately, it looks like you must have changed more than just the partition table for /home if SSH isn't even starting up. ("Connection refused" means the server probably isn't even booting, not that you can't log in because you're missing /home.)
You need out of band management to fix a problem with booting, network, or sshd. Usually some kind of console provided by the hypervisor or hosting provider.
Sorry to be the bearer of bad news, and I have had remote servers broken myself and know how painful it is. Try to work with your hosting company/provider and maybe they will give you a discounted price. Ask yourself if the data on the server was worth more than $200 to you.
If you can get to that point where the server actually mounts its root filesystem, then it might actually boot and start SSH. Then you can also fix the inability to actually log in via SSH as some other commenters have already provided assistance for (disclaimer: I work for Userify, which does SSH key management), but I think you're unfortunately a long way from that at this point.