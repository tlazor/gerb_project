The problem with this is that using these to test strings is easy, but generating them is hard, since there are usually infinite possibilities. You might not want trees that take seconds to generate, because rng happened to choose the repeating path over and over again.
When doing compiling, grammars are used to check whether the code is put together in a compilable way. Let's say your grammar has an expression rule, that can be basically anything from function calls to basic math and you want to implement for loops. The easiest method is to also define a block rule which consists of a list of expressions and use that for the loop. But you might also want to add single-expression (blockless or one-line) for loops (for (a in b) doThis();), then you also let the for loop be followed by an expression. Something like the following:
Where ([a] || [b]) gives a choice between the two. This lets you create pseudo-recursion where rules can apply to arbitarily long token chains (like in the case of the multi-expression helper rule, which can apply to one, two or more expressions).
However, traditional L-systems usually only generate self-similar fractals. This can be solved by making some rules randomly choose between two or more options for the letters (like when doing trees for instance, a branch could either abruptly stop by generating a constant/terminator or branch further)
A good alternative as mentioned by Bram in the comments is to use an L-system. L-systems are very close to lexical grammars, but they are separated into stages. Each stage you take every letter, apply the production rules and output the new string. You can stop whenever you want, extract the constants and do what you want to do with them.
Grammars might not be the best here. There's a fundamental difference between what compilers do and what you want to do: the applying direction.