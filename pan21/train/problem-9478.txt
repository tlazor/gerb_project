Also, lftp can parallelize file transfers via ftp, ftps, http, https, hftp, fish, sftp.   A lot of times, there are some advantages to using lftp, because managing permissions, restricted access, etc for rsync can be challenging.
So I wrote this script to run 1 rsync for each directory it encounters.  It depends on being able to list the source directories (be careful to escape ARG 3) but I think you could set that stage with a non-recursive rsync that just copied files and directories to the appropriate level.
It also determines how many rsync's to run based on the number of processors but you might want to tweak that.
I created the following script to upload many folders with pictures in parallel. You run it with first the sync target and then all the folder names to copy.
I'd suggest you find whatever it is that's doing this rate-limiting and have a serious talk with whoever maintains/manages it.
No. No such feature exists. You could split the synch into multiple calls to rsync if you really wanted to.
Looks like someone has written this utility for you.   It breaks the transfer into parallel chunks.  This is a better implementation than the "parallel big file" version listed under GNU Parallel:
I just had a similar problem having to move several TB from one NAS to a different NAS with no backup/restore capability that would allow me to just feed 1 set to the other.
Then run 1 rsync for each file in your list if you used xargs to manage the number of rsyncs you had going this could be very elegant.  Actually probably a more elegant solution than my little script here...