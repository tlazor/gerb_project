I always like to emphasize that stuff like physics simulations (which is what I work on, and have also done assignments for students) or image classification should in principle, mathematically speaking, have infinite computational cost: an image is a function from continuous 2D space to colour space, i.e. the space of images is an infinite-dimensional $L_2([0,1]^2)$ Hilbert space. When doing numerics one quickly (IMO, often too quickly and without proper thought) restricts everything down to a finite-dimensional subspace thereof, but that's to some degree arbitrary â€“ the subspace just needs to be big enough (and suitably constructed) to still represent the features that are important for the use case.
Scaling down the project might work, actually, at an undergraduate level, since the students aren't expected to build commercial quality software, but only enough so that the appropriate learning occurs. If this were a doctoral program, on the other hand, where the students are involved in serious research, then there doesn't seem to be any alternative to finding the resources - perhaps through grant writing or even begging resources from local organizations. 
But for the current situation, simply realize that there are several ways for the students to learn, not just through successfully building software for a problem. While you might be able to change the conditions for a future offering of the course, you need to deal with today's students. 
However, if you only want an answer to the stated question (how to provide for computationally intensive projects), then you need to either scale down the exercise to make it reasonable, or you need to provide adequate resources somehow. Upgrading all the laptops is probably not in the works, but you could, perhaps, provide access to a more powerful system (or systems) on which the students all work (individually or in groups). 
Besides that there is an offer available (Monthly free credits) if someone has access to a MSDN subscription or you can get a MSDN subscription as part of the MCT (As Professor) benefits.
There is currently running a 200 USD free trial offer that you can utilize. There is one setback about this offer though. I think it requires a credit card in order to activate it. But the 200$ will last you the 1st month and allow you to spin up some serious hardware during that time.
If the results aren't so great with a stripped-down accuracy, well, that's actually particularly instructive, because this kind of tradeoff is absolutely crucial in real-world applications too. You may have more processing power available in a research of commercial situation, but you'll also have much higher demands in terms of data amount and needed accuracy. So, discuss these points intensively, and make a presentation of how much better the students' results look when run by you in higher resolution on a solidly powered machine.
Besides that I agree with other that mention it is hard to give assignments that require resources that are not commonly available. 
In an image-processing example, you should definitely check how much you can downsample the images that the students should work with and still get somewhat sensible results. If necessary, choose only a smaller set of images with clearer distinguishable features, that the algorithm can pick up quickly.
First, and you may have done this yourself, before you give any assignment you should create a reference implementation yourself and test it in the student's environment. If you did this, then just let it be a warning to others. This is true, actually, for any assignment, not just one that might run up agains limitations. 
One of the most valuable educational experiences is a Retrospective of a project in which you formally explore what worked well and what needs to be changed (or should have been changed). Instead of a working program per student, a paper per student (or student group) can impart the desired learning. Students could examine the results of other students, for example, and reflect (in writing) on the relationship between the code and the outcomes. 
Most computionally intensive problems can be stripped down to something that's just as instructive but runnable on any normal computer. In particular, for inexact algorithms, you can generally trade off accuracy against computational cost, and often actually get still quite good results with only a fraction of the cost of a publication-level run.