Were you by any chance using VLANS? I found using a VLAN on the VMkernel port for storage caused the 4000-5000ms 'hangs' for all storage traffic on the VMHost. However if I move the VMkernel port off the VLAN so it receives untagged packets I don't see the issue.
A large Write Call can break up into 300 individual TCP packets, and only the first is delayed, but the rest all fly through. Never does the delay occur in the middle. I'm not sure how window size could effect the beginning of the connection so drastically.
I'd start tweaking the NFS options like NFSSVC_MAXBLKSIZE downward rather than the TCP window. Also, I noticed that 2.6.18 works while 2.6.38 doesn't. I know that support was added for the VMXnet3 driver during that timeframe. What NIC drivers are you using on the hosts? TCP offloading yes/no? Around the 95second mark there is more than 500 TCP packets for a single NFS Write call. Whatever is in charge of TCP and breaking up the large PDU could be what's blocking.
Grasping at straws here, but what NICs are you using in these servers? The Stack Overflow sysadmins have had weird networking issues with Broadcom NICs that went away when they switched to Intel NICs:
I have what looks like the same issue using ESXi4.1U1 and CentOS VM's. The hosts are Dell R610s, storage is an EMC2 Isilon cluster.
Thanks, nfsstat looks good. I've reviewed the capture. Haven't found anything conclusive, but did find something interesting. I filtered on tcp.time_delta > 5. What I found in every delay instance was the exact start of an RPC call. Not all new RPC calls were slow, but all slowdowns occurred at the exact start of an RPC call. Also, from the capture it appears that 192.168.250.10 contains all the delay. 192.168.250.2 responds immediately to all requests. 