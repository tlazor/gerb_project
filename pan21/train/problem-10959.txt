To validate such a hypothesis, you can try applying other string similarity metrics and compare the results. However, be cautious: different metrics might be useful in different contexts.
Decision trees can classify categorical data. Even if they treat every string as a separate (non comparable to the others) category, they are still able to detect when two strings are equal.
This is not the case with statistical methods, such as logistic regression. These need I'm interval data. This is why you need to define a string similarity metric over your strings. Unless there's a reason for defining your metric this way (notice that differences in the first character of otherwise similar strings get evaluated as much more distant than differences in the last character), probably the algorithm is confused by the values of your strings. Thus, it is possible that your regression learns something that you've introduced with your string conversion, and which does not exist in the original data. This introduced dependency could be confusing your regression and could be overshadowing a more efficient way of learning.