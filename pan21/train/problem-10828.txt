I have seen servers last over 10 years. When I was at a $BIGPHARMA in 2005, I performed decommissions and retirements in the datacenters. I retired a handful of old HPUX cabinets, did some SGI IRIX machines (Indys, I think), some Netware boxes that had to be near 10 years, and at least two Sun SPARCstations. One pizza box, one lunchbox, and I think they were both running DNS servers.
This can't be said as confirm it is totally depends on how servers has been used and what was the use of server. What application installed on server. How much load was server having.
However, they may not be economical in that respect. The capacity/power of new machines keeps rising dramatically, while your overhead costs (human and power) remain expensive. At some point the raw capacity becomes just not worth it.
In my experience they can easily run for 5 years easily before becoming unreliable. (replacing the disks in the raid array along the way).
Generally, if you're buying a server from one of the big vendors the service-contract for it will be pretty cheap until it turns 5 years old. At that point, if you can even get one, it'll be VERY expensive. For our hardware vendor the 1 year service-contract cost at Year 6 is about 50% the cost of a new server. This is how they encourage regular hardware replacement. You are factoring in the service-contract in your ROI calculations, right?
Once a server turns 5 around here it tends to get relegated to test/dev/low-impact roles and we keep a grave-yard of similar machines for parts if it comes to it. Those of us who keep this hardware running frown very discouragingly when entities ask to use old stuff that we have 'just laying around'. 
My oldest monster right now is an ancient HP LH3, a 12 year old server, that is performing a single task: monitoring our datacenter UPS internals. The next oldest is a 9 year old that's due to be replaced in the next 3 months. Neither of these have service-contracts, and if they die we will sigh deeply but won't be otherwise inconvenienced.