Yes you can. I'd expect it to help a little with vanishing gradients, but not be as effective generally as a LSTM is. Also, without some kind of bounded function in the feedback loop, a basic RNN using ReLU will suffer very easily from exploding values, resulting in very large errors as the feedback is run over multiple time steps - so you may need some adjustment like gradient clipping to account for that.
For solving the problem of vanishing gradients in feedforward neural networks, ReLU activation function can be used.
Even using ReLU in simple RNN architecture, I suspect that by most measures and in most problems, the LSTM or GRU architectures will perform better. There will be some edge cases where RNN performs better, such as if your goal is to get to a certain accuracy quickly during training and not necessarily to get the best accuracy in the end.
If you already have a problem to analyse, have prepared your data and decided on a top level architecture (e.g. how many recurrent layers, whether this is a seq2seq type model etc), then it should be relatively easy to compare LSTM with a basic RNN using ReLU. It is likely different hyperparameters would suit each architecture so you may need to do a search, and you could compare best possible results for each model, or look at things such as number of parameters or training time.
I expect many researchers have already tried this comparison, although the examples I found on a quick search appeared to use tanh for the simple RNN. 
When we talk about solving the vanishing gradient problem in RNN, we use a more complex architecture (e.g. LSTM). In both of these, activation function is tanh. Can't we use ReLU instead of tanh in RNNs for solving vanishing gradients too rather than opting for a more complex architecture?