For example, you will NOT get Gigabit speed if your cable has only 4 conductors as Gigiabit speed uses the 4 pairs/8 conductors.
If your switch and your server supports Gigabit Ethernet, you probably need to check your cabling. Not all cables support Gigabit Speed. Also, you may have a bad/damaged cable/connector, etc.
Running ethtool eth0 on the server provides shows that 1000baseT/Full is a "Supported link mode" and is part of the "Advertised link modes" as well.
This is fairly easy to test by getting a gig switch (cheap is fine) and a Cat5e or better cable and connecting the two directly just to see if the link speeds come up as gigabit. (If your in-wall cabling is Cat5 - especially likely in older buildings - then you may have an expensive rewiring job ahead).
I'm having an issue where some of my servers are not autonegotiating to the 1GB/s speed from their 1Gb NIC interfaces.  These servers are Dell R610s running RHEL 5 wired to a Cisco switch.  I have attempted to force the 1Gb speed from both the switch and server sides but have had no luck.
The previous commands will work if I try to force the speed to 100 instead of 1000.  This has been puzzling me since we have other R610s which autonegotiate fine to the 1Gb speed.
I've tried to force the 1Gb interface by first running ethtool -s eth0 autoneg off and then ethtool -s eth0 speed 1000 duplex full.  After running the second command I get the following errors:
Gigabit Ethernet requires Category 5e ("enhanced") or Category 6 network cabling at ALL points - that means from the computer/server to the jack, the jack to the patch panel, and the patch panel to switch.  You may need a Cat5e certified patch panel and/or Jack as well.  I've heard some people dispute these.  And on SHORT distances, you MAY be able to get away with the cable/panel/jack being only Cat5, but I wouldn't try to.  I have personally seen auto-negotiate and link reliability issues on gig connections when the cable path is NOT Cat5e or better.  