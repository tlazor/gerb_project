There is very little you can do to prevent your hosting company from doing this again.  In order to do so you would need to have a router or other host running BGP connected to your provider at a minimum.  Unless you also have another provider, it wont help if they accidentally turn off the peering router.  
For the record, it exists several gratis BGP monitor and alarm systems. None provide a resolution of 15 mn as you want. And, since you can have many other causes of outage, monitoring the IP connectivity from the outside is the only real solution.
A better solution may be to have a failover site as mentioned by another answer.  Depending on your risk tolerance you can setup failover to happen in a very short time but it involves complete control of your DNS.
As far as monitoring is concerned, we have a dedicated server outside our network, which we use as an external Nagios server. You could buy a cheap VPS server and use that to monitor things from the PoV of an external user. For example, we check SMTP and HTTP work, rather than checking that exim and apache are running, which we do on our internal monitoring.
You're unlikely to be able to run BGP unless you have at least a /23 of Provider Independant address space and have an ASN number. As such, you need to trust your hosting company. Router changes tend to be fairly rare, so the likelihood of this problem happening again is slim. You could investigate any SLA you have with them, but this is probably just going to involve getting a refund on your hosting fees.
It may be easier to just ping both your host and the router one step back from your server from the outside.  You can use traceroute to determine which address to use.  
Depending on how things are setup, the size of the netblock being advertised, and how things get agregated upstream, you may be able to use one of the looking glass scripts to monitor BGP announcements for the block your server is in.  