so, to compute a DNN or CNN learnable parameters, you can build your model using keras, keras can automatically generate the models learnables parameter for each layer, when you complete building your model type model.summary() it generates a list of learnable parameters when you run it.
In neural networks in general, and in deep learning algorithms (CNN, DNN, ...) that are also based on neural networks, learnable parameters are parameters that will be learned by the model during the training procedure such weights and biases.
DNN is a deep neural network, what I understand that when a neural network becomes deep, it can be said that its a deep learning model.
A more common situation is given when you do transfer learning with pretrained neural networks. In this situation, you take a trained neural network and modify it (e.g. add additional layers or chop of some layers and add your own layers). Then when you want to train the new network your data set you can decide if you want to use the pretrained parameters (non-trainable parameters) or train the network from the scratch. In the last case, you will have a model in which all parameters are trainable.
Imagine a simple input vector $\boldsymbol{x}=[x_1,x_2,x_3]^T$, with $x_i\in \{0,1,\ldots,255\}$. If our neural network has a layer which normalizes the inputs into the range of rational numbers between $0$ and $1$. The output of this operation would be
How would i compute the number of learnable parameters in a DNN? Could anyone please explain what those are with an example? I'm new to machine learning so i would appreciate some help on this.
I've came across the term "learnable parameters" recently, and googling didn't help much as most search was describing learnable parameters in a CNN instead of a DNN. Is there any difference between the two?