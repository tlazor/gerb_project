The TV is rated HDMI (UHD) (which is really just a stupid marketing name for HDMI 2.0), but your graphics card does not implement the HDMI 2.0 standard.
You cannot get 4K@60 Hz using HDMI older than 2.0, the 1.4 spec. does not have the necessary bandwidth.
This really has a lot less to do with the cable or a fundamental limitation of your graphics card itself, but rather a limitation of the HDMI specification at the time the card was manufactured. A Radeon 5100 does not support HDMI 2.0; about the best you could hope for is 4K@30 Hz over HMDI 1.4. If the card supports Display Port the story might be slightly different, but chances are your TV does not so that makes it moot.
This is very much similar to the days when RAMDACs dictated the maximum resolution/refresh a graphics card supported; the card might have been able to draw at higher resolutions, but the problem was actually a limitation of the technology used to output an image from the card. Gone are the days when the framebuffer had to be converted to an analog signal by a separate signal processor, now the limitation is actually the version of the digital signal standard(s) the card can communicate.