I'm afraid there's no meaningful definition of "best" unless you specify some concrete metric.  If the results in each case seem reasonable, then all you can do is be educated about the various distance metrics and decide which makes the most sense for your use-case.  (For example, Euclidean distance measures spatial difference, while cosine similarity measures directional difference.)
Note that this is just a general strategy, many parts of it can be refined/adapted to your data and of course it depends how much time you want to spend on this problem.
Based on the dataset you have built, you can now train a supervised model, with a pair of titles as instance. You can use various similarity measures as features, and you should vary the type of similarity (char-based, ngram-based, word-based) across these features in order to provide the model with a diversity of characteristics.
A proper evaluation framework would require annotating manually a large amount of pairs of titles as similar/not similar (or even a degree of similarity). Unless you have a lot of time, this is completely impractical because there is certainly a massive imbalance between positive and negative pairs. So instead you could use bootstrapping, which means running a few similarity measures on your data, extract the top N pairs for each measure, then manually annotate only these. It's likely that this would give you a high amount of (rare) positive cases, and you can build a labeled dataset by assuming that other instances are negative. It's obviously a simplification, otherwise you can take the time to annotate a lot of negative cases as well (it's still much faster than without bootstrapping, since you already have your positive cases).
If you really want to, then the K-Means implementation from NLTK allows you to specify a custom distance metric.  Just keep in mind the GIGO principle.  If you violate the assumptions of the model, don't be surprised if you get poor results.
Then you can predict the similarity between any two pairs. This gives you a graph of similarity relations between all the movies, from which you can extract groups which are similar together.
First, you could have a look at approximate string matching measures. These are likely to give you much better similarity results on a pair of movie titles. It's usually a good idea to use not only word-based measures but also character-based or char n-grams based measures.
Keep in mind that k-means is designed for Euclidean distance, which is why many implementations (such as scikit-learn) do not allow the use of any other distance function.  For many other distance metrics, such as cosine similarity, taking the mean of the points in the cluster is not appropriate, so you'd also need to replace the centroid-estimation function as well.