Note: it's crucial to consider parameters as a whole, but not just separately (if value $a$ of the parameter $A$ is not the best choice, it doesn't mean, that it won't be the best option in combination with value $b$ of the parameter $B$)
As you have not mentioned the strategy, which you use for parameters choosing, I'd suggest to attend to it. For example in this article https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53 you can find, how parameters can be chosen. 
It would be reasonable to check outliers in the data (objects which appear to be inconsistent with the remaining instances). For this aim you can use box plot approach. It might be the reason, why it's impossible to get particular model performance.
The other powerful approach is Bayesian Optimization Methods https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a It allows to move in parameters values space, taking into account previous results of optimization.  