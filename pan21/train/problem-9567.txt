If the guest is aware that vCPUS physically share a last-level cache (LLC, usually L3) cache enqueueing tasks is relatively cheap operation that consists of adding the task to a list and setting some bits in a data structure.
Qemu recently added support for a virtual L3 cache option which allows you to tell the guest about these relationships if it can't figure out on its own. The patch also have a pretty nice description of the performance wins achievable.
On the other hand, if KVM does bare metal virtualisation, maybe the cache levels reported by the guest represents a direct correlation with the real CPU, since the guest has direct access to the hardware CPU. Thus installing a better CPU will give better performance in the guest.
On the other hand, if vCPUs do not share an LLC an inter-processor interrupt (IPI) needs to be sent to the destination vCPU which is much more expensive, especially on a guest where handling interrupts requires switching into the host via VMEXIT.
It depends. Certainly the cache topology (which virtual CPUs share a cache) is used by the Linux kernel scheduler in the guest when enqueueing tasks on vCPUS.
In Host, "lscpu" will show the L1/L2/L3 cache info. Guest also have such cache info when use "lscpu", because of the guest is implemented as a host standard process, I want to know the L1/L2/L3 cache which see in guest is really matter for guest?
The host makes this data available to the guest, via a virtual CPU/Core. I can imagine that the host can provide the guest with arbitrary values without really affecting performance that much, since it's the host that ultimately determines performance anyway.