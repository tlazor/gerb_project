Basically, you adjust the likelihood in the /proc/*/oom_adj file. Eg. raise the likelihood of killing any of the currently running apache instances?
Also limits.conf has lot of limitation,better Solution is to use cgroups to restrict the memory utilization per process
First of all I would say restart is not a problem of solution and the better way is to found the offending process and why it's consuming high memory.
Like mentioned above linux already have OOM mechanism to find the offending process and to kill it to release memory pressure
Also i recommend completely disabling swap on a server where you have this issue, because swap is so slow that it can grind the server to a virtual standstill, even though theres still swapspace left, thus never triggering the OOM killer.
I have a Debian Wheezy VPS box where am running a couple of Django apps in production. Ideally, would have tried addressed my current memory footprint issues by optimizing the apps, adding more RAM or augmenting with Swap. But the problem is that I doubt there's much memory optimization I'd milk from optimizing the Django apps (the stack being open-source and robust), and adding RAM is a cost constraint for me (this is a remote VPS), also, the host doesn't offer options to use Swap!
So, what I would love in a solution is the ability to detect when a process (or generally, total system memory usage) exceeds a certain critical amount (for now, example the FREE RAM falls to say 10%) - which I've noticed occurs after the VPS's been up for long, and when also traffic is suddenly much to some of the heavy apps (most are just staging apps anyway).
vm.panic_on_oom = 1(/etc/sysctl.conf),this will generate vmcore when system is going out of memory.You can find more info about it here 
The linux kernel has a so-called OOM Killer built-in. It is the "Out of memory killer". So when your box has exhausted its ram & swap, the kernel will start killing stuff to make the server accessible. 
So in /etc/cgconfig.conf you can define the control group like this(here I am restricting my app to use only 256MB of memory)
So, in the meantime (as I wait to secure more resources to afford more RAM), I wish to mitigate the scenarios where the server runs out memory so that I just have to request a VPS restart (as in, at that point, I can't even SSH into the box!). 
and then in /etc/cgrules.conf I can define that your app utilzation(in your case django  can't exceed beyond 256)
You can tweak the priorities of processes, to determine the "likelihood" of a process being killed. 
So, I wish to be able to kill/restart the offending process(es) - most likely Apache. Which solution when done manually in these situations has restored sane memory usage levels - a hint that possibly one or more of the Django apps has a memory leak?
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html 
If your processes are leaking memory, you can use /etc/security/limits.conf to limit the amount of memory a server can contain.  This will prevent servers from growing too large.  The same effect can be achieved on a temporary basis using the ulimit command.  It may be best to use ulimit to discover an appropriate size, and then set those values in the limits.conf file.  If your server supports it, drop a file into /etc/security/limits.d rather than editing /etc/security/limits.conf.