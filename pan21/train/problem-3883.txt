The other thing you need to consider is performance of your SQL application. Is the client web based or on the PC? If it's on the PC, have a look at using RemoteApp to run the client in the cloud. Heck, you might like RemoteApp so much that you put all apps in the cloud and don't need to look at BranchCache at all because of client/server traffic would stay in Azure.
The other consideration is availability sets. Azure, etc, were designed for big "designed for the cloud" services. When MSFT does a patch cycle, everything in a fault/update domain is powered down for a short time. That means your services would have downtime. The scehdule is announced as something like "you'll see downtime sometime between Thursday and Sunday". They way around this is to create highly available services that are spread across fault domains using Availability Sets. That's easy with a DC (maybe 2 Basic A2s), but much harder and more expensive with traditional workloads like file servers.
I've tested a similar configuration, focusing on network latency (on limited consumer broadband). Latency is the key here. Good news is that browsing folders etc was boosted with the "new" network stack in Vista and later. However, file download/upload is still affected. Most file server activity is reads. I used BranchCache to accelerate this - hosted branchcache on a Microserver (maybe also a RODC if that is possible) would be ideal. 