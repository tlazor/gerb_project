Alex made a number of good points, though I might have to push back a bit on his implication that DBSCAN is the best clustering algorithm to use here. Depending on your implementation, and whether or not you're using accelerated indices (many implementations do not), your time and space complexity will both be O(n2), which is far from ideal.
Personally, my go-to clustering algorithms are OpenOrd for winner-takes-all clustering and FLAME for fuzzy clustering. Both methods are indifferent to whether the metrics used are similarity or distance (FLAME in particular is nearly identical in both constructions). The implementation of OpenOrd in Gephi is O(nlogn) and is known to be more scalable than any of the other clustering algorithms present in the Gephi package.
FLAME on the other hand is great if you're looking for a fuzzy clustering method. While the complexity of FLAME is a little harder to determine since it's an iterative process, it has been shown to be sub-quadratic, and similar in run-speed to knn.
Kernel k-means could work IF your similarity is a good kernel function. Think of it as computing k-means in a different vector space, where Euclidean distance corresponds to your similarity function. But then you need to know k.
PAM (K-medoids) should work. Assign each object to the most similary medoid, then choose the object with the highest average similarity as new medoid... no triangle inequality needed.
... and probably many many more. There are literally hundreds of clustering algorithms. Most should work IMHO. Very few seem to actually require metric properties. K-means has probably the strongest requirements: it minimizes variance (not distance, or similarity), and you must be able to compute means.
k-means, for example, requires a given k, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?
Assume that we have a set of elements E and a similarity (not distance) function sim(ei, ej) between two elements ei,ej âˆˆ E. 
Note, that sim is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of E).
All it needs is a binary decision. Commonly, one would use "distance < epsilon" but nothing says you cannot use "similarity > epsilon" instead. Triangle inequality etc. are not required.
Hierarchical clustering, except for maybe Ward linkage, does not make any assumption. In many implementations you can just use negative distances when you have similarities, and it will work just fine. Because all that is needed is min, max, and <.
Topological Data Analysis is a method explicitly designed for the setting you describe. Rather than a global distance metric, it relies only on a local metric of proximity or neighborhood. See: Topology and data and Extracting insights from the shape of complex data using topology. You can find additional resources at the website for Ayasdi.