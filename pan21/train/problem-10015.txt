There's also software designed to do this task, and more, on their own. One such programs, which I've used myself, is simply called "motion" and is available for most distributions. It has built-in motion triggering (record and/or take snapshots) or continuous modes. It can be a bit CPU-intensive on systems like a Raspberry-Pi, but it certainly works.
That way you can run your script as often as you want using cron or a systemd timer, and have it be flexible enough to always try to upload any files which it didn't succeed with the later time it ran.
I would accomplish this task by making a bash (or perl, node.js, ruby, powershell?, ...) script that calls wget and outputs to a filename with the datetime. Call aws s3 cp ... in a for loop to upload all of the files in the folder.  In the loop, upon each successful aws s3 cp call for each file, move it to an archive folder to be saved locally as well.  If you don't want a local archive use aws s3 mv to auto-magically purge the things that have already been uploaded.
I would use AWS S3 instead of an FTP server in EC2, and the AWS CLI tool to upload the files.  It is a much lighter solution requiring no systems administration.  S3 provides much more durable storage than the volumes for EC2.
If you want to step it up a bit, perhaps run multiple remote/local cameras, and have the motion detection offloaded to a more powerful central machine, look at Zoneminder. It takes longer to set up, and is in my experience picky about you manually setting the correct resolutions on your camera feeds, but it can be scripted to some degree.
Like you mentioned, there are several tutorials about scripting FTP uploads available. At least one of them should have included an example which uploads files by a pattern, such as "Snapshot-*.jpg", where the wildcard would match the timestamp. Or, you could point the FTP program (such as lftp or ncftp, which have binaries meant for scripting) to upload everything in a certain folder. Then wipe the folder if the program succeeded.
You can create a user that can only upload to the S3 bucket using IAM (so the criminals cant erase the files!)  
My first tip would be to name the files using the date and time they were taken. That way you won't need to keep a counter anywhere, which would be difficult in a script which doesn't run continuously as its variables would get reset on each invocation. You could store the variables in files, but it's easier if you ensure the names won't collide.
Something like wget http://127.0.0.1:8080/?action=snapshot -O "Snapshot-$(date).jpg" if you are using Bash. (Sorry if the syntax doesn't work, I'm no Bash expert and I'm typing this in my phone.)