dd_rhelp is a shell script that uses dd_rescue "[...] on your entire disc, BUT it will try to gather the maximum valid data before trying for ages on bunches of badsectors"
I have found that playing with the -K parameter you can speed things up. From what I've seen if ddrescue finds an error when running with the -n option tries to jump a fixed amount of sectors. If it still can't read it jumps double the size. If you have large damaged areas you can indicate a big K value (for example 100M) and so the jumping on an error will be larger the first time and it will be easier to avoid problematic areas quickly in the first past.
Then I moved the rescue image and logfile to another temporary place, re-formatted the USB hard drive with the ext4 file system, moved the files back on it and resumed the ddrescue process - and now it runs with 1-20MB/sec again (fluctuating but around 7MB/sec on average)!
What is the file system of the hard disk where you save the rescue image and the logfile? I just made the experience that rescuing a 500GB internal hard drive (connected via SATA) on a Laptop running Linux Mint from a USB Stick, saving the rescue image and logfile on an exFat formatted USB hard drive, was starting rather slowly (1-2MB/sec) but after around 250GB it was only crawling at <100KB/sec. It seemed to become slower the larger the rescue image file was growing.
If your aim is to obtain the bulk of the data intact, then you could speed up its extraction. But if you really want to rescue as much data as possible, then letting ddrecue nibble at each and every is the route to take.