If you're trying to "serve" the Windows share from a non-Server build of Windows (XP Pro, Vista Business, etc), then you're going to run afoul with the 10 client connection limit "feature" of these operating systems.
autofs has given me some trouble (if the mount isn't available, attempts to access take forever to time out), but overall it seems to be performing. I'm using it in a one linux server to one windows server configuration (in petrol station back rooms), and the windows server (and associated network) are not necessarily the most reliable.
A third option is to write a script of your own that checks whether the mount is OK (by trying to access it) and if it's NOT OK, perform the umount & mount to get it back into operation. This has the advantage of being under your control, but the disadvantage of not being able to respond instantly to program requests.
A Windows Server OS on a reasonable modern server computer will handle 60 simultaneous connections to a share with no headaches whatsoever. I've had well over 300 simultaneous connections from Windows clients to the same share on a Windows Server with absolutely no problems (on vintage 2004 hardware). I can't tell you what the per-client encumbrance to server memory is for each "connection" but it's not a worrisome number.
The other (easier) option is to mount it using fstab so that it's always mounted.  Are there any performance implications for mounting a CIFS share using fstab on so many machines?
The problem with a simple fstab mount is that if anything goes wrong at mount time, you're hosed until you can manually intervene and fix it. For some applications, this may actually be more useful.
I need to mount a Windows share on a large number of computers (nodes in a supercomputer).  The share will rarely be used, but it should be there when needed.  It seems like a good case for some type of automounter.  Unfortunately, I'm getting spotty results with autofs.