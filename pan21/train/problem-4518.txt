(The main drawback of L2 loss is that the model becomes more sensitive to outliers, therefore less robust.)
I am confused about how this code penalize weights? the code adds L2 to weights then adding the result to loss. could anyone please explain this for me?
I know that L2 regularization technique to used to reduce over-fitting and penalize large weights. In more than one place I saw that it used like the code below in tensorflow library:
L2 loss is based on the square of the weights of your Network. As a given weight increases in size, the loss will increase exponentially (quadratically, to be precise). Neural Networks are therefore "pushed" to spread the weight values more evenly across each layer, since for such a quadratic loss factor it's better to have a lot of smaller weights, rather than few very big ones. The exponential relationship between weight value and loss is the reason why large weights get penalized. L2 loss is typically useful when your model suffers from overfitting, since weights redistribution across layers is good to prevent few neurons from staling all the "explanatory power" and generating overfitting.