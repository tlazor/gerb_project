When the data is highly imbalanced, accuracy could be a bad measure of model performance. Instead consider precision and recall values, which separately takes into account the no of positive and negative samples. One idea to improve the performance is to use class weights (generally inversely proportional to the no of samples in that class) while calculating loss metric. This makes the model understand that class with less samples should be given more priority while training.
For anomaly detection problems, i'd recommend treating it some other way  by training only on the normal samples and in prediction mode , classify samples that are normal as normal and flag everything else as abnormal. This approach is usually followed when you don't have enough "abnormal" samples in any anomaly detection problem to train on. Plus , no matter how much you train your model on anomalies, your model could face an unseen anomaly and pass it as normal and that could be fatal ( in healthcare especially ). You can use your abnormal samples as test samples in a hold-out set as a final evaluation step for your model.