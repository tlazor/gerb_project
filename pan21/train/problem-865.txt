In the previous section, I said that training an RBM with SGD, as described in your question, works. This is not the full truth: this is NOT an RBM anymore!
where $E$ is the energy function, and $Z$ is the partition function (to normalize $p$ such that it is a valid probability).
As described in the answer by Quittend a restricted Boltzmann machine models the probabilistic distribution. The goal of RBM training is not to exactly reconstruct the input, but to learn the probability distribution of the data. The reconstruction is thus not exactly the same as the input, but is a sample from the same probability distribution.
We try to make $\hat{x}_1$ equal to $x_1$. In your code, you add the additional constraint that $W_1$ (the weights from the input layer to the hidden layer) are equal to $W_2$ (the weights from the hidden layer to the output layer).
During training, we want to increase $p(\mathbf v)$ for all $\mathbf v$ in our training data, so we have to calculate the derivative 
Yes, you can train a model like you described. This is well known under the name "Autoencoder". The goal of an autoencoder is to find a (usually smaller) representation of the input data, which can then be used to reconstruct the data. This looks as follows:
Training an unsupervised neural network with SGD exists and is known as autoencoder. An RBM is a different concept, which models the probability distribution and does not strictly reconstruct the input. RBMs can not be trained with SGD.
The goal is to assign a high probability to the input data we train on. The probability of an input vector $\mathbf{v}$ is given by