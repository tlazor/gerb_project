Running one process at a time, I am always limited to about 4mbps. I have FiOS at 35/35mbps speeds, so it isn't an outright limit. AND, I can run parallel instances and can go all the way up to 35mbps, so I know the problem isn't gateway/nic/machine/amazon related. Running parallel instances works to a degree as a solution, but increases the complexity of my workflow greatly. Solving this would make my life dramatically easier. 
I strongly suspect you're seeing a bandwidth-delay product (BDP) artifact. A "long, fat pipe" (high bandwidth, high latency) limits the amount of data that TCP can put "in flight" at any given time. Your observation that parallel transfers run faster is a big indicator of BDP coming into play.
When I was first doing this I was playing around with a bunch of Windows TCP parameters and was able to briefly get unconstrained bandwidth, but it wasn't repeatable. 
You might try enabling the Compound TCP Congestion Provider (CTCP) via the netsh interface tcp set global congestionprovider=ctcp command. This congestion provider is supposed to "aggressively" set the send window to put more data "in flight" on high bandwidth, high latency networks.
I do a ton of uploading to S3 and am experiencing capped speeds and I can't quite figure out how to address it. 
The setup: Windows Server 2008 R2 x64, external HD, using a Java based upload tool called Jsh3ll and custom VBS scripts to kick the jobs off. 