By the way, this method could work when scaled up or down to meet your accuracy and speed needs. (e.g. if your CNN still trains too slowly with sentence vectors, you can create paragraph vectors instead).
One way to handle documents of different length is through padding. Your document sequences should all be equal in length to your longest document. So if your longest document is 400 sentences then all document sequences will be 400 vectors in length. Documents shorter than the max length would be padded with vectors filled with zeros.
Either way once you have your sentence vectors, line them up in sequence for each document like you're already doing for your word vectors and run then through your model. Because the sequence length for each document is shorter, your model should train more quickly than with word vectors.
Finally, to increase speed, you could try to reduce the dimension of the text by only including important sections (maybe only the beginning of the document is sufficient to have good classification accuracy)
You could use region embeddings. Rather than converting individual "tokens" to vectors you could use a strategy to convert regions of text to vectors. This approach is used here:
Note that, with this method, you will still have to convert all the word vectors to embeddings, but not all at once. 
You could reduce the length of your input data by representing your documents as series of sentence vectors instead of a longer series of word vectors. Doc2vec is one way to do this (each sentence would be a "document").
If you're not limited to CNN, you could use a hierarchical attention models such as this one: https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf where you have a pipeline of this kind:
If you don't want to use Doc2vec, one way to create the sentence vectors would be to average the word vectors for each sentence, giving you a single vector of the same width for each sentence. This may not be as precise as some methods available through Doc2Vec but I have used it with considerable success for topic modeling.