CPUs are good at tasks that do not parallelize well and require massive amounts of decision making. They can tolerate high memory requirements even with only moderate temporal correlation. This includes artificial intelligence, user interface, disk and network I/O, and so on. So this is precisely what modern games use the CPU for.
GPU are very difficult to program. You should search howto to sort a list on a GPU. Many thesis have search to do it. 
GPUs are good at tasks that can be massively paralellized and require massive amounts of computation with either low memory requirements or high temporal correlation with only small amounts of decision making. This includes rendering images, physics simulations (particles, collision, cloth, water, reflection) and so on. So this is precisely what modern games use the GPU for.
Use a CPU with one thread is easy, use multi-threads is more difficult, use many computers with parallel library as PVM or MPI is hard and use a gpu is the hardest. 
Readback is another reason I can think of to occasionally prefer the CPU.  Not in terms of bandwidth (as GPU->CPU bandwidth is not so much an issue on modern hardware) but in terms of stalling the pipeline.  If you need to fetch back results from a computation and do something interesting or useful with them, using the GPU is not a wise choice (in the general case - there will be special cases where it can remain appropriate) as reading back will always require the GPU to stop whatever it is doing, flush all pending commands, and wait for the readback to complete.  This can kill performance to the extent that it not only wipes out the benefit of using the GPU, but may actually be considerably slower.
Developers do use GPUs for all the functions they're good at. They use CPUs for all the functions they're good at. What makes you think they don't?