Long-distance relationships will take a lot of data to learn with any model. And it's not clear to me that paying more attention to more recent data is a bad thing. Systems change over time and so paying a lot of attention to ancient data probably isn't useful.
RNNs aren't necessarily faster. It's hard to compare different architectures, but in a text-mining application I'm doing, my RNN takes 3-5x longer than my CNN to train and score.
And theoretically, RNNs (including LSTMs, of course) can process varying-length series straightforwardly, while CNNs or other NNs need some work. (In practice -- for example if you're using tensorflow as your underlying engine -- you won't be able to handle absolutely any length of series with RNNs, but the principle is there.)
The choice isn't "LSTMs are all hype" versus "LSTMs ROCK". For example, in text use (categorical time series of words or characters), CNNs are better at some things, while LSTMs are better at others.
What do you mean by "understand": Comprehended in the abstract, visualize, probe about specific predictions?