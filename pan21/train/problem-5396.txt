I guess it will really depend on what your application developers and ops engineers can support; I imagine that you won't keep this data set on a single server, for redundancy and performance.
There are many possible factors influencing the answer and without more information it is hard to give a definitive answer.  Examples of these factors are what the source of the data is and at what speed it needs to be recorded, how often it needs to be queried, how easy it is to segment etc.  
You won't like this answer, but neither. Oracle and MS SQL Server are roughly equal in terms of their large scale data handling (SQL Server can take an edge in ease of use, Oracle in utility), and when it really comes down to the raw data PostgreSQL can actually edge them both out by a very small margin when optimized.
It performs fine.  I think the scaling question is not so much the size of the DB, as it is the number of users and transactions per second.
Without doing a correct design for both, then comparing the fully-prototyped application with production-volumes of data, you're not going to be able to compare them. I guess that's going to be ineffective in terms of cost (developer effort to build TWO prototypes and test them at full data load on production-grade hardware).
In general Oracle certainly has the reputation that it can handle larger loads better and can be run on more powerful hardware than SQL Server.
Most of the performance will come out of correct design, which will to some extend depend on the features of the specific product used.
If you're writing 6Tb over 8 months that isn't actually a massive insert-rate, so the data churn won't be a problem for any decent hardware.
I imagine that either would do the job quite nicely, as well as anything else you care to name. It's all going to be about correct application design, using the specific features of the database properly. 
Oracle can do clustering to expand capacity as far as transactions/second (in certain circumstances) but I wouldn't necessarily prefer either one regarding raw DB size.
You are probably better off looking at an "exotic" database product specifically designed to handle those kind of volumes, such as Vertica or even considering non-relational products designed for massive volumes as used by the cloud service providers such as Amazon Elastic Mapreduce and Google App Engine datastore.  These products are gaining traction in industries that require huge volumes of data such as telecommunications providers and the financial services industry and the telematics industry.
But if you really want a "big" database, the kind that is proven to work well when you are making legitimate use of 64bit ID columns and TBs of data, then it's (IBM) DB2.
However I think that for large volumes as you are describing it would be wise to also consider other options unless your organisation mandates the use of only those two products.  
I can't speak to "5-6 TB of data", but I currently have 1700 full time fat-client users (Application built in .NET) hammering against a 1.5 TB database using SQL 64bit Itanium.