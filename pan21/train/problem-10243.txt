Most CS instructors are aware of security problems connected with software, but not all of them understand the hardware issues. Unless you happen to have a professional cybersecurity background, you may be better off "farming out" that one session to someone that does. In such matters, you want to present an integrated view, or at least one that is as fully integrated as you can make it.
I have seen a lot of bad practice taught, that then needs to be untaught latter on. When I was a software engineer, it was often me un-teaching new recruits. This was not easy when they were not good at critical thinking, and saw what they were taught as truth. 
Security has to be taught and the value learned, usually painfully. I have lived through many "no one will do that!" only to have a product, or network, broken because someone did.
The worse thing we can do (and I have seen this in classes), is to demonstrate bad code, unless it is part of a “What is wrong with this?”. I have seen a teacher present this as an example of how to do things (a lesson on selection).
Security vulnerabilities are just bugs. In your example the code does not do what it is supposed to do (keep the bad people out).
Make it into an assignment/game. Each student creates a project that is passed to another, randomly chosen student, to hack. Give points for both the project and the "hacking" results. The hacking assignment should also be graded.
I have heard the argument that doing this properly would be do difficult for a beginner class. I agree. I would not try to fix this program. I would just present a different program.
I would have a "guest lecturer" who was a cybersecurity professional teach one class. (Maybe I would give a test on the lecture the following day.)
I would just try to make resources available to them and make sure to give examples of what not to do (e.g. Your example). In terms of resources, I'd provide info about attack vectors, threat models, etc. Specifically I'd provide the OWASP top 10.