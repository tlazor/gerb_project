Since then, three of the five disks have failed. Thankfully I was able to replace and rebuild the array before the next disk failed, but it's got me very very worried.
If you bought all the drives at the same time from the same place it is possible that they all come from a single iffy batch.
What are your thoughts? Did I just get them in a bad batch? Or are newer/higher capacity disks more likely to fail than tried and tested disks?
The higher failure rate of large drives could just be a function of the size of the drives. A drive with fifty million sectors has ten times the chance of having a bad sector than a drive with five million sectors. I'm assuming the failure rate among large drives and small drives is the same here, which is probably not a good assumption -- as someone else said, the fact that terabyte drives are still relatively new, they probably have a a higher failure rate to begin with.
Maybe I'm overly paranoid, but I would be wary of trusting 1Tb of data to one single drive, even if that drive is part of a redundant array.
Obviously there are physical constraints at play which may make the technique impractical to you, power draw constrains too, so YMMV. As a "for instance" when an array or arrays isn't practical: I'd rather have four drives as R10 in one of our servers here in place of the larger drives in an R1 array, but it doesn't physically have room, buying/building an external array was out of budget, and we could not use space on an existing array as the data had to be kept physically separate from all other data due to data protection requirements.
I recently deployed a server with 5x 1TB drives (I won't mention their brand, but it was one of the big two). I was initially warned against getting large capacity drives, as a friend advised me that they have a very low MTBF, and I would be better getting more, smaller capacity drives as they are not 'being pushed to the limit' in terms of what the technology can handle.
You probably got a bad batch.  I am nervous about deploying arrays built from disks from the same batch for that reason -- they are likely to have a similar life-span, which makes getting replacements potentially very exciting when one fails.
When putting together a RAID array I generally recommend mixing drives a little, i.e. a mix of manufacturers or at least drives from different suppliers (to reduce the risk of all the drives being from one bad batch).
It isn't impossible that there is some design defect with the drives, that's definitely happened before; however usually the Internet is full of complaints about the drive if there is really something wrong with it, as opposed to the usual background noise that you'll find about anything.
Another recommendation I would make is to use smaller drives if possible (i.e. you have physical space for the drives and controller ports to hang them off), so instead of a RAID 1 volume or two 1Tb drives have a RAID 10 of four 500Gb units. This way when a drive goes bad you are only rebuilding a smaller array which is part of a larger array instead of rebuilding the whole array (reducing the length of time during which the array is not complete), and it also offers a little more redundancy (in four of the six of the "two drives fail at once" scenarios a 4 drive RAID10 array will live). You can do the same with combing smaller R5 arrays into an R50 array too if supported by your RAID controller/software.