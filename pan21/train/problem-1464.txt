Like I said - very little to do with code from my POV - perhaps someone can skim through it and add their ideas on how to optimize it further. 
Not quite a lot that can be done performance-wise...you have a lot of data to pour over (16,000 * 81,000 = 1,296,000,000 records and with the new data 17,000 * 160,000 = 2,720,000,000 records). You could possibly think of moving over to a faster database (Redis? Cassandra?). 
If you need more information about what the RPI (Ratings Percentage Index) is, this is a good primer. Please note, I have modified from this formula to include a weighting factor (function reduction_factor() ) so it is slightly different that that page states.
Is there a good way to prepare the queries used in the class? That is, can I prepare the queries once somehow, so when they are called ~16k times each it helps?
Anyways, I made a few minor changes to your code - added empty() checks rather than empty string checks, shortened a few if statements, etc. 
It currently takes about 10 minutes to process ~16k teams and ~81k games. I could soon have ~17k teams with ~160k, and multiple sports. I run this as a cron job overnight and store the results in a serialized gzipped array which is processed upon each pageload to sort teams by states for displaying and actually calculate the RPI (add WP, OWP and OOWP together) which works well for speed for displaying purposes. I'm only concerned with trying to speed up this calculation: