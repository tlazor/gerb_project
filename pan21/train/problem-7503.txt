Currently the use of "KB", "MB", etc to mean anything other than 1000 bytes, 1000 x 1000 bytes, etc is deprecated
For some additional reading, you might peruse the Timeline of binary prefixes. It won't make things any clearer, but it is a fun read.
And the confusion continues. For practical purposes, you should probably assume that disk capacities are based on base 10 units of measure. Similarly, RAM capacities will (probably) always be based on base 2 units. For other kinds of devices and products, it'll probably depend upon the underlying technology and what the advertising folks think they can get away with. After all, bigger is better, isn't it?
Basically, it's just down to the fact that the computer and the storage device usually work slightly differently, so there are different definitions for the same thing.
Yes, as it's a syntactical disaster.  This Wiki article explains the issue and include a table of the "new" words to be used.  Here in the real world no one uses those terms.  You have to use context to determine which definition you are looking at.
and contrary to most official standards. The new way of expressing 1024 bytes, 1024 x 1024 bytes, etc is KiB (kibibyte),
In base 10, however, you can actually have 1000. Disk drives work like this and on an HDD, usually 1MB will be 1000000 bytes (though solid state works in binary).
Basically, you define the "thousand" which is either 1000 or 1024 and use that for the multiplications. They don't get swapped around.
Where the confusion was introduced, and what drives people crazy, is when disk drive manufacturers, in their quest to offer bigger and bigger drives said, "Hey! You know, in other areas (not computers), a K is 1000, and an M is 1,000,000." If a couple of guys were discussing the selling price of a car, for example, and one suggested, "I think '8K' sounds about right," the other would automatically understand that '8K' meant $8,000, (not $8,192). So, those disk drive manufacturers said, "Let's start advertising our products using those measures so they'll sound bigger." But the legal guys stepped in and said, "Wait a minute here. Virtually everyone who, you know, actually works with computers and understands how they work is going to cry foul." To which those clever advertising guys replied, "No problem. Well just put a tiny little asterisk next to the capacity on the box and in our documentation that says something like '* 1GB = 1,000,000,000 bytes'. Problem solved."
So here we are, with all of this confusion. Quoting from the wiki article above, referenced by @kreemoweet,
So, here's the deal. (Most of) today's computers operate in base 2, not base 10. (Yes, I know there are exceptions, but they really are exceptional cases.) For our (your) purposes, all general purpose computers (and certainly, all consumer use general purpose computers) use base 2 for all internal processing. The fundamental unit is a bit which can be zero or one. Eight of these together make a byte; 16 a word. Now, this is generally speaking... 32-bit computers are generally thought of as using 32-bit "words", 64-bit computers, 64-bit words, and so on. Have you noticed? These are all powers of 2. A computer with one "K" or kilobyte of memory will always have 1024 bytes of memory. One megabyte of memory will always mean 1024 x 1024 bytes. Again, all are powers of 2. So a computer with 64 MB of memory will always have 64 x 1024 x 1024 bytes. When you see computers advertised for sale, the memory capacity (RAM) will always be based on powers of 2. You'll never see a video card, for example, claiming 1GB of RAM to mean one billion bytes.
In base two (binary), the closest you can get to 1000 cleanly is 1024 (2^10). This is how computers actually think about things, so in memory (RAM), for example, 1MB would be 1048576 bytes.