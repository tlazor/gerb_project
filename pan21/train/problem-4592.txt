First, use a systematic grid search instead of just guessing the parameters. This is a very good practice which I picked up very late. If you are using a soft margin SVM ( which you should) with the gaussian kernel, you will have just two parameters to search over, C and gamma. This will make sure that you are extracting the best parameters from your classifier. 
Second, you have 13.5K samples and only 450 are spam. Which means you have 13 K desired mails. So you have 450 positive samples and 13K negative samples. If this is indeed the data distribution, you have an unbalanced data. You will have to weight the data according to the proportion otherwise SVM will end up classifying most of the samples as negative. To read about this in more detail look at Section 7 of this document . Weighted SVMs are implemented in LibSVM, so you can directly use that. 
Yes, you are essentially on the right track. You can normalize the data , if your text samples have varied length. Also, you can use categorical data but not in the same feature. For example, having one feature which is labelled "1" for facebook and "2" for instagram is not good. Have separate features for these kind of categorical data with {0,1} labels. There are two additional inputs that I would like to give here. 