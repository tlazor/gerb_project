1.) Jumbos -might- help if you're seeing a lot of processor load for interrupt traffic but if TCP is operating correctly it should be able to ramp well past 2G on a 10G link.  I've seen plenty of 10GE links running above 90% without jumbos enabled.
3.) I'm not particularly familiar with the Procurve gear but TCP traffic can be tricky at high speeds if there are any questions about buffer availability. I've seen other testing where this has manifested (without apparent TCP drops) as a huge cut in performance that ended up being fixed by actually reducing buffer sizes.  
4.) Make sure that the actual TCP settings (1323, SACK, etc) are all configured consistently.  The operating systems in question should be fine out of the box but I don't know much about the storage node.  It might be worth digging into - either in terms of settings on the device or via a protocol trace (wireshark or tcpdump) to observe window sizing and any retransmissions going on.
2.) If you do use jumbos, enable the same size on every NIC and every switchport in the VLAN and/or broadcast domain.  PMTU works when packets cross routers and mixing MTU values within the same network will lead to nothing but misery.
5.) Try eliminating as many variables as you can - even getting down to a crossover cable between one of your storage nodes and a single workstation - to further isolate the issue.  Don't be afraid to disable some of the offloads you mentioned as well, as they've been known to cause issues from time to time.