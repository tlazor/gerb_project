Once you are armed with that information you can go about deciding what to do.  If the the system is write bound you may need to partition your data.  If you are read bound, there are things that can help.  Indexing tables properly can give a significant boost to read performance in many cases.  If you have a lot of identical queries adding a cache may help.  Adding read slaves can also improve performance.  Caching and adding slaves will increase performance but will add some additional complexity to your network and application.
I would be careful with running MySQL Proxy on a production system. I have found it to lock up occasionally. Additionally you have a further piece of infrastructure you need to monitor.
SHOW FULL PROCESSLIST provides a list of all queries and tasks the MySQL is currently doing.  It needs to be run as a user with either 'PROCESS' or 'SUPER' priveleges in the database.  mysqladmin has a processlist argument as well.
Both of these will tell you what commands are running, where they are coming from and how long MySQL has been trying to service them so far.
Have you tried logging into the mysql service and issuing a SHOW FULL PROCESSLIST or using mytop to see which queries are running?
Once that is done, you can use mysqlbinlog to parse the slow logs (it has some great fields to order the queries/normalize the queries) or even use something more advanced like a profiler to find the slower queries
Another method that's not so complicated but may work is to set your slow-query timeout to a very low value, and then make sure you turn the slow query log on. That way you'll see anything that takes longer than a few seconds. This won't help you if the developers (or their ORM) makes a bunch of basic queries and then summarizes the data in the application, of course. 
First step is to not have all your processes use the same user. Especially if all of them are coming through 'localhost'. Even for queries coming from the same tool (as in php), have the ones in cron use a separate user from the ones running through apache. 
Yes you can, but profiling without using a client-based profiler requires that you pass the requests through something that can catch them -- In this case, that's going to be MySql-Proxy. There's instructions on their website on how to set it up to snag and profile queries, and then you can run explains and other operations against the ones that seem to be spending a long time. 
We mark queries with comments, as mibus suggested above. We then regularly write the FULL PROCESSLIST output to files. When we have performance problems, we import these files into the database and munge the data to see long queries. For us at least, these cause the most load.
mytop is very good for showing what the server is doing on an ongoing basis.  Think of it as something like top for MySQL.  