I'm running a python script to do some statistics and the actually memory which used is low,about 10%.And no other process cost more memory.However,when i use free -m and it shows that almost 95% memory has been used.The point is that my script should do a lot of read from files,so i wonder if there's any mechanism of Linux memory cache that caused the problem?echo 1 >> /proc/sys/vm/drop_caches works,but it seems manually.How can i reduce the memory cost and doesn't make a bad effect on reading files?
Linux tries to use your memory as efficiently as possible. If you've got tons of free memory, and you're reading lots of files, it'll use the free memory to cache those files. If you've got tons of active memory, it'll use a lot less memory for caching. That's the way it's supposed to work. If you don't want the memory to be used at all, take the chips out of your computer.
The Linux kernel automatically caches files in memory for efficiency. This is not a bad thing. When running free -m you will see on the right the amount of memory used in the cache. If an application needs the memory the kernel will free some of the cache. You should not try and manage it yourself.
There are some uncommon cases where lots of caching could be bad. Maybe the burst of reading always happens right before a burst of memory allocation. Maybe something about your specific case means there's no possible benefit, and you need to squeeze out that last 0.1% performance cost. Maybe you're on an embedded system that uses RAM with a limited lifetime. If you have such a case, tell us what it is.