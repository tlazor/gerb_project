$$ = \sum_{\mathcal{R_m}}^{|T|} N_m \left( \frac{ C_m }{N} \log p_m + \frac{ N - C_m }{N} \log (1 - p_m) \right)$$
Given data $(x_1,y_1), ..., (x_N, y_N)$, with $y_N$ a categorial variable over $K$ classes, we can model the conditional probability $p_k(x)$ for class $k$, where it satisfies $\sum_{k=1}^K p_k(x) = 1$ for each $x$. Then the sum
$$ =  \sum_{\mathcal{R_m}}^{|T|} N_m \left( \frac{ \#\text{$\{y = 1\}$ in $\mathcal{R}_m$} }{N} \log p_m + \frac{ \#\text{$\{y = 0\}$ in $\mathcal{R}_m$} }{N} \log (1 - p_m) \right)$$
is the (conditional) log-likelihood, and also the cross entropy between $p$ and the "one-hot" distribution $q$ that has $P(Y = k | X) = 1$. Logistic regression has the same equation, except there we model $\log p_k(x_i)$ via a log-linear model. 
which is just the entropy of $\hat{p}_m$. Since $C_m$ depends on the split points and the parameters chosen in the tree, so does $\hat{p}_m$. 
where $C_m$ is the number of times $y = 1$ in $\mathcal{R}_m$. Taking a derivative and setting equal to zero shows that the MLE is actually $\hat{p}_m = C_m / N$, and so this is 
$$\frac{1}{N} \sum_{i = 1}^N  \sum_{\mathcal{R}_m}^{|T|} N_m \Big( y_i \log p_{m} + (1 - y_i) \log(1 - p_{m}) \Big)$$
$$\frac{1}{N}\sum_{i = 1}^N \sum_{k = 1}^K \mathbf{1}\{y_i = k\}\log p_k(x_i) = \frac{1}{N} \sum_{i = 1}^N \sum_{k = 1}^K q_k(x_i) \log p_k(x_i) $$
$$ = \sum_{\mathcal{R_m}}^{|T|} N_m \left( \hat{p}_m \log \hat{p}_m+ (1 - \hat{p}_m) \log (1 - \hat{p}_m) \right)$$
In the classification tree setting, for a binary tree with $|T|$ nodes corresponding to regions $\mathcal{R}_1, ..., \mathcal{R}_{|T|}$, and where the $m$th region contains $N_m$ points, we model $p_k(x_i)$ as a constant in each region: 