High level concepts are abstract and difficult to grasp. Low level concepts may appeal to the strong math student, but can be a road block to the average student. I see value in focusing on code that students can write and run and see  results. This is more likely to hook them and encourage the students to want to learn more.
However, we cannot avoid the low level concepts and CS is all about layers of abstraction, so we cannot honestly give students an introduction to this field without hitting bits, bytes, and abstraction. Students in an intro class should see the practical value of understanding bits and bytes. These are terms used in consumer products and data plans; my students appreciate learning what a megabyte is and how much information it can store. I don't get many "when will I ever need this?" questions.
In an introductory course, start with the practical stuff, and build up or down as needed. All the theory in the world will do you no good if you cannot apply it, and while bits, bytes, and logic gates are all very well and good, it is hard to see the forest for the trees that far down. History can also help provide context and motivation, so weave that in with the practical stuff ('this method was discovered/invented by so-and-so') but remember, again, the practical stuff is your meat and potatoes.