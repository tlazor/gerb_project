I have a Ryzen 1700X (got it on launch), running on my main Linux machine, with other components that are on par with it. During the day when the computer is on and I'm not using it, I like to run BOINC client on it, which maximizes all 16 threads of the Ryzen to 100% without a single one falling back, and it can maintain that for hours. My question is, if I were to run the computer like this, ignoring the electrical bill and the failiure of the PSU (which isn't yet to come, my 1050w 80 gold is running very smoothly), how long can I expect the CPU to run for before significant damage is done to the silicone. I haven't overlocked it, not even to the "AMD guaranteed +400M Hz (OC on Ryzen seems unstable), and its cooled by Corsair Hydro H60, which is acceptable, but I have many fans setup which never let the temperature rise above 56 C when the CPU is at 100%.
Hard drives will spin up and spin down, resulting in mechanical stress on the motors and thermal stress as the drive power controller warms up and cools down.
Damage to the silicon in the processor is unlikely to be any kind of failure mode you will see, the voltages are low and the paths are well designed. It is only in NAND Flash memory devices that you are likely to see a failure due to silicon insulation breakdown, but then that is because they are intentionally driven to cause a non-catastrophic breakdown (but it eventually will be catastrophic).
BGA packages combined with lead-free soldering - those are a pain, especially in applications where the BGA packaged chip runs hot (the heatsink is small, or big enough but not properly thermocoupled to the chip) and where the temperature keeps changing up and down.
The only problem i see with CPU running for years is, that heat sink will fill with dirt and clog. and won't be able to cool it properly. As long as you keep heat sink in proper working conditions CPU will be fine.
The problem with system reliability is actually all the other components in the system. The CPU will undergo thermal heating and cooling cycles as the load changes and as you turn the system on and off. The same is true for pretty much every component in the machine. The CPU silicon die is connected to pads or pins on the CPU by tiny bits of wire which will be affected.
Heatsinking is generally the part where computer makers often cheat - and it's difficult to tell negligence from planned obsolescence. With sloppy thermocoupling of the CPU to the heatsink (especially in passive/fanless computers) it's not the CPU that dies - it's stuff around the CPU. VRM elyts used to be a classic, then there came a period when they were a non-issue (when each aluminum wet elyt was replaced by a comparable solid-poly), then the board/computer makers pressed on and reduced the number of solid-poly capacitors in the VRM, or made the VRM all-ceramic but running extremely hot, or some such... Modern ceramic capacitors (MLCC's) are no longer immortal, and the dependency of MLCC lifetime on operating voltage and temperature climbs along 2nd or 3rd power (some say even higher). Another rule of thumb is, that every 10*C down in temperature mean twice the lifetime.
You are more likely to see problems due to thermal or mechanical stress, as temperatures change and components move very slightly, than you are to see any failure within the actual CPU silicon.
In the 14 years of assembling and selling industrial PC's, I've seen one or two CPU's dead (out of thousands sold). But, everything around the CPU can be a different story.
My ancient DIY/HAM sense still seems to be valid: if you can keep your finger on it, and it doesn't smell, it has some chance. I like designs where the heatsinks only ever gets lukewarm (and where I know that the heat source inside, typically the CPU, is well thermocoupled).