The setup:  8 disks in a mdadm-managed RAID5 array (/dev/md0, made from /dev/sdc through /dev/sdj).  One disk (/dev/sdh) is showing SMART errors (increasing pending sector count) so I'm looking to replace it.  Additionally, the machine boots from a Revodrive SSD in a PCIe slot that's configured with a RAID0 stripe.
I'm guessing the disk superblocks are screwy, despite everything being functional.  I'd like to get this fixed before proceeding with the replacement of the suspect disk, as I'm a little concerned about how the disks might behave if I failed a drive.  What's the best way for me to proceed?
Obviously, there's a metadata format error in the first line, from an auto-generated metadata flag in mdadm.conf, but this is mdadm v2.6.7.1 running on Ubuntu, and I've chalked it down to a known issue
The proc table only shows the mdadm managed array of SATA drives, not the revodrive, which I'd expect as the revodrive  RAID should be managed by its own hardware controller.
The oddness:  mdadm --detail output shows the array as clean, and everything looks to be running well (I can mount, read, write the array without problems).  mdadm --examine output for every disk shows an array state of failed.
But in the --examine output, the Array state is failed.  Each disk seems to show itself as the failed member - /dev/sdd shows uUuuuuuu, /dev/sde shows uuUuuuuu, etc - but all show the mystery 9th "failed" slot between slots 6 and 7 on the previous line.