I have a biggish-data application where multiple (up to 10) process may be doing sequential writing and reading of files/objects concurrently. Using a single RAID volume, they would all be writing to the same filesystem and perhaps having some contention on the same RAID system and filesystem. The poor RAID controller may get too busy and not be as fast as I want it to be. On the other hand, using /mnt/{a..g}, I can introduce some sharding logic on the application layer so that based on the name of the 'object' being written, one of /mnt/{a..g} is selected for storing that object so that the processes don't end up all writing to the same RAID and filesystem, perhaps avoid performance problems related to RAID or filesystem contention.
I was previously under the impression that the sequential throughput of a RAID5 scales somewhat linearly with the number of disks up to some not-too-small number but my recent experience tells me that the reality is not even close. (On 7-disk RAID5 ext4, I'm getting only 160MB/s write 320MB/s read.) Hence, I'm thinking of alternatives for maximizing total sequential disk throughput. Am I likely to get better total throughput for my 10 processes if I mount the 7 disks separately and use the 7 filesystems separately concurrently?
I'm trying to get maximum sequential disk throughput for an application on my Linux server. It has 7 SATA disks that I could put in one RAID0 or RAID5 using a hardware RAID controller (HP P420i). Or, I could use them separately and put a filesystem on each one separately and mount them under /mnt/a, /mnt/b, ... /mnt/g. (The RAID/SATA controller could assign each disk to its own 1-disk volume.)