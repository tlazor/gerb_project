It's an awesome concept and can really help with debugging issues. It's free for up to 500MB a day of logs:
You should consider replacing the stock system sylog with something like syslog-ng for better performance and the ability to filter logs as they move through the pipeline. syslog-ng also supports blocking pipes so you can redirect an existing program's output directly into syslog-ng without modifying the program and without losing data.
Of course there are commercial tools to do this sort of thing as well.  One of the most popular is Splunk which is free to try and for limited amounts of data.  Splunk comes with a client that you can  run on multiple servers to save you the step of getting all your logs into a central syslog server.  If you have more money that developers, something like Splunk might be worth considering.
Logwatch is a good solution, but you are still dealing with lots of emails.  I favor feeding everything into syslog and then collecting those syslogs on a central logging machine.  You can then do various sorts of processing and event correlation on the logs all in one place.
Once you get you logs in one place, you can set up tools like Simple Event Correlator to find patterns.  You can also run tools like logstash to save logs in a database and enable more powerful queries and graphing.
First, how to get your application logs into syslog?  There are a few ways. For the simplest case you can call logger in shell scripts to create syslog messages.  If you are running perl scripts, you can retrofit them to use Log4Perl to redirect logs into syslog.  There are similar approaches available for other languages.
Finally, here's a nice central logging mini-howto that covers a lot of the same ground as I just did.
There are just too many places that I need to check the logs and mails for stuff that might have potentially went wrong. My question is, what is the best way to do this or even better is like a log parser application which will go through all the system logs when something really goes wrong instead of me having to go through it daily?
Another solution if you have the resources is SPLUNK. You create a SPLUNK server on your network ship all the logs from all your servers and all your apps to it. It indexes the logs and syncs them against a timeline.
I run a few applications which creates their own logs. Then I run cron scripts on the same server to do importing of data for my app. When these cron errors out, the default is it sends emails to the user that runs the cron job. 