Perhaps come back to us with more details of your existing environments and budget and we could help you to narrow down what type to go for.
As a specific example of basic load balancing when I moved from 1 to 2 application servers I got a another server setup to run Squid which uses round-robin load-balancing on the 2 servers in addition to adding a caching layer. Once Squid was setup and working it was a simple matter to change the DNS entries for the site to point from the original application to the new Squid server.
In my case Squid was an obvious choice as it added load-balancing and caching and was already supported by the application (MediaWiki) but there are many other software choices. Most web servers (Apache, lighttpd, etc...) can be setup as a simple load balancer though usually a faster serer like nginx is preferred. Then there caching apps like Squid/Varnish and finally more specialized apps like HAProxy, Finally, there are also hardware load balancers but these will be typically more expensive and used in higher traffic/high availability setups.
I'm going to release a website soon, and was thinking of having one database server, and one front-end server (so no load balancing needed yet). But if I add more front-end servers, how would I load balance them so that there is an equal amount of users on each site?
It may sound obvious but you'd use a load-balancer. There are lots of types but essentially you create a virtual IP (VIP) for your service and have it alternate the incoming requests across any number of servers 'behind' it, each with their own IPs. There are lots of mechanisms for deciding which server gets which requests from the most basic 'round-robin' method which simply hands out a request then moves onto the next server in a circular fashion, to very complex ones that look inside the servers to decide which are working and least busy.