The actual numbers are of course hard to prove; the Wikipedia article references this study which speculates at the size as being around 91,000 TB.
Between those two methods I think I can see where they are getting their numbers from, but I don't see how that is "1,000 to 2,000 times better than that of the surface Web"
Recently Google announced that they're working to improve the way that their search bots 'read' web pages, which implies they're trying to index more of this hard-to-get-to content.  Engadget has a writeup on it here.
Almost all of those points are pure marketing and I don't beleve them, but there could me some small truth behind them. What is likely happening is two things: 
As I understand it, 'Deep Web' refers to the portion of the web that isn't easily indexed in an automatic manner; for example, many pages are dynamically generated on the fly in response to a query, or a form being filled in.  Or, some sites require member login before all the content is available.  These kinds of sites are not generally transparent to automated search robots, and so the only part of the site that ends up being indexed is the 'surface' portion.