My RMAN script backs up the control file first, then all of the data files, then the archive log files. If you know when time the data files are done getting backed up and added 20 minutes for cushion, you could use that as a set until time. Hence, you would have the least amount of redo being applied. Ideally you have the archive logs available to the cloned database, so that you can roll forward the changes that are needed. Do you know how many archive logs are creating during the backup of the data files?
Unless you have a very unusual application, the amount of Redo Logs to be re-applied will be minimal compared to the size of the Data Files you are restoring. You're not going to reduce the overall restore time by much at all. 
I have a weekly script that automates a duplicate from prod backup.  WHen I inherited it, it did a duplicate until time and was highly dependent on jobs completing on schedule so it could assume a good 'until time'.  I modified it to get a good scn and so duplicate 'until scn'.  I get the scn with this snippet of code querying the rman catalog, in a shell script.  If you are on windows, you'll need to adjust scripting technique, but principle is the same: