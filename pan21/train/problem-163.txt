I am not sure if most answers consider the fact that splitting categorical variables is quite complex. Consider a predictor/feature that has "q" possible values, then there are ~ 2^q possible splits and for each split we can compute a gini index or any other form of metric. It is conceptually easier to say that "every split is performed greedily based on metric (MSE for continuous and e.g. gini index for categorical)" but it is important to addess the fact that number of possible splits for a given feature are exponential in the number of categories. It is correct observation that CART handles it without exponential complexity, but the algorithm it uses to do so is highly non-trivial, and one should acknowledge the difficulty of the task. 
If you really know the definition of Gini Index, you wouldn't have posted this question. You should not be worried about type of variable. It's to do with homogeneity of splitted classes.   
Second methodology is to convert it to categorical attribute and make rules like this if a<100 and if a<100
But for Continuous Variable it uses a probability distribution like Gaussian Distribution or Multinomial Distribution to discriminate.
When using the Decision Tree, What decision tree does is this that for categorical attributes it uses the gini index, information gain etc.