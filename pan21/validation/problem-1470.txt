My big VMWare deployments are NFS and iSCSI over 10GbE. That means dual-port 10GbE HBA's in the servers, as well as the storage head. I'm a fan of ZFS-based storage for this. In my case it's wrapped around commercial NexentaStor, but some choose to roll their own.
Currently you can get 40gbe switchs far cheaper (thats a strange thought) then 10gbe switches but I doubt you're blade center will support that.
but if the bladecenter can support qdr infiniband and you can afford native infiniband then thats the solution you should pick.  
The key features of ZFS-based storage in this context would be the ARC/L2ARC caching functionality, allowing you to tier storage. The most active data would find its way in RAM and SSD storage as a second tier. Running your main storage pool off of 10k or 15k SAS drives would also be beneficial.
This is another case of profiling and understanding your workload. Work with someone who can analyze your storage patterns and help you plan. On the ZFS/NexentaStor side, I like PogoStorage. Without that type of insight, the transport method (FC, FCoE, iSCSI, NFS) may not matter. Do you have any monitoring of your existing infrastructure? What does I/O activity look like now?
If you want iscsi or nfs then minimally you'll want a few 10/40gb ports or infiniband which is the cheapest option by far but native storage solutions for infiniband seem to be limited. The issue will be the module for the bladecenter what are its options, usually 8gb fc or 10\1gbe and maybe infiniband.  Note that infiniband can be used with nfs and nothing comes closed to it in terms of performance\price.  if the blade center supports qdr infiniband i'd do that with a linux host of some kind with an qdr infiniband tca via nfs.  Here's a good link describing this http://www.zfsbuild.com/2010/04/15/why-we-chose-infiniband-instead-of-10gige