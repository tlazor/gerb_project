If you're transferring very large files with lots of changes, use the --inplace and --whole-file options, I use these for my 2Gb VM images and it helped a lot (mainly as the rsync protocol wasn't doing much with passing incremental data with these files). i don;t recommend these options for most cases though.
Another strategy is to make ssh and rsync faster.  If you are going over a trusted network(read: private), then encrypting the actual payload is not necessary.  You can use HPN ssh.  This version of ssh only encrypts authentication.  Also, rsync version 3 starts transfering files while building the file list.  This of course is a huge time savings over rsync version 2.  I don't know if that's what you were looking for, but I hope it helps.  Also, rsync does support multicasting in some way, though I will not pretend to understand how.
If you keep a local copy of the last rsynced state (i.e. a copy of what the mirrors look like right now) on the same machine as the master, you can generate this "patch" on the master without even contacting any mirror:
Transfer the my-batch.rsync file from the master to all of your mirrors, and then on the mirrors, apply the patch so to speak:
It requires that you invoke rsync with the master and mirror with --write-batch; it produces a file.  You then transfer this file to any number of other targets, and you then apply the batch to each of those targets using --read-batch.
Presuming that the data you're rsyncing isn't already compressed, turning on compression (-z) will likely help transfer speed, at the cost of some CPU on either end.
rsync has a way of doing disconnected copies. In other words, rsync can (conceptually) diff a directory tree and produce a patch file which you then later can apply on  any number of files that are identical to the original source.