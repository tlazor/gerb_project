It starts by regression the labels on each feature individually, and then observing which feature improved the model the most using the F-statistic.  Then it incorporates the winning feature into the model.  Then it iterates through the remaining features to find the next feature which improves the model the most, again using the F-statistic or F test.  It does this until there are K features in the model.
Sklearn DOES have a forward selection algorithm, although it isn't called that in scikit-learn.  The feature selection method called F_regression in scikit-learn will sequentially include features that improve the model the most, until there are K features in the model (K is an input).  
Notice that the remaining features that are correlated to features incorporated into the model will probably not be selected, since they do not correlate with the residuals (although they might correlate well with the labels).  This helps guard against multi-collinearity.
No, sklearn doesn't seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the documentation here:
I'm working on the problem with too many features and training my models takes way too long. I implemented forward selection algorithm to choose features.