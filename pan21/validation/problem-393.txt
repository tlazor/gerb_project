I'm using Mellanox ConnectX-2 cards and 10GBASE-LR transceivers. Have jumbo frames, and everything else enabled, and I do get higher speeds if I'm not using that specific iSCSI drive. The server does not seem to struggle when I'm copying, I mean it has an 8-core Opteron and 16GB ram, so that should be enough.
I'm using Starwinds in our production environment, you can take a look at it. It also has a free version. HPE VSA are also good, but they're configured in virtual machines, so that's a little "-" to the performance.
The regular MSFT iSCSI target is very slow, that's a proven fact unfortunately. There are plenty other programs that have it's own iSCSI target. 
I have set up an iSCSI share on the Win Server 2016 machine on a RAID0 array that has r/w speeds around 680/670MBps. The problem is that I've got a dynamic vhdx file on that array, connected to a single client, and when I'm trying to read or write on that drive, it tops out at 350MBps. Otherwise, when I'm reading directly from the array over the 10G connection, the speed goes up to 520MBps, which is the maximum speed of the SSD in the Win10 client.
iSCSI is doing wire speed even on 40 GbE if done right. Make sure you a) don't have storage as a bottle neck - same DiskSPD test run locally shouldn't pop up with the similar numbers, you should get x2-x3 more IOPS/bandwidth b) TCP itself shouldn't be a bottleneck - iPerf & NTtcp should be able to show 90%+ of what your network connection can do, c) CPU should't be a bottle neck - no single core should show 100% time while others are lazy - use PerfMon for that, and d) iSCSI should be configured properly.