I've got an nginx configuration for a Laravel 4 deployment. It works well, but I'm trying to make a change to prevent Twitterbot from accessing anything other than the robots.txt file (they are spamming us with traffic to update share cards, and apparently caching the robots.txt file for far too long). The issue is that since Laravel routes everything through index.php, including the robots.txt file (built in a Blade template), I'm not sure how to match the Laravel route for that file. Here's what I'm working with. Blocked just the less important bits for privacy:
The correct URI for robots.txt is /robots.txt. Your approach seems perfectly valid, but you will need to hardwire the fastcgi_param for SCRIPT_FILENAME to point to /index.php. For example:
What location do I need to put in to get it to bypass the Twitterbot check? Is there a better way to do this? I know if's are bad, but I think this is a case where I don't have a choice. I'd gladly be proven wrong. 
Also, return 403 is one of the few things that you are allowed to do in an if block. See this document for details.