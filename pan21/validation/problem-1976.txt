A different solution is to grab the signal processing toolbox: The artifacts seen in your image are essentially aliasing, caused by insufficient sampling of the wave heightfield by the projected grid. Therefore, one solution is to filter the heightfield appropriately, depending on the projected area of a grid cell. I believe this is used in offline rendering of oceans, and it essentially makes sure that waves at the horizon go flat. However, I'm not sure how feasible this is in real-time rendering, since you would need high-quality anisotropic filtering to make this approach look reasonable.
I believe a common solution is to split the camera transform used to project the grid from the camera transform that is used to render the grid. At perspectives close to top-down, the two cameras coincide, but as the viewing camera gets close to a horizontal perspective, the projection camera deviates and tries to keep a minimum inclination, i.e. it hovers somewhere above the view camera and looks down slightly.
The tricky bit is making sure that the field of view of the projection camera always covers the region of the scene seen from the render camera. I don't have a resource at hand that details how to compute the appropriate transforms, and it might be tedious to derive by hand.