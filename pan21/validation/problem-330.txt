as for rsync, I believe it skips zeros similarly to the way tar does with that option, so it's transmitting less data than NFS.
Maybe it's not slower transfer speed, but increased write latency. Try mounting the NFS share async instead of sync and see if that closes the speed gap. When you rsync over ssh, the remote rsync process writes asynchronously (quickly).  But when writing to the synchronously mounted nfs share, the writes aren't confirmed immediately: the NFS server waits until they've hit disk (or more likely the controller cache) before sending confirmation to the NFS client that the write was successful.
Do you have file locking setup on the nfsshare? You might get a lot more preformance if that was disabled.
with netcat, as with scp, you have to send all the source data all the time, you can't be selective as with rsync so it's not suitable for incremental backups or that sort of thing, but it's good for copying data or archiving.
SOURCE: tar cvif - /path/to/source | pixz -2 -p12 | nc DESTINATION PORTNUM# tar, ignore zeros, level 2 pixz compression using 12 cpu cores
Alternatively, if you have highly compressible files / data, fast CPUs, and a network that doesn't have quite the speed of the CPU(*), you could get speedup just from implicit compression over the ssh link.
if you tune the compression level and cores right, depending on your data set, you should be able to keep the network close to saturated and do enough compression you're bottleneck becomes the disk (usually the write side if read and write disk systems are the same).
If you have scads of little files (e.g. emails in individual files), NFS efficiency may be tanking due to not making use of the full MTU (maybe this is less likely with TCP over UDP though).
netcat has basically none.. it'll send full TCP packets which contain nothing but data you care about.
If 'async' fixes your problem, be aware that if something happens to the NFS server mid-write you very well might end up with inconsistent data on disk.  As long as this NFS mount isn't the primary storage for this (or any other) data, you'll probably be fine. Of course you'd be in the same boat if you pulled the plug on the nfs server during/after rsync-over-ssh  ran (e.g. rsync returns having 'finished', nfs server crashes, uncommitted data in the write cache is now lost leaving inconsistent data on disk).
Although not an issue with your test (rsyncing new data), do be aware that rsync over ssh can make significant CPU and IO demands on remote server before a single byte is transfered while it calculating checksums and generating the list of files that need to be updated.
NFS can't make assumptions about the data so has to transmit every byte along with the NFS protocol overhead.
(*) In this case by 'speed', I'm referring to the rate at which the CPU can compress data compared to the rate the network can transmit data, e.g. it takes 5 seconds to send 5MB across the wire, but the CPU can compress that 5MB into 1MB in 1 second.  In this case your transmit time of compressed data would be slightly over 1 second, whereas uncompressed data is 5 seconds.
I am a fan of pixz .. parallel xz,  it offers great compression and I can tune the number of cpu's I have to the network bandwidth.  if I have slower bandwidth I'll use higher compression so I'm waiting on cpu more than network.. if I have a fast network I'll use a very low compression:
if you're goal is to just copy all files from one place to another, then tar/netcat will be the fastest option.
This is interesting.  A possibility that you may not have considered is the content / type of file you are transmitting.
A third possibility is that the files (or one version thereof) already exist in the destination.  In this case the speedup would be because the rsync protocol saves you transferring the files.