Well, if the 7 gigabit guy was a modern 3 spacial stream 802.11n client getting the top-end 450 megabit per second signaling rate, he used up just one quarter of the airtime in that 60-second sample.
Imagine you sample for 60 seconds and see that in that time, one client transferred almost 7 gigabits, and another client transferred just 45 megabits. Who's the hog? 
If the other guy was far from the AP, getting only the lowest 1 megabit per second signaling rate, he may have taken 45 seconds of airtime transfer his puny 45 megabits.
Without looking at signaling rates, it's easy to look at the 7 gigabit guy and think he's the hog, when actually he only used 1/4 of the time, whereas the little 45 megabit guy used 3/4 of the time.
Beware that with wireless you can't tell who the hog is just by looking at amount of data transferred. You have to look at the signaling rates used as well.
This is an extreme example and I've oversimplified it by leaving out protocol overhead and retransmissions, but the point stands. Whatever solution you choose, make sure it accounts for airtime used, not just data transferred.