Whenever I look at SQL I'm always looking at the same patterns for serious performance bottlenecks - nested queries, joins, and where clauses. Roughly in that order.
From comparing execution plans and client statistics in SQL Management Studio, I have not sped the Stored Procedures up so I am apprehensive about suggesting wholesale re-writes of all Stored Procedures.
And I don't think it's true that the dynamic sql query gets reevaluated each time. As far as I remember, the DBMS will cache the query plan based on query text. But if you modify even one space in it, it will get reevaluated. So, to optimize the dynamic query it would definitely be better if you used variables in the text (@myValue) instead of concatenating their values (you pass the variables values when calling the exec).
You don't have any nested queries, but you do have joins and where clauses. Without seeing you real database it's extremely difficult to identify problems but the simplest change you can make is index optimization.
Basically I suggest, what a commenter already wrote. Write a statement for each IF-condition. Not that much work:
I have recently inherited a codebase with a large amount of Stored Procedures.  The system they are supporting is encountering numerous performance problems which I am looking in to.
Now there is no dynamic stuff. Also consider an indexes on columns of the where clauses, the select could potentially run faster. Should you also do updates depending on conditions you should go for a merge statement.
The problem is I have read that dynamic SQL is not always a closed case - i.e. it depends on how it is used.  My understanding of locking also falls down at the fact that nowhere can I get 100% confirmation of how this type of INSERT SELECT will lock tables.
Can anyone offer any confirmation of the theory behind my thoughts, or any better ways of proving my suspicions?
As for the temp table - it's a variable, which means it's a bit faster/less resource demanding than the actual temp table (e.g. #table). We had recently a similar problem where the server took long times to execute a complicated query and the optimization turned out to be to first trim down results and after that do the rest of the joins, which is exactly what they're doing here. When this turns out to be the issue, one first tries to do a statistics update for all tables, but if that doesn't do the trick, then this would actually be the solution. 
In support of this, you've written the code out to the generated conclusion and performance is roughly the same.
Dynamic SQL runs as fast as 'compiled' SQL in recent editions of SQL Server (for the purposes of us SQL layman at least) so simply don't worry about that until all other options have been explored. The main argument against dynamic SQL is maintainability (readability, testability, clarity, etc).
I am trying to set up some profiling of a live customer scenario, but as yet have been unable to prove my thoughts.