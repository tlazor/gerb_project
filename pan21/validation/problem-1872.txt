Hope one day documentation will be more clear for users like me (novice) and not only for (experts) advance people... it took me a while to discover that the chunk size was 1GiB for data... i was writting a small (<1KiB) file on a newly btrfs raid 1 (two devices) and wow, there were 2GiB less on free space (one on each device)... and i was thinking all my data was getting lost, since i write more files and free size did not change... there were all being writted on one whole chunk (really two chunks, one on each device)... till i understand there is a pre-allocation in units of 1GiB.
You can reduce metadata usage by rebalancing your metadata to single profile at the cost of chance of recovery which may be desirable depending on your use case. You do that like so:
Or if you have plenty of time just use 100 directly, that way all chunks will be rellocated and it will use as less chunks as possible.
I found that metadata has taken up to 1.25GiB while the real metadata size is only less than 100MiB. As you can see, the whole disk volume is 4.5GiB. Such waste of space is not acceptable.
I've tried to run btrfs balance start -m and btrfs balance start -musage=x where x varies from 0 to 50 but none of them helped.
First of all, BTRFS does allocate metadata (and data) one chunk at the time. Each chunk is 1GB. Even if a chunk of metadata is allocated it does not mean that this chunk is fully utilized. Keep in mind that BTRFS also stores smaller files in the metadata which may contribute to your "high" metadata usage.
Is there any way to force btrfs reduce the allocation size of metadata? Or, is there any way to resize a btrfs partition near the size it actually taked?
But since you are putting 50 as the value, you are accepting a 100%-50%=50% wasted space. If you put 75, then 100%-75%=25%, so only 25% wasted space. Ando so on.
The 50 indicates that you are allowing to have chunks with 100-50=50% non used space (wasted). If you put 60 you will tell you only wnat chunks with at most 40% wasted space, so chunks with more free space will be merged and freed.
Just use a bigger number, that number indicate how much percent of the chunk space must be used on each chunk, if a chunk has less usage than that percentage it will be merged with others into new chunk, freeing chunks.
Also it is worth mentioning that when you run balance with the usage filter what you are saying is, only balance chunks that have a utilization factor of less than X.
Putting 100 does not mean each chunk (neither each chunk less 1) will be 100% filled, but it means all chunks except one will be filled to the maximum possible, so it will make as much chunks as possible to be freed, at the cost of moving arround a lot of data / metadata, that is why all people recomend to try with a bigger value in low increments, to move as less data / metadata as possible until user is happy with wasted space.
By default BTRFS also duplicate metadata to increase the chance that your filesystem can recover in case of a corruption. Data is not duplicated.
If one chunk is not filled, it still takes 1GiB of space; so if you have two chunks at 75% filled, you are wasting 25% of two chunks of 1GiB, that is 250MiB on each chunk, so in total 500MiB, since i talk about RAID 1, it also happens the same on both devices so in total the waste is 1GiB of the 4GiB (2 chunks * 1GiB * 2 devices), that is a 25% wasted space.
Note: If someone knows how to force Btrfs to not use new chunks until actual chunks are filled, that would be great for me to know! I mean by not manually doing a balance
You can also look up mixed block groups which will make BTRFS not allocate separate metadata chunks, but store data and metadata in the same chunks.
This question is almost the same as btrfs: HUGE metadata allocated , except for the magnitude of data size on the partition, and a new version of linux kernel 4.4 .
If you want to minimize wasted space, use 99 or 100, etc., a high value; but be aware that implies a lot of moves because of CoW (Copy on Write), extra caution if using SSD / NVME / etc., also extra care on USB flash memories/cards/etc.