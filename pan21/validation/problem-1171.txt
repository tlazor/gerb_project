As far as scale - clearly, a number with a precision and scale of (8,0) can represent a number three orders of magnitude larger than one with a scale of (8,3) (12345678 > 12345.678). To my way of thinking, that is a valid definition of scale.
As noted in ypercube's Wikipedia link, precision is related to the concept of significant figures (or significant digits).
To the naive non-expert (=me) this is always a bit confusing. I would have expected it the other way round.
Example: If you have a scale that measures mass to the milligram, and you have a precision of eight digits (three to the right of the decimal point, just for argument's sake), you can say something has a mass of 12345.000 grams, 12345.333 grams, or 12344.555 grams. If, on the other hand, you only have a precision of five digits, all of these would be listed as 12345 grams - therefore, your representation is less precise.
Just to be clear: This is not a rant disguised as a question, and more than mere curiosity. I am convinced that there is a good reason for this terminology, and I think I can learn about databases by understanding the underlying thinking here. (If this question is considered too basic for here, I understand.)