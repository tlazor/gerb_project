For instance, in the cell probe model one can prove that the problem of maintaining prefix sums of word-length values (that is, maintain an array of values subject to operations that change one value in an array or that query the sum of a prefix of the values) requires $\Omega(n\log n)$ time to perform a sequence of $n$ operations on an array of length $n$. The difficulty here comes in part from the fact that the operations have to be performed online (not knowing in advance the sequence of future operations).
Fredman and Saks' STOC'89 paper "The cell probe complexity of dynamic data structures" and Patrascu and Demaine's SICOMP'05 "Logarithmic lower bounds in the cell-probe model" are probably good starting points; look up cell probe lower bounds in Google scholar for much more.
One area where unconditional and nontrivial time lower bounds are known is in data structures, where the time is for individual data structure operations (or sequences of operations). The standard model for this sort of thing is called the "cell probe model"; it assumes only that main memory is divided into words of a certain size and that the CPU has a very limited amount of local storage (e.g. in registers or cache), and bounds the number of word read/write operations that must be performed.