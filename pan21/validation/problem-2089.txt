Your question is very broad and I can only answer part of it. Most importantly, there is no way to say this or that is always better. It depends on the method you work with and (often) also on the data. Let‘s consider two examples. 
2) Think about linear regression. You need to specify the functional form in your regression equation to capture the data generating process well. Let‘s say you have „age“ as a feature and this feature does not have a linear relation to your y. You may try a quadratic form or other things. However, if you generate age classes (say 10 year intervals), you can add this features as „dummies“ to your model and you don‘t need to worry too much about parameterization of „age“ as a feature (dummies work similar as regression splines in this case). 
So it really depends on the problem. In reality you need to try different representations, contingent on your model and the data. Also take Kaggle kernels too serious. They often provide good examples, but most of the kernels are really hands-on.
1) Think about neural nets. They often work well with data which has not too much variance. This is one reason why data is often scaled and/or normalized. Transforming continuous features to categorical can be helpful here.