There have been lots of papers over the years on different techniques for drawing height-field terrain in a ray-tracer. Some algorithms ray-march the grid directly (or via a quadtree); others transform the terrain into a polygon mesh and use a standard ray-triangle intersection test. The research seems to have moved on in the last few years, and it's hard to find papers written in the last decade, but the balance between memory and compute (both CPU and GPU) is still changing.
What kind of algorithm gives best performance on high-end desktop machines nowadays? Or if there isn't a single answer, how do the performance characteristics of the current best algorithms differ?
The basic idea is to skip a lot of space by having knowledge of the maximum value over large areas of terrain. If the ray stays above that, skip to the next large area. 
If you look at Figure 8, you'll see a comparison of basic linear stepping vs. maximum mipmaps. Linear stepping results in 200 steps, which can be done real-time on modern gpus but is still actually slow. Max mipmaps do the same in about 10 steps, all in shader.