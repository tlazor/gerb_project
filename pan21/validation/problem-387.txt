If there is an actual, observable problem occurring right now, that is the place to start: try to identify exactly under which circumstances it occurs, whether it is precisely reproducible, and guesstimate what the impact is on the business.
Additionally, if you have mod_logio enabled, you can get exact byte counts (after encryption/compression/everything) for incoming and outgoing bandwidth for each request using %I and %O respectively in your LogFormat string.
Traditionally (and also sanely!), you START by identifying an actual problem; then you progress to diagnosing its causes, and only THEN do you add hardware or change software configurations.
Then you can run the appropriate columns through whatever number-crunching you want to get totals, averages, identify particularly slow or abusive pages, or what-have-you.
Next up, start collecting performance data from your application stack; this may involve but is probably not limited to : OS resource utilization, web server metrics, PHP caching, databases, disk I/O and latency.
I would start with mod_log_config. Define one or more LogFormat/CustomLog setups in httpd.conf with just the stats you're interested in and any metadata about the requests you care to filter by and then you can quickly generate comparative stats from those log files. For example:
I always run collectl on all servers.  It samples tons of metrics every 10 seconds and slab/process data every minute and writes it all to a file you can playback OR plot with colplot (part of collectl-utils).  You can also run it in real-time at any sampling interval of your choice.
We are having performance issues on our server which may or may not be the inevitable result of high traffic to our sites.  We would like to optimize each site for performance so that we can be sure we're getting the most "bang for our buck" before purchasing additional hardware.
Based on what you see over several hours or even days, you can then begin to take the approach  very clearly stated by apaptr, of first figuring out if there even is a problem and then taking steps to correct it.  Having collectl data at each step in the process makes it very easy to tell if things have improved or gotten worse.
I was wondering if anyone knew of some creative ways to measure the resources (memory, bandwidth, CPU time, etc) consumed by each Web site.  This could allow us to spot the biggest "offenders" and start working on those first.
%t is the timestamp, %v is the virtual server hostname, %B is bytes sent (excluding headers), %D is microseconds elapsed, %h is client IP address, and %r is the first line of the actual HTTP request sent by the client. So you can leave in or out any of that other information, depending on what you're looking for, or have one log for each stat, or whatever you want. (I like using cronolog to rotate logs on a daily basis. Tack on -%H if you want hourly rotation.)