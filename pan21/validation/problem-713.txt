As you can see, the speed of a disk itself is but one of many factors. It's a largish factor, but still one of many. If that 1.5TB copy is all on the same disk, then the disk will (95% likely) be performing a 100% random read/write performance, which generally turns in the worst performance metrics. If the copy is from one disk to another, and the data is 100% sequential and the target disk is completely empty, this should turn in the fastest performance possible with this disk subsystem. Real world performance will be somewhere between these two extremes.
I have a storage array at work that can saturate 3Gb (gigaBIT) SAS channels when doing largely sequential operations. If I had 6Gb SAS it could probably get very close to saturating those too. For random I/O this particular system performs very differently based on what the OS is (OpenSolaris, for instance, had the worst random I/O, and Linux XFS the best by a factor of 3). 
There are many, many variables involved in these kinds of calculations. Real world disk systems have a lot of inter-dependencies. Just within a single computer: