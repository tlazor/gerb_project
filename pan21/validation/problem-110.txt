The SAN itself doesn't need an Ethernet connection in this case, all it needs is FC to the ESXi hosts. The ESXi hosts will need GigE Ethernet. During VMotion, the source ESXi starts sending machine-state to the target ESXi host over the VMKernel connection in the virtual-switch. That's actually a connection between the two ESXi hosts over your physical Ethernet switch. Meanwhile, when state is fully transferred control of the VMDK files is passed over to the target server it it goes live. So, vMotion requires both FC and Ethernet.
Unless you meant Hardware Virtualization, or direct LUN presentation. That can also work, but is trickier. The same volume needs to be presented to both ESXi hosts using exactly the same LUN number, if it doesn't have the same LUN number that volume won't be visible when the VM is on one or the other ESXi host. 
HA (what I presume you meant by HV) requires vMotion to work, so should be available if vMotion is working. 
As long as both your ESXi hosts can see the same shared storage, HA, vMotion, Storage vMotion, etc. should all work. It doesn't matter how these systems see the storage - you could have one host attached via FC and one via iSCSI. As long as they have consistent access to the same VMFS volume(s), you should have no problems.
I have a PowerVault with 4 FC modules at 8gb/s. I don't want to jack the SAN into our 10/100/1000 switch because of the max 1gb/s speed.