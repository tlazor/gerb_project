My question would then be: why do you want to do this? Once you have the centroids for each cluster using k-means, it is pretty quick to be able to determine which cluster a new record would belong to. I'm not sure I understand why you would want to use a random forest in this fashion.
If you were to have fully formed trees in your random forest, then they should be able to recreate the rules you ended up clustering by, and accuracy would be 100%. Now, if you are cutting off the depth of the trees in the random forest (which is pretty standard, let's say you set max_depth = 8), then you will have an accuracy that may be less than 100%. This means that there are salient features that you are not consistently getting information from because they are not part of the 8 levels that one or more decision trees are using to classify.
I have made clusters for my data set (1.5 million samples and 800 features) using k-mean. I am aware of internal indices for evaluating clusters. However, I was thinking about training a supervised classification model (e.g. random forest classifier) in which cluster numbers are dependent variable and all other features are independent variable. Is using the accuracy of this model a good evaluation of the kmean cluster and why?