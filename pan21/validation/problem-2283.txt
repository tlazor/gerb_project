I assume each new tarball is pretty similar in structure to the previous tarball (since I'm often only changing a few lines of source code in between deployments). Is there a way to take advantage of this fact to speed up my uploads using rsync?
I'm putting together a deployment script which tars up a directory of my code, names the tar file after the current date and time, pushes that up to the server, untars it in a directory of the same name and then swaps a "current" symlink to point at the new directory. This means my older deployments stay around in timestamped directories (at least until I delete them).
You'll want to minimise the changes on each invocation of tar. Would it help to make sure that the files are always in the same order in each instance. You can then compress with the --rsyncable option. 
Ideally I'd like to say "hey rsync, upload this local file called 2009-10-28-222403.tar.gz to my server, but it's only a tiny bit different from the file 2009-10-27-101155.tar.gz which is already up there, so try to just send over the differences". Is this possible, or is there another tool I should be looking at?
What does it have to be a tar file? Why not rsync the code to your deploy directory and use the tar as backup?
I think you'll be best off with the efficient rsync of individual files included in other answers, and then generating the .tar.gz from the resulting directory on the server (or the client if that's where you want to keep your archive). What's wrong with your version control system, as a record of what you deployed when? You're not deploying uncommitted code are you?
Another thing to consider is that tar supports blocking, and will pad out each file with nulls to a block offset. Check out block sizes. You could set this to the rsync block size (ah, that depends on the size of the file, erm how about 8k?). Which will help the algorithm when a single file is reordered. Now, drop the gzip on each end (gzip the last-but-one on the server if you're worried about disk space), and I think you might get the speed up you want.
Can you order the files by last modified date? That way the files that don't change are always in the same order, and at the beginning, and the files that change are at the end, so when they change length they don't break the blocking algorithm.
This allows rsync to select a file on the destination system that is similar to the file being transferred, and use that file as the base upon which to apply its delta uploading algorithm.  It's a bit memory and I/O hungry, particularly if you have a large directory on the destination side, however it should give you the upload improvements you're looking for without having to rejigger your approach as other answers have suggested.
I'm not that impressed with the --rsyncable option. I'm using it on daily postgres dumps, and find that, although only a small amount of the dump changes each day, rsync uses about half the bandwidth of just copying the .gz around. I might ask a question about this actually.