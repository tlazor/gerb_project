In prose: I cast a ray from the player's/camera's current position along the speed vector, which extends until it hits wall. (The Raycast function is used elsewhere, so I already know it works correctly.) I then define the maximum distance the player is permitted to move in each dimension to be slightly less than the component of the ray in that dimension (so that we never end up exactly on a wall). I then compute a provisional set of deltas to add to the current position based on the speed and the time since the last frame (in fractional seconds), and check to see if any of them exceed the maximum allowed magnitude which would take the player through the wall. If they do, I clamp the magnitude of the offending component to its max value, and set the component of speed in that direction to 0; so, the player should end up just barely outside the wall, and sliding along it.
Unfortunately, I cannot consistently prevent the camera from moving through walls. There is no off-the-shelf game engine or framework involved, so I have to implement all this from scratch.
I'm building a fairly basic 3D first-person puzzle game. The only objects for which collisions matter are walls, and all walls are at convenient 90 degree angles aligned with the coordinate axes.
This usually works pretty well if you happen to run straight into a wall, but it fails frequently but unpredictably (as in, sometimes it doesn't fail) near corners, and especially near three-way corners (where two walls and a ceiling or floor come together).