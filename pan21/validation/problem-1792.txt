In both SVMs and linear regression models you can calculate the distance of a sample from the border and treat it as a confidence measurement (but it is not exactly that).
With decision trees I'm not an expert but a similar question was posted and answered in the following link:
One concern for such a small data set is overfitting when the number of features is much larger than the number of samples. You should address that by using some form of regularization.
The question whether to use a linear classifier depends less on the number of samples you have in your dataset and more whether your dataset is linearly separable (by the way, SVMs can be non-linear with the kernel trick).
Now with regards to confidence in the classification, In SVMs there is a method that calculates the probability that a given sample belongs to a particular class using Platt scaling ("Original Paper"). This is the approach that is used in sklearn's SVM confidence implementation. You can read more about it in the following link:
I would strongly recommend using some known embedding method like the word2vec, since as you mentioned, your dataset is too small for your model to be able to properly learn an encoding of context and vocabulary from.
The only way to know if a classifier is suitable for your data set is to try it and test it. All classifiers you've mentioned have a way to give confidences of their predictions. Logistic regression and decision trees will give you the probability that a sample is the positive class. SVM's will give you the distance to the decision hyperplane which can be used as a confidence measure; with some additional computations, you can also get a probability with SVM's, but I won't detail that here.