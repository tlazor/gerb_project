A lot of applications have poor connection discipline, keeping connections open even when they are not being used.
When you go back and read the Optimal Database Connection Pool Size article, you will find that it suggests that you set your active connection pooling at the client side, as opposed to the server side.  They also suggest that you leave sufficient capacity in your max_connections value to accommodate fixed connections, such as hands-on client activity and administrative activity.  You don't want to set your max_connections to the active connection limit of your workers or you might not be able to psql in when you need to!
An important distinction to make between the two claims compared in the question is that the first is a rough formulation for the number of active connections at any one time.  The second claim is for the setting that you put in place for the maximum allowed that Postgres will accept.  These are two separate things.
For me - not an experienced DBA -  there's a discrepancy somewhere in here, especially looking at the offerings of some DB-as-a-Service providers.
Even more extreme is the discrepancy for another DBaaS provider, who proposes a 2 core server with 500 concurrent connections. How could this possibly work well?
For example,  at this time Amazon RDS's largest machine (db.r3.8xlarge) has 32 vCPUs, which according to the first formula would perhaps manage to run optimally with 100 connections in the pool, given many disks. Wouldn't it though run very badly with the "few hundred connections" from the second formula?
Setting a high connection limit is cheap insurance against these applications.  Until something changes and the applications decide to actively use all of those connections, then the insurance becomes pretty expensive.