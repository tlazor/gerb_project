Is hit and trial the only way to find the optimum number of neurons in a MLP hidden layer. How can I further tweak the parameters , can it be some other hyper parameters that I can play with like intialzing hidden layer weights initially.or adding more layers.(but adding more layer doesn't help much) I tried it.
I am trying to find out what is optimum number of neurons that can be used in MNIST dataset(60,000 training and 10,000 testing data). I build a single hidden layer model using keras,with relu activation but when I increase the neurons in hidden layer the accuracy improves slightly nearly .4% per 100 neurons ,even if I use 2000 neurons instead of 784 neurons it doesn't has any considerable change.I thought that increasing neurons above certain limit will decrease the classification accuracy because of overfitting.but It doesn't seem to look happening I even tried using 2000 neurons but the accuracy doesn't decrease on the test data.exceeding this limit I think is not possible for me because it can crash my computer.