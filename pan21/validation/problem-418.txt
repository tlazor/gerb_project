This approach is the cleanest from a pure dimensional modeling perspective. The logic in your CASE statements would be applied in an ETL process and the size on disk is reduced.  In Sql Server the CPU overhead is increased because a join is typically more cpu intensive then a CASE statement (I assume this is also true for Oracle).  If I/O is the bottleneck then this is a trade-off worth making and is the preferred approach.
This approach would require one row in source_codes for each distinct source_code.  Size on disk increases due to source_codes table.  Disk IO remains the same or increases depending on the ability of the source_codes table to fit in memory.  This approach makes sense only in the case where the queries are CPU bound due to logic in the CASE statement that is horrendously complex.
Here are four approaches that each have advantages and disadvantages depending on what the current bottleneck is.
If the bottleneck is the CPU and the cost of computing the CASE statements is less than the cost of joining to a another table then keeping your existing queries may be the best bet.
This approach makes sense if the bottleneck is network bandwidth to the client or the CPU load is too high with either the JOIN or the CASE statements.  By moving the complex CASE logic off the server you can distribute CPU load.  This would be the most intrusive solution and should be considered last resort.