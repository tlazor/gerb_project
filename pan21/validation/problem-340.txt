In theory so long as the PCI-E card fits in the slot then the card and host should negotiate the number of lanes that is used, from Wikipedia:
I'm somewhat under the impression that the only requirement for x8 is because of the potentially high bandwidth that these cards can handle. As I'll only be using a maximum of 4 Hard Drives in RAID 5 or 6 mode, should the x4 electrical/x16 mechanical slot work?
After exhaustively searching around on the internet and here at superuser I couldn't find a definitive answer to this. I've heard how with some devices they work with less pci-e bandwidth even if they are rated higher. Without further delay, here is my dillema.
I'm looking for a Hardware RAID solution as I cannot afford downtime that would be incurred by simply backing up data, yes I will still be backing up data as I'm well aware RAID is not a backup, and require performance either in RAID 5 or 6 mode. 
In a SOHO environment I doubt you would ever notice the difference.  In a server environment where you wanted to maximize throughput and I/O's, you probably would notice the difference.  I suppose you might also notice the difference if you were running solid state drives vs. mechanical drives or you were only seeking maximum throughput.
However, something didn't seem to get covered clearly.  For gamers, you want the fastest speeds possible for your graphics card right?  That means 16x mechanical, and 16x lanes.  However, many motherboards do NOT give you that rating if you add another card into the motherboard (depending on where you stick that card).  For instance, it will say something like, "16x will drop to 8x if a card is inserted in the 8x slot, and the 8x slot will run in 4x because the bandwidth is shared", which is stupid as heck.  Anyway, the 4x slot in many cases does NOT share the bandwidth with the 16x slots, and is therefore SAFE!  Instead, (in the case of my motherboard for instances) it takes up all the bandwidth of the 1x slots making them unusable, which I could care less about.  The problem I ran into was my raid card needs 8x MECHANICAL, and the 4x slot, which can take an 8x card in size, has metal pins that correspond to only 4x mechanical, NOT 8, so I HAVE TO USE THE 8x SLOT!  So lame!  Oddly enough, the card works, but only 50% of the time, dropping off and vanishing the other 50%, which caused me to look into what was happening with this solid card in my last machine, but unstable as heck card in my new build.  So now my card is in the 8x slot, FULLY stable, and dropping my gaming rig 50% in bandwidth...not happy, but I understand the problem now.  Wish I had a 4x SATA RAID card, but I haven't really found any in existance by Areca.
I've done it with x8 physical/x8 electrical 3Ware cards which I have plugged into x16 physical/x4 electrical slots on motherboards.  They work just fine.  In my case, the 3Ware GUI also confirmed the card was only seeing 4 electrical lanes, as expected.
I don't really want to run backups either, hence I run a RAID 5, and every so often, as an extra precaution, I use a large single drive to backup the entire RAID 5.
I've been having a bit of a problem with performance of FakeRAID which as I've learned isn't much of a shocker. My dillema exists because my motherboard has two PCI-E x16 slots, except one runs at x4 mode so it's merely x16 in a mechanical sense. See More here: http://www.newegg.com/Product/Product.aspx?Item=N82E16813130608
I've been shopping around and determined the LSI MegaRAID SATA/SAS 9260-4i with BBU is my best solution, only it and any Hardware RAID cards worth mentioning are x8. 