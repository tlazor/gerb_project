Another important thing to know is whether you want to update all rows in the table.  Are some of the values already correct?  If so, you will get a big performance boost by not updating the rows that don't need it.  Add AND temp.customername is distinct from user.customername to the WHERE clause.
If you follow the very good advice of kgrittn and still have performance issues, you may need to perform the update in batches.  You would still perform set-based updates but limit them to the first 1000 (or whatever number works for you, I've used from 500 to 50,000)records that don't match and then keep looping until all are done. 
As suggested in a comment, your query may run faster with a join rather than a correlated subquery.  Something like this:
If you limit the number of rows updated in each statement, and VACUUM ANALYZE after each UPDATE, you will avoid table bloat.  If the point of the desire to minimize CPU time is to avoid a performance impact on concurrent transactions, this would give you the opportunity to introduce a short delay (in the form of a sleep or something similar) before you start the next UPDATE of a set of rows.
Even better, why are you redundantly storing the information in the temp table rather than joining to it when needed?  (Sometimes there is a good reason; quite often there isn't.)
The first question is: Why is it important that you not use a lot of CPU time? The query will be bottlenecked on some resource; if you could introduce enough additional disk access, the CPU time used per second would go down, but would that really be an improvement?  What resource would you prefer to saturate?  Understanding why you have emphasized this might help guide people to providing an answer that you will find useful.
PostgreSQL has no way to reduce the amount of CPU time a process can use.  On Linux you can use OS features like the renice command to do that.  See Priorities for more information and some samples.