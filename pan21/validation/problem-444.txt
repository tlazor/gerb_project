rsync will work on database files. It is tested. There is only one prerequisite - offline your database before your final rsync sync over network. So you'll have consistent snapshot on a remote target. It's no different than copying files of a deattached database.
(Of course, you need to worry about other things too, like jobs, packages, login and user synchronization, changing DSNs on the web servers during a failure, etc. If you have a large environment and a serious disaster, like the loss of your primary data center, you will be trying to do this when IIS guys, file server guys, network guys and whatever else guys are trying to bring their stuff up too.)
You need an automated method to restore that full backup and any relevant tlog backups. Ideally, this should be simple enough for someone else to do and/or simple enough for you to figure out at 3AM when your primary server fails and you are half asleep.
If my only disaster recovery option was a remote site that was normally down, I'd use log shipping and make sure that I had all of the pieces to restore the database(s) on the remote site. If you can't make the built-in log shipping do that, writing your own isn't that hard. I've probably written three or four (simplistic) log shipping systems from scratch in the last 14 years.
The minimum deltas you have to ship over is ... the log shipping delta! RSync will not reduce the delta, it will increase it. Log shipping does not need to ship over any MDF delta, because the the standby knows how to interpret the LDFs delta into actions to be applied to the local MDF (ie. by running recovery with standby on the log it received). 
Log shipping is supported, is optimal, is exposed in the SQL management toolset, licensing is Standard (and one can always roll his own log shipping on Express using few scripts) and, last but not least, is correct.
101% of the times one attempts to circumvent the logic of the WAL protocol by doing file level/block level replication outside of SQL Server control it ends up in disaster because file/block level operations are not aware of the WAL protocol. And that is even when the file level replication does manage to handle correctly the LDF rollover on the primary... 
I have seen enough examples of people who thought that they were replicating their files but were not. They were left with corrupt files on the recovery site and with inaccessible backup files at the primary site. So, that meant no database for them.
When you have an event, bringing up the other server will take longer because you would have so much manual stuff to do. That means that this isn't as good as simply implementing regular log shipping. You will need to test this periodically, as well.
If it were me, I would be agitating for a warm standby server at the remote site. That would make (standard, out-of-the box) log shipping easier and database mirroring possible. It sounds like you have tried that.
I would not trust rsync to copy the ldf and mdf files for user or system databases, nor would I trust anything I hacked together (VSS or otherwise) in a production environment. SQL Server is very fussy about when (and if) things get written to the ldf and the mdf files. Software (rsync) that isn't designed with that in mind might not get it right, especially if it doesn't understand that the ldf files and mdf files need to be treated as an interrelated system of files. If that software doesn't get things right, nothing might be noticed until you failover, try to go live and have your databases flagged as suspect due to what SQL Server sees as data corruption. Even worse, "getting it right" might be dependant on how much load is on the system and you might not find the problem in a lightly-loaded test environment.
If you had some sort of block-level replication technology like EMC's SRDF or were looking at shared-nothing clusters, that might be different. Those technologies interface with SQL Server and the clustering services that Windows provides in a way that your writes will be safe and your files will/should be consistent.