Since, as your VM load is usually split between two server, having all of them running on a single node might exhaust the RAM of the node or congest the CPU. 
Very hard to cover all bases but redundant power supplies, good quality power and good backups can help.
With the 2nd option (VM normally split between node A and B), you have to prioritize which VM are allowed to come back online. 
There are intermediate solutions that can avoid SPoF and still be appropriate in small/medium sized businesses : node-to-node replication without SAN storage.
The downside, is the lost of data since the last replication (which depending on your settings and hardware performances can be as low as 1 minute, and as high as a few hours). 
So instead of having an expensive 1 or 2 servers and a SAN with internal redundancy measures, you can use several commodity low-power low-cost machines. 
I would personally opt for multiple servers. I don't think equipment failure is more likely in this scenario. Yes, you have more equipment that could fail, but the odds of any given unit failing should be constant.
My default approach is to avoid any centralized infrastructure. For example, this means no SAN, no Load Balancer. You can also call such a centralized approach "monolithic".
As a software architect, I'm working with the customer's infrastructure. That might mean using their own private data-center, or using something like AWS. So I don't usually have control over whether they use a SAN or not. But my software usually spans multiple customers, so I build it as if it will be run on individual machines in isolation on a network.
This is supported for example by Proxmox (but I think it also is supported by XCP-ng/XenServer and probably by ESXi). 
We have used Backup Exec System Recovery for some critical systems.  Not so much for daily backup but as a recovery tool.  We can restore to different hardware, if available, and we also use the software to convert the backup image to a Virtual Machine.  If the server fails and we need to wait for hardware repairs, we can start a VM on a different server or workstation and limp along.  Not perfect but it can be up and running quickly.
The software architecture solution, is that a Web Application would connect to one of a list of available nodes (at random), and if that's unavailable it will try to connect to another node (at random). A client might get kicked off a server, if it's too busy. Here, there's no need for a load balancer to connect through to a web farm; and, there's no need for a SAN for high availability. It's also possible to shard the database per-department or geography.
This kind of setup can tolerate a network failure, a total and major node failure (any of the three), with a downtime of a about 1 minutes (roughly the time needed for a VM to boot up). 
Email is weird, because it's a legacy system (that works). If email was invented today, it would probably use RESTFul APIs on web servers, and the data would be in a database that could be replicated using normal tools (Transactional Replication, Incremental Backups).
What having multiple servers in a non-redundant/non-HA configuration gives me is the ability to off-load some of the work to another server in the event of a failure. So, say my print server goes down. If I can map a few printers to the file server while I'm fixing the print server, the impact to operations is lessened. And that's where it really matters. We often tend to talk about hardware redundancy, but the hardware is only a tool for continuity of operations.
I think this is a question with many answers but I would agree in many smaller shops the several server solution works and as you say, at least something keeps going if there is a failure.  But it depends on what fails.
Your original post hypothesize that you can't afford a cluster, but you consider solutions with two servers (not including backups). That would imply that you most likely have three server on hands, enough to start a cluster.