If you are going the linux route, I would either use a hardware raid card ( with battery backup ) or have a raid controller in the disc enclosure. I agree that xfs is the filesystem of choice on Linux, however be aware that filesystems of around 50TB on xfs take more than 16GB of RAM if you need to run xfs_check. 
I'd like it all to be mounted as one volume, due to the type of data we're storing on it. One crazy idea we had was to configure 6 4-disk RAID 5 volumes, then do software RAID 5 over those 6 volumes. That would mean any one volume could die on us and we'd still not lose data.
Getting nfs/samba to perform well is a bit of a dark art, Are you going to be using 10GB ether or just aggregations of 1GB/sec ? ( Do not get Broadcomm cards especially 10GB ones ).
I'm thinking of building a 24 1TB disk NAS box, but I'm not sure what the best drive configuration is.  I'm looking at using the areca ARC-1280ML-2G controller, and hanging all 24 drives off of it.
If you are going to get lots of small writes go with RAID 10 since RAID 5+ takes a 4 x hit on small writes.
What is the best solution to this type of configuration? With 24 1 TB disks, it's likely that more than one will fail at the same time (or within the time it takes to rebuild the volume after a first failure), so I'm having trouble finding a good solution.
Test the way the system can fail before it goes into production and have it written up where you and your colleagues can find the docs when all goes wrong.
I should note that this is an R&D project, we've got an upcoming application where we'll be needing tens of terabytes of storage to be fast and highly available. But for the initial R&D phase we can accept some risk.
I would seriously consider a good NAS box such as a NetApp as they are a lot less work long term, it depends how much your/the storage admins time is worth to the company.
As a rough guide a RAID5 array shouldn't have more than about 14 disks in one array, whilst a RAID6 should be fine with up to 54 disks or so - obviously the bigger the array the bigger the gulf between read and write performance and the slower rebuilds will take but it CAN be a good trade-off.
It depends on your read/write ratio. We use a lot of HP MSA70 external 25-disk SAS drive enclosures and always create them as a single RAID6 array as our read to write ratio is 99%:1% so we don't care that R6 is about the slowest at writing (still pretty quick, just not that good compared to others). This way we have 23 disks worth of data available to us, have very good, as in VERY good, random read and overall read bandwidth benefits and can survive two disk failures.
wazoox, answers are good I don't have the rep to give him more plus points, but I would add the following.
RAID 6 or at least 2 live parity discs per 10 discs, 16 at most that is if you can take around a day when performance will be impacted by your raid rebuild. If you can't live with the degradation then its going to have to be mirrored stripes.