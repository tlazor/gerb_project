One note about this model is that for some NP problems, including SAT, you can print their truth table in polynomial time per bit by exploiting the self-reducibility of the problem.  For example, with SAT, you can always efficiently find the next bit of the truth table by fixing one of the variables, computing the reduced version of the problem under this variable fix, and then looking up the solution to the reduced version of the problem in the truth table that you have computed so far.
Some of the very early work on complexity theory used a sequential time model -- that is, rather than studying the worst-case runtime of the TM that can produce the correct output on an arbitrary input, they studied machines that would run infinitely and enumerate the correct output for each input in lexicographic order.  The complexity of the machine was then based on the worst-case time gap ("delay") between the enumeration of consecutive outputs.  This model can be used to study the problem of taking an input $1^n$ and producing on output the $2^n$-sized truth table of a language on all inputs of length $n$ , while trying to minimize the average computation time required per input (so $2^n poly(n)$ is considered "efficient" in this model).  This seems pretty similar to the question you're asking.
Here is a paper that uses that model.  Here is a blog post that is only somewhat related, but includes some references that you might find interesting.