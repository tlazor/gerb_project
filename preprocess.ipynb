{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 08:43:28.355651: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-06 08:43:28.404010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 08:43:29.089363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(random_seed)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(random_seed)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(random_seed)\n",
    " \n",
    "# Set a random seed for PyTorch (for GPU as well)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pan21_functions import *\n",
    "import datetime\n",
    "from keras.callbacks import CSVLogger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pan21PyDataset(\"pan21/train\", \"pan21/train\", \"train_ds_uncompressed\").to_file()\n",
    "# Pan21PyDataset(\"pan21/validation\", \"pan21/validation\", \"val_ds_uncompressed\").to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the files are good\n",
    "def test_load(path, name):\n",
    "    np.load(path)[name]\n",
    "\n",
    "# _ = Parallel(n_jobs=-1)(delayed(test_load)(Path(\"train_ds\") / f\"{i}.npz\", \"batch_x\") for i in tqdm(range(350)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pan21FourierDataset(\"pan21/train\", \"pan21/train\", \"train_ds_uncompressed\", num_fourier_features=512).to_file()\n",
    "# Pan21FourierDataset(\"pan21/validation\", \"pan21/validation\", \"val_ds_uncompressed\", num_fourier_features=512).to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = Parallel(n_jobs=-1)(delayed(test_load)(Path(\"train_ds\") / \"fourier\" / f\"{i}.npz\", \"fourier_batch_x\") for i in tqdm(range(350)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start_idx = 0\n",
    "# limit = 5\n",
    "# # limit = len(train_ds)\n",
    "\n",
    "# before = time.time()\n",
    "# for i in range(start_idx, start_idx+limit):\n",
    "#     batch_x, batch_y = train_ds.__getitem__(i, force_compute=True)\n",
    "# after = time.time()\n",
    "# for i in range(start_idx, start_idx+limit):\n",
    "#     batch_x, batch_y = train_ds.__getitem__(i, force_compute=False)\n",
    "# after_after = time.time()\n",
    "\n",
    "# print(f\"Compute: {round((after - before)/limit, 2)}s vs File read: {round((after_after - after)/limit, 2)}s\")\n",
    "# # Compute: 8.95s per batch\n",
    "# # Compute with compression: ~17s\n",
    "# # Uncompressed read: .7s per batch\n",
    "# # Compressed read: 1.6s per batch\n",
    "# # Compressed is 1/10 the size of uncompressed, but takes ~twice as long to precompute and save\n",
    "# # Compressed 512D Fourier takes ~30s per batch\n",
    "# # Compressed 512D Fourier is about 500-700MB per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Input, Flatten\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "\n",
    "# Code implementation of the RNN for sequence labeling\n",
    "def create_rnn_model(num_labels, embedding_dim, max_input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(max_input_length*2, embedding_dim)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_labels, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.RMSprop(),  # Optimizer\n",
    "        # Loss function to minimize\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        # List of metrics to monitor\n",
    "        metrics=[metrics.BinaryAccuracy(), metrics.AUC()],\n",
    "        jit_compile=True\n",
    "    )\n",
    "\n",
    "    return model\n",
    " \n",
    "num_labels = 1\n",
    "embedding_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_num_ff(train_ds, val_ds, model_name, epochs=5):\n",
    "    max_input_length = train_ds.max_input_length\n",
    "    # print(f\"{max_input_length=}\")\n",
    "\n",
    "    print(model_name)\n",
    "    time_string = f\"{datetime.datetime.now().strftime('%Y_%m_%d-%I_%M_%S_%p')}\"\n",
    "    model_name.mkdir(exist_ok=True)\n",
    "    checkpoint_name_format = time_string + \"_cp-{epoch:02d}.weights.h5\"\n",
    "    checkpoint_path = model_name / checkpoint_name_format\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        verbose=1, \n",
    "        save_weights_only=True,\n",
    "        # Save weights, every epoch.\n",
    "        save_freq='epoch')\n",
    "\n",
    "    model = create_rnn_model(num_labels, embedding_dim, max_input_length)\n",
    "    csv_logger = CSVLogger(f'{model_name}_{time_string}.log', separator=',', append=False)\n",
    "\n",
    "    already_trained_epochs = 0\n",
    "    if model_name.exists():\n",
    "        checkpoints = list(Path(model_name).glob('*.weights.h5'))\n",
    "        if checkpoints:\n",
    "            already_trained_epochs = len(checkpoints)\n",
    "            model.load_weights(checkpoints[-1])\n",
    "\n",
    "    with tf.device(\"/device:GPU:0\"):\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs - already_trained_epochs,\n",
    "            validation_data=val_ds,\n",
    "            verbose=1,\n",
    "            callbacks=[csv_logger, cp_callback]\n",
    "        )\n",
    "\n",
    "    model.save(f\"{model_name}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does using the frequency domain spectra provide usefule information?\n",
    "model_dir = Path(f\"models/num_fourier_features\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True)\n",
    "\n",
    "num_fourier_features = [0, 512//4, 512//2, 512]\n",
    "for num_ff in tqdm(num_fourier_features):\n",
    "    fourier_train_ds = Pan21FourierDataset(\"pan21/train\", \"pan21/train\", \"train_ds_uncompressed\", num_fourier_features=num_ff)\n",
    "    fourier_val_ds = Pan21FourierDataset(\"pan21/validation\", \"pan21/validation\", \"val_ds_uncompressed\", num_fourier_features=num_ff)\n",
    "\n",
    "    model_name = model_dir / f\"{num_ff}_{datetime.datetime.now().strftime('%Y_%m_%d-%I_%M_%S_%p')}\"\n",
    "\n",
    "    train_model_num_ff(fourier_train_ds, fourier_val_ds, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many epochs should be trained?\n",
    "model_dir = Path(f\"models/num_epochs\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True)\n",
    "\n",
    "num_fourier_features = [0, 512//4]\n",
    "for num_ff in tqdm(num_fourier_features):\n",
    "    train_model_num_ff(num_ff, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pan21_functions' from '/home/ubuntu/tar/pan21_functions.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib; import pan21_functions as p21; importlib.reload(p21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239a6736eb004f45a0023a58721efaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/cutoffs/0.0_0.25\n",
      "Epoch 1/3\n",
      "\u001b[1m 487/2186\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34:15\u001b[0m 1s/step - auc_3: 0.5228 - binary_accuracy: 0.5374 - loss: 0.7037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1717664328.742487 1526693 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 412 bytes spill stores, 416 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2185/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - auc_3: 0.5837 - binary_accuracy: 0.5689 - loss: 0.6764\n",
      "Epoch 1: saving model to models/cutoffs/0.0_0.25/2024_06_06-08_48_44_AM_cp-01.weights.h5\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3396s\u001b[0m 2s/step - auc_3: 0.5837 - binary_accuracy: 0.5690 - loss: 0.6764 - val_auc_3: 0.7190 - val_binary_accuracy: 0.6515 - val_loss: 0.6172\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:45:33.350853: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:12: Filling up shuffle buffer (this may take a while): 7 of 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/2186\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:41:26\u001b[0m 13s/step - auc_3: 0.6727 - binary_accuracy: 0.6250 - loss: 0.6488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:45:34.840486: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2185/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - auc_3: 0.7193 - binary_accuracy: 0.6596 - loss: 0.6095\n",
      "Epoch 2: saving model to models/cutoffs/0.0_0.25/2024_06_06-08_48_44_AM_cp-02.weights.h5\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3251s\u001b[0m 1s/step - auc_3: 0.7193 - binary_accuracy: 0.6596 - loss: 0.6095 - val_auc_3: 0.7363 - val_binary_accuracy: 0.6577 - val_loss: 0.6026\n",
      "Epoch 3/3\n",
      "\u001b[1m 167/2186\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42:20\u001b[0m 1s/step - auc_3: 0.7695 - binary_accuracy: 0.6986 - loss: 0.5684"
     ]
    }
   ],
   "source": [
    "# Does filtering help?\n",
    "model_dir = Path(f\"models/cutoffs\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True)\n",
    "\n",
    "N = 4 # High pass, band stop, band stop, low pass\n",
    "# Frequencies respresented as percent of Nyquist frequency\n",
    "cutoff_frequencies = [(i / N, (i + 1) / N) for i in range(N)]\n",
    "for cutoff in tqdm(cutoff_frequencies):\n",
    "    filter_train_ds = p21.Pan21FourierFilterDataset(\"pan21/train\", \"pan21/train\", cutoff)\n",
    "    filter_val_ds = p21.Pan21FourierFilterDataset(\"pan21/validation\", \"pan21/validation\", cutoff)\n",
    "\n",
    "    model_name = model_dir / f\"{cutoff[0]}_{cutoff[1]}\"\n",
    "\n",
    "    train_model_num_ff(filter_train_ds, filter_val_ds, model_name, epochs = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
